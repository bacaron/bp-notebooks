{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bctpy\n",
      "  Downloading bctpy-0.5.2-py3-none-any.whl (102 kB)\n",
      "\u001b[K     |████████████████████████████████| 102 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jgf\n",
      "  Downloading jgf-0.2.2.tar.gz (9.6 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from bctpy) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from bctpy) (1.18.4)\n",
      "Building wheels for collected packages: jgf\n",
      "  Building wheel for jgf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jgf: filename=jgf-0.2.2-py3-none-any.whl size=10821 sha256=6869d2f4067e89a83bb8351078bf9be0d257b250792b0669167925db7a1c83cd\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/73/d9/37/4c2e9282bc99f955151b06d0b7b970c521aef95f484084fc13\n",
      "Successfully built jgf\n",
      "Installing collected packages: bctpy, jgf\n",
      "Successfully installed bctpy-0.5.2 jgf-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bctpy jgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "import bct\n",
    "import jgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will add a subjectID and sessionID column to the output data\n",
    "def addSubjectsSessions(subject,session,path,data):\n",
    "    \n",
    "    if 'subjectID' not in data.keys():\n",
    "        data['subjectID'] = [ str(subject) for f in range(len(data)) ]\n",
    "    \n",
    "    if 'sessionID' not in data.keys():\n",
    "        data['sessionID'] = [ str(session) for f in range(len(data)) ]\n",
    "        \n",
    "    return data\n",
    "\n",
    "## this function calles checkForDuplicates and attempts to find duplicates. then uses that output, sets a dumby sessionID if not present,\n",
    "## and appends the object data\n",
    "def appendData(subjects,sessions,paths,finish_dates,obj,filename):\n",
    "        \n",
    "    # check for duplicates. if so, remove\n",
    "    finish_dates, subjects, sessions, paths = checkForDuplicates(obj,finish_dates,subjects,sessions,paths)\n",
    "\n",
    "    # append data to appropriate lists\n",
    "    subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "    if 'session' in obj['output']['meta'].keys():\n",
    "        sessions = np.append(sessions,obj['output']['meta']['session'])\n",
    "    else:\n",
    "        sessions = np.append(sessions,'1')\n",
    "    paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "    finish_dates = np.append(finish_dates,obj['finish_date'])\n",
    "    \n",
    "    return finish_dates, subjects, sessions, paths\n",
    "\n",
    "## this function will call addSubjectsSessions to add the appropriate columns and will append the object data to a study-wide dataframe\n",
    "def compileData(paths,subjects,sessions,data):\n",
    "    # loops through all paths\n",
    "    for i in range(len(paths)):\n",
    "        # if network, load json. if not, load csv\n",
    "        if '.json.gz' in paths[i]:\n",
    "            tmpdata = pd.read_json(paths[i],orient='index').reset_index(drop=True)\n",
    "            tmpdata = addSubjectsSessions(subjects[i],sessions[i],paths[i],tmpdata)\n",
    "        else:\n",
    "            tmpdata = pd.read_csv(paths[i])\n",
    "            tmpdata = addSubjectsSessions(subjects[i],sessions[i],paths[i],tmpdata)\n",
    "\n",
    "        data = data.append(tmpdata,ignore_index=True)\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def compileNetworkAdjacencyMatrices(paths,subjects,sessions,data):\n",
    "    \n",
    "    # loop through paths and append adjacency matrix to dictionary\n",
    "    for i in range(len(paths)):\n",
    "        data[subjects[i]+'_sess'+sessions[i]] = jgf.conmat.load(paths[i],compressed=True)[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "### subjects dataframe generation\n",
    "## this function will make a dataframe from a list of subjects and groups. will also add a color column for easy plotting\n",
    "def collectSubjectData(topPath,dataPath,groups,subjects,colors):\n",
    "\n",
    "    # set up variables\n",
    "    data_columns = ['subjectID','classID','colors']\n",
    "    data =  pd.DataFrame([],columns=data_columns)\n",
    "\n",
    "    # populate structure\n",
    "    data['subjectID'] = [ f for g in groups for f in subjects[g] ]\n",
    "    data['classID'] = [ g for g in groups for f in range(len(subjects[g]))]\n",
    "    data['colors'] = [ colors[c] for c in colors for f in subjects[c]]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'subjects.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "## this will create a subject-specific color for each subject in the subjects dataframe\n",
    "def createColorDictionary(data,measure,colorPalette):\n",
    "\n",
    "    # Create subject keys and color values\n",
    "    keys = data[measure].unique()\n",
    "    values = sns.color_palette(colorPalette,len(keys))\n",
    "    values = values.as_hex()\n",
    "\n",
    "    # zip dictionary together\n",
    "    colors_dict = dict(zip(keys,values))\n",
    "\n",
    "    return colors_dict\n",
    "\n",
    "### load data\n",
    "## this function is useful for identifying duplicate datatypes. if it finds one, it will update the data with the latest finishing dataset.\n",
    "def checkForDuplicates(obj,finish_dates,subjects,sessions,paths):\n",
    "    \n",
    "    # first checks if there is a session id available in the keys of the object. if finds one, then checks if the subject and session ID \n",
    "    # were already looped over. if so, will delete position in list and update with appropriate path. if it doesn't find a session ID, it\n",
    "    # just attempts to find if the subject has already been looped over\n",
    "    if 'session' in obj['output']['meta'].keys():\n",
    "        if (obj['output']['meta']['subject'] in subjects) and (obj['output']['meta']['session'] in sessions):\n",
    "            index = np.where(np.logical_and(subjects == obj['output']['meta']['subject'],sessions == obj['output']['meta']['session']))\n",
    "            if finish_dates[index] <= obj[\"finish_date\"]:\n",
    "                subjects = np.delete(subjects,index)\n",
    "                paths = np.delete(paths,index)\n",
    "                sessions = np.delete(sessions,index)\n",
    "                finish_dates = np.delete(finish_dates,index)\n",
    "    else:\n",
    "        if (obj['output']['meta']['subject'] in subjects):\n",
    "            index = np.where(subjects == obj['output']['meta']['subject'])\n",
    "            if finish_dates[index] <= obj[\"finish_date\"]:\n",
    "                subjects = np.delete(subjects,index)\n",
    "                paths = np.delete(paths,index)\n",
    "                sessions = np.delete(sessions,index)\n",
    "                finish_dates = np.delete(finish_dates,index)\n",
    "\n",
    "    return finish_dates, subjects, sessions, paths\n",
    "\n",
    "## this function is the wrapper function that calls all the prevouis functions to generate a dataframe for the entire project of the appropriate datatype\n",
    "def collectData(datatype,datatype_tags,tags,filename,outPath,net_adj):\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "    \n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    sessions = []\n",
    "    paths = []\n",
    "    finish_dates = []\n",
    "    \n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects and find appropriate objects based on datatype, datatype_tags, and tags. can include drop tags ('!'). this logic could probably be simplified\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            # if datatype_tags is set, identify data using this info. if not, just use tag data. if no tags either, just append if meets datatype criteria. will check for filter with a not tag (!)\n",
    "            if datatype_tags:\n",
    "                # if the input datatype_tags are included in the object's datatype_tags, look for appropriate tags. if no tags, just append\n",
    "                if set(datatype_tags).issubset(obj['output']['datatype_tags']):\n",
    "                    # if tags is set, identify the data using this info\n",
    "                    if tags:\n",
    "                        # if input tags are included in object's tags, append. check if user wants to filter with a not tag (!)\n",
    "                        if set(tags).issubset(obj['output']['tags']):\n",
    "                            finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                        elif '!' in str(tags):\n",
    "                            tag = [ f for f in tags if '!' in str(f) ]\n",
    "                            tag_drop = [ f for f in tags if f not in tag ]\n",
    "                            if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                                if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                                    finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                    else:\n",
    "                        finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                elif '!' in str(datatype_tags):\n",
    "                    datatype_tag = [ f for f in datatype_tags if '!' in str(f) ]\n",
    "                    datatype_tag_drop = [ f for f in datatype_tags if f not in datatype_tag ]\n",
    "                    if not set([ f.replace('!','') for f in datatype_tag]).issubset(obj['output']['datatype_tags']):\n",
    "                        if tags:\n",
    "                            if set(tags).issubset(obj['output']['tags']):\n",
    "                                finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                            elif '!' in str(tags):\n",
    "                                tag = [ f for f in tags if '!' in str(f) ]\n",
    "                                tag_drop = [ f for f in tags if f not in tag ]\n",
    "                                if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                                    if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                                        finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                        else:\n",
    "                            finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "            else:\n",
    "                if tags:\n",
    "                    if set(tags).issubset(obj['output']['tags']):\n",
    "                        finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                    elif '!' in str(tags):\n",
    "                        tag = [ f for f in tags if '!' in str(f) ]\n",
    "                        tag_drop = [ f for f in tags if f not in tag ]\n",
    "                        if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                            if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                                finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "                else:\n",
    "                    finish_dates, subjects, sessions, paths = appendData(subjects,sessions,paths,finish_dates,obj,filename)\n",
    "\n",
    "    # shuffle data so subjects are in order\n",
    "    paths = [z for _,_,z in sorted(zip(subjects,sessions,paths))]\n",
    "    subjects = [x for x,_,_ in sorted(zip(subjects,sessions,paths))]\n",
    "    sessions = [y for _,y,_ in sorted(zip(subjects,sessions,paths))]\n",
    "    \n",
    "    # compile data\n",
    "    if net_adj:\n",
    "        data = {}\n",
    "        data = compileNetworkAdjacencyMatrices(paths,subjects,sessions,data)\n",
    "        np.save(outPath,data)\n",
    "    else:\n",
    "        data = compileData(paths,subjects,sessions,data)\n",
    "    \n",
    "        # output data structure for records and any further analyses\n",
    "        # subjects.csv\n",
    "        data.to_csv(outPath,index=False)\n",
    "    \n",
    "    return data\n",
    "\n",
    "### dataframe manipulations\n",
    "## cut nodes for profilometry / timeseries data\n",
    "def cutNodes(data,num_nodes,dataPath,foldername,savename):\n",
    "\n",
    "    # identify inner n nodes based on num_nodes input\n",
    "    total_nodes = len(data['nodeID'].unique())\n",
    "    cut_nodes = int((total_nodes - num_nodes) / 2)\n",
    "\n",
    "    # remove cut_nodes from dataframe\n",
    "    data = data[data['nodeID'].between((cut_nodes)+1,(num_nodes+cut_nodes))]\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'/'+foldername+'-'+savename+'.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "## will compute mean dataframe \n",
    "def computeMeanData(dataPath,data,outname):\n",
    "\n",
    "    # make mean data frame\n",
    "    data_mean =  data.groupby(['subjectID','classID','structureID']).mean().reset_index()\n",
    "    data_mean['nodeID'] = [ 1 for f in range(len(data_mean['nodeID'])) ]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data_mean.to_csv(dataPath+outname+'.csv',index=False)\n",
    "\n",
    "    return data_mean\n",
    "\n",
    "### scripts related to outlier detection and reference dataframe generation\n",
    "## this function will compute distance measures from input data and reference data\n",
    "def computeDistance(data,references_data,measures,metric):\n",
    "\n",
    "    # imports important distance functions\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    # if distance metric desired is euclidean distance (i.e. for profiles), computes distance of profile from reference profile. else, just computes difference using emd\n",
    "    if metric == 'euclidean':\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: euclidean_distances([x[measures].values.tolist(),references_data[measures].values.tolist()])[0][1]).values\n",
    "    else:\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: wasserstein_distance(x[measures],[references_data[measures].values[0]]))\n",
    "\n",
    "    return dist\n",
    "\n",
    "## this function will compute simple average references for a given input data\n",
    "def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "    # computes mean and sd of the measures in a dataframe\n",
    "    references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "    references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "    references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "    return references_mean, references_sd\n",
    "\n",
    "## this function calls computeReferences and computeDistances to create a dataframe of distance measures\n",
    "def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "    # set up output lists that we will append to\n",
    "    dist = []\n",
    "    subj = []\n",
    "    meas = []\n",
    "    struc = []\n",
    "\n",
    "    # loop through appropriate structures\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        # set data for a given structure\n",
    "        subj_data = data.loc[data['structureID'] == i]\n",
    "        # compute reference for given structure\n",
    "        references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "        # loop through measures and compute distance from reference\n",
    "        for m in measures:\n",
    "            if dist_metric == 'euclidean':\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'euclidean'))\n",
    "            else:\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'emd'))\n",
    "            \n",
    "            # append data to appropriate lists\n",
    "            subj = np.append(subj,subj_data.subjectID.unique().tolist())\n",
    "            meas = np.append(meas,[ m for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "            struc = np.append(struc,[ i for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "\n",
    "    # create distance dataframe\n",
    "    dist_dataframe = pd.DataFrame()\n",
    "    dist_dataframe['subjectID'] = subj\n",
    "    dist_dataframe['structureID'] = struc\n",
    "    dist_dataframe['measures'] = meas\n",
    "    dist_dataframe['distance'] = dist\n",
    "    \n",
    "    return dist_dataframe\n",
    "\n",
    "## this function is useful for saving reference.jsons for a given structure\n",
    "def outputReferenceJson(ref_data,measures,profile,resample_points,sourceID,data_dir,filename):\n",
    "    \n",
    "    # load module for resampling profile data for generating reference.json\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    # loop through structures in dataframe\n",
    "    for st in ref_data.structureID.unique():\n",
    "        # set up important measures\n",
    "        reference_json = []\n",
    "        tmp = {}\n",
    "        tmp['structurename'] = st\n",
    "        tmp['source'] = sourceID\n",
    "        # loop through measures\n",
    "        for meas in measures:\n",
    "            # grab data\n",
    "            tmp[meas] = {}\n",
    "            if profile:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][['nodeID',meas]].dropna().groupby('nodeID')[meas]\n",
    "            else:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][[meas]].dropna()[meas]\n",
    "            \n",
    "            # if resample_points is a value, resamples references and computes multiple summary measures.\n",
    "            # else, will just output the entire data. this second option is really only useful for non-profile/series data\n",
    "            data_tmp = []\n",
    "            if resample_points:                \n",
    "                mean_tmp = resample(gb_frame.mean().values.tolist(),resample_points).tolist()\n",
    "                min_tmp = resample(gb_frame.min().values.tolist(),resample_points).tolist()\n",
    "                max_tmp = resample(gb_frame.max().values.tolist(),resample_points).tolist()\n",
    "                sd_tmp = resample(gb_frame.std().values.tolist(),resample_points).tolist()\n",
    "                five_tmp = resample(gb_frame.quantile(q=.05).values.tolist(),resample_points).tolist()\n",
    "                twofive_tmp = resample(gb_frame.quantile(q=.25).values.tolist(),resample_points).tolist()\n",
    "                sevenfive_tmp = resample(gb_frame.quantile(q=.75).values.tolist(),resample_points).tolist()\n",
    "                ninefive_tmp = resample(gb_frame.quantile(q=.95).values.tolist(),resample_points).tolist()\n",
    "                tmp[meas]['mean'] = mean_tmp\n",
    "                tmp[meas]['min'] = min_tmp\n",
    "                tmp[meas]['max'] = max_tmp\n",
    "                tmp[meas]['sd'] = sd_tmp\n",
    "                tmp[meas]['5_percentile'] = five_tmp\n",
    "                tmp[meas]['25_percentile'] = twofive_tmp\n",
    "                tmp[meas]['75_percentile'] = sevenfive_tmp\n",
    "                tmp[meas]['95_percentile'] = ninefive_tmp\n",
    "            else:\n",
    "                data_tmp = gb_frame.values.tolist()\n",
    "                tmp[meas]['data'] = data_tmp\n",
    "        reference_json.append(tmp)\n",
    "\n",
    "        with open(data_dir+'/'+filename+'_'+st+'.json','w') as ref_out_f:\n",
    "            json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "    return reference_json\n",
    "\n",
    "## this function is used to build the reference dataset removing any subjects identified as outliers. the dataframe may or may not be useful\n",
    "def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "    \n",
    "    # set up dataframe\n",
    "    reference_data = pd.DataFrame()\n",
    "\n",
    "    # loop through structures and measures and set data\n",
    "    for s in outliers.structureID.unique():\n",
    "        for m in outliers.measures.unique():\n",
    "            if profile:\n",
    "                meas = ['structureID','subjectID','nodeID',m]\n",
    "            else:\n",
    "                meas = ['structureID','subjectID',m]\n",
    "            tmpdata = data[(data[\"structureID\"] == s) & (~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique()))][meas].reset_index(drop=True)\n",
    "            reference_data = pd.concat([reference_data,tmpdata])\n",
    "    # if not profile, will compute average\n",
    "    if not profile:\n",
    "        reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "\n",
    "    reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "\n",
    "    return reference_data\n",
    "\n",
    "## this function will identify if a given subjects' data is an outlier based on distance from reference and a threshold percentage\n",
    "def computeOutliers(distances,threshold):\n",
    "    \n",
    "    # set up dataframe\n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    # loop through structureas and measures and identify outliers based on thereshold distance\n",
    "    for i in distances.structureID.unique():\n",
    "        for m in distances.measures.unique():\n",
    "            tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "            outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "    return outliers\n",
    "\n",
    "## this function calls computeOutliers, createDistanceDataframe, and buildReferenceData and outputReferenceJson to actually generate the outliers\n",
    "## and final reference datasets\n",
    "def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,profile,resample_points,sourceID,data_dir,filename):\n",
    "    \n",
    "    # set up important lists\n",
    "    outliers_subjects = []\n",
    "    outliers_structures = []\n",
    "    outliers_measures = []\n",
    "    outliers_metrics = []\n",
    "\n",
    "    # compute distances and identify outliers\n",
    "    distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "    outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "    # if building references, build the reference data. otherwise, output a blank array\n",
    "    if build_outliers:\n",
    "        reference_dataframe = buildReferenceData(data,outliers_dataframe,profile,data_dir,filename)\n",
    "        reference_json = outputReferenceJson(reference_dataframe,measures,profile,resample_points,sourceID,data_dir,filename)\n",
    "    else:\n",
    "        reference_dataframe = []\n",
    "        reference_json = []\n",
    "        \n",
    "    return distances, outliers_dataframe, reference_dataframe, reference_json\n",
    "\n",
    "## this function is useful in identifying subjects who may have had a flipped profile as compared to a reference profile.\n",
    "## calls computeReferences, computeDistance.\n",
    "## logic: if the distance of a subject's tract profile from a reference profile is positive and greater than a threshold percentage, then it's likely\n",
    "## the data has been flipped. needs more work for all use cases. works well for easy examples like uncinate\n",
    "def profileFlipCheck(data,subjects,structures,test_measure,flip_measures,dist_metric,threshold,outPath):\n",
    "    \n",
    "    # set up important lists\n",
    "    flipped_subjects = []\n",
    "    flipped_structures = []\n",
    "    distance = []\n",
    "    flipped_distance = []\n",
    "\n",
    "    # loop through structures\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        # set data frame for structure, including a copy that has the test_measure flipped\n",
    "        struc_data = data.loc[data['structureID'] == i]\n",
    "        flipped_struc_data = struc_data.copy()\n",
    "        flipped_struc_data[test_measure] = flipped_struc_data.groupby('subjectID',sort=False).apply(lambda x: np.flip(x['fa'])).tolist()\n",
    "\n",
    "        # build reference data\n",
    "        references_data = computeReferences(struc_data,'nodeID','nodeID',flip_measures)\n",
    "\n",
    "        # compute distances for normal data and for flipped. then compute difference\n",
    "        dist = computeDistance(struc_data,references_data[0],test_measure,dist_metric)\n",
    "        dist_flipped = computeDistance(flipped_struc_data,references_data[0],test_measure,dist_metric)\n",
    "        differences = dist - dist_flipped\n",
    "\n",
    "        # identify threshold of distances based on percentile. identify those that have positive differences and are greater than the threshold.\n",
    "        # if so, append information\n",
    "        percentile_threshold = np.percentile(differences,threshold)\n",
    "        for m in range(len(differences)):\n",
    "            if differences[m] > 0 and differences[m] > percentile_threshold:\n",
    "                flipped_subjects = np.append(flipped_subjects,subjects[m])\n",
    "                flipped_structures = np.append(flipped_structures,i)\n",
    "                distance = np.append(distance,dist[m])\n",
    "                flipped_distance = np.append(flipped_distance,dist_flipped[m])\n",
    "\n",
    "    # generate ouput dataframe containing flipped subject data\n",
    "    output_summary = pd.DataFrame()\n",
    "    output_summary['flipped_subjects'] = flipped_subjects\n",
    "    output_summary['flipped_structures'] = flipped_structures\n",
    "    output_summary['distance'] = distance\n",
    "    output_summary['flipped_distance'] = flipped_distance\n",
    "\n",
    "    if outPath:\n",
    "        output_summary.to_csv(outPath+'_flipped_profiles.csv',index=False)\n",
    "        \n",
    "# this function will merge the structural and diffusion data for the reference datasets\n",
    "def mergeStructuralDiffusionJson(data,structuralPath,diffusionPath,outPath):\n",
    "    for i in data.structureID.unique():\n",
    "        print(i)\n",
    "        with open(structuralPath+'_'+i+'.json','r') as structural_f:\n",
    "            structural = json.load(structural_f)\n",
    "\n",
    "        with open(diffusionPath+'_'+i+'.json','r') as diffusion_f:\n",
    "            diffusion = json.load(diffusion_f)\n",
    "\n",
    "        merged = {**structural[0],**diffusion[0]}\n",
    "\n",
    "        with open(outPath+'_'+i+'.json','w') as out_f:\n",
    "            json.dump(merged,out_f)\n",
    "\n",
    "### adjacency-matrix related fuctions for computing network values locally\n",
    "def binarizeMatrices(data):\n",
    "    \n",
    "    # use brain connectivity toolbox to binarize data\n",
    "    bin_data = [ bct.utils.binarize(data[f]) for f in data.keys() ]\n",
    "    \n",
    "    return bin_data\n",
    "\n",
    "def thresholdMatrices(data,bin_data,thresholdPercentageSubjects):\n",
    "    \n",
    "    # compute sum of matrices\n",
    "    sum_mat = np.zeros(np.shape(bin_data)[1:])\n",
    "    \n",
    "    for i in bin_data:\n",
    "        sum_mat = sum_mat + i\n",
    "        \n",
    "    # compute threshold value\n",
    "    thrs = thresholdPercentageSubjects*len(bin_data)\n",
    "    \n",
    "    # loop through data and make those nodes that dont meet threshold 0\n",
    "    for i in data.keys():\n",
    "        data[i][sum_mat < thrs] = 0\n",
    "        \n",
    "    return data\n",
    "\n",
    "def computeMeanNetworkConnectivity(data,networks,indices,out_path):\n",
    "\n",
    "    mean_data = []\n",
    "    subs = []\n",
    "    nets = []\n",
    "\n",
    "    for l in data.keys():\n",
    "        for n in networks:\n",
    "            tmpdata = []\n",
    "            subs = np.append(subs,l)\n",
    "            nets = np.append(nets,n)\n",
    "            for i in indices[n]:\n",
    "                for j in indices[n]:\n",
    "                    if i != j:\n",
    "                        tmpdata = np.append(tmpdata,data[l][int(i)][int(j)])\n",
    "            mean_data = np.append(mean_data,np.mean(tmpdata))\n",
    "\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df['subjectID'] = [ f.split('_sess1')[0] for f in subs ]\n",
    "    out_df['FC'] = mean_data\n",
    "    out_df = pd.merge(out_df,subjects_data,on='subjectID')\n",
    "    out_df['structureID'] = nets\n",
    "\n",
    "    if out_path:\n",
    "        out_df.to_csv(out_path,index=False)\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "### scatter plot related scripts\n",
    "# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure\n",
    "def averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X = x_stat.groupby(measure).mean()[x_measure].tolist()\n",
    "    Y = y_stat.groupby(measure).mean()[y_measure].tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure\n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X,Y = [np.array([]),np.array([])]\n",
    "    for i in range(len(x_stat[measure].unique())):\n",
    "        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]\n",
    "        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]\n",
    "\n",
    "        if np.isnan(x).any() or np.isnan(y).any():\n",
    "            print(\"skipping %s due to nan\" %x_stat[measure].unique()[i])\n",
    "        else:\n",
    "            # checks to make sure the same data\n",
    "            if len(x) == len(y):\n",
    "                X = np.append(X,x)\n",
    "                Y = np.append(Y,y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject\n",
    "def ravelNetwork(x_stat,y_stat):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    X = np.ravel(x_stat).tolist()\n",
    "    Y = np.ravel(y_stat).tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel. \n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def ravelNonNetwork(x_stat,y_stat,x_measure,y_measure):\n",
    "\n",
    "    X = x_stat[x_measure].to_list()\n",
    "    Y = y_stat[y_measure].to_list()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# wrapper function to call either of the above scripts based on user input\n",
    "def setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,isnetwork,measure):\n",
    "\n",
    "    x_stat = x_data\n",
    "    y_stat = y_data\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        X,Y = averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'append':\n",
    "        X,Y = appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'ravel':\n",
    "        if isnetwork == True:\n",
    "            X,Y = ravelNetwork(x_stat,y_stat)\n",
    "        else:\n",
    "            X,Y = ravelNonNetwork(x_stat,y_stat,x_measure,y_measure)\n",
    "\n",
    "    return x_stat,y_stat,X,Y\n",
    "\n",
    "# function to shuffle data and colors\n",
    "def shuffleDataAlg(X,Y,hues):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    X,Y,hues = shuffle(X,Y,hues)\n",
    "\n",
    "    return X,Y,hues\n",
    "\n",
    "# simple display or figure save function\n",
    "def saveOrShowImg(dir_out,x_measure,y_measure,img_name):\n",
    "    import os,sys \n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    " \n",
    "        # save or show plot\n",
    "        if dir_out:\n",
    "            if not os.path.exists(dir_out):\n",
    "                os.mkdir(dir_out)\n",
    "\n",
    "            if x_measure == y_measure:\n",
    "                img_name_eps = img_name+'_'+x_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'.svg'\n",
    "            else:\n",
    "                img_name_eps = img_name+'_'+x_measure+'_vs_'+y_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'_vs_'+y_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'_vs_'+y_measure+'.svg'\n",
    "\n",
    "            plt.savefig(os.path.join(dir_out, img_name_eps),transparent=True)\n",
    "            plt.savefig(os.path.join(dir_out, img_name_png))     \n",
    "    #         plt.savefig(os.path.join(dir_out, img_name_svg))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results\n",
    "# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def relplotScatter(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,column_wrap,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data: CANNOT BE AVERAGE\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,'ravel',False,hue_measure)\n",
    "\n",
    "    p = sns.relplot(x=X,y=Y,col=x_stat[column_measure],hue=x_stat[hue_measure],kind=\"scatter\",s=100,col_wrap=column_wrap)\n",
    "\n",
    "    # setting counter. looping through axes to add important info and regression lines\n",
    "    i = 0\n",
    "    for ax in p.axes.flat:\n",
    "        x_lim,y_lim = [ax.get_xlim(),ax.get_ylim()]\n",
    "\n",
    "        if trendline == 'equality':\n",
    "            ax.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        elif trendline == 'linreg':\n",
    "            m,b = np.polyfit(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'],1)\n",
    "            ax.plot(ax.get_xticks(),m*ax.get_xticks() + b)\n",
    "            plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(x_measure)\n",
    "        ax.set_ylabel(y_measure)\n",
    "\n",
    "        # compute correlation for each subject and add to plots\n",
    "        corr = np.corrcoef(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'])[1][0]\n",
    "        plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # compute rmse for each subject and add to plots\n",
    "        rmse = np.sqrt(mean_squared_error(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y']))\n",
    "        plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # update counter\n",
    "        i = i+1\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def singleplotScatter(colors_dict,x_data,y_data,x_measure,y_measure,logX,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,colorDistance,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "    \n",
    "    if colorDistance:\n",
    "        category = colorDistanceScatter(X,Y)\n",
    "    else:\n",
    "        colors = sns.color_palette('colorblind',len(x_stat[hue_measure]))\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    if logX == True:\n",
    "        X = np.log10(X)\n",
    "\n",
    "    if colors_dict:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    elif colorDistance:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=category,hue_order=['two-sd','one-sd','lt-one-sd'],palette=['red','green','blue'],s=100)\n",
    "    else:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100)\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    if x_measure == y_measure:\n",
    "        p.axes.axis('square')\n",
    "        y_ticks = p.axes.get_yticks()\n",
    "        p.axes.set_xticks(y_ticks)\n",
    "        p.axes.set_yticks(p.axes.get_xticks())\n",
    "        p.axes.set_ylim(p.axes.get_xlim())\n",
    "        p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        ax = plt.gca()\n",
    "        ax.get_legend().remove()\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "        ax = plt.gca()\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    elif trendline == 'groupreg':\n",
    "        for g in range(len(groups)):\n",
    "            if stat_name == 'volume':\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            else:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "    \n",
    "def colorDistanceScatter(x,y):\n",
    "    \n",
    "    # set output variable (category) and compute ratio and difference from 1 (i.e. perfect ratio)\n",
    "    category = []\n",
    "    ratio = [ x[i] / y[i] if y[i] > 0 else 5 for i in range(len(x)) ]\n",
    "    diff = [ np.abs(ratio[i] - 1) for i in range(len(ratio)) ]\n",
    "    \n",
    "    # compute standard deviation thresholds for difference values\n",
    "    one_sd = np.std(diff)\n",
    "    two_sd = one_sd * 2\n",
    "    \n",
    "    # loop through each data point and determine category (one sd: within 1 and 2 sds, two-sd: greater or equal to 2 sds, lt-one-sd: within one sd)\n",
    "    for i in diff:\n",
    "        if i >= two_sd:\n",
    "            category = np.append(category,'two-sd')\n",
    "        elif one_sd <= i < two_sd:\n",
    "            category = np.append(category,'one-sd')\n",
    "        else:\n",
    "            category = np.append(category,'lt-one-sd')\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up variables\n",
      "setting up variables complete\n",
      "grabbing demographic data\n",
      "grabbing demographic data complete\n"
     ]
    }
   ],
   "source": [
    "### setting up variables and adding paths\n",
    "print(\"setting up variables\")\n",
    "topPath = \"./\"\n",
    "os.chdir(topPath)\n",
    "data_dir = topPath+'/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "img_dir = topPath+'/img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "groups = ['hcp']\n",
    "colors_array = ['blue']\n",
    "diff_micro_measures = ['ad','fa','md','rd','ndi','odi','isovf']\n",
    "print(\"setting up variables complete\")\n",
    "\n",
    "### grabbing subjects demographic data\n",
    "print(\"grabbing demographic data\")\n",
    "subjects_data = pd.read_csv('./subjects_data.csv')\n",
    "print(\"grabbing demographic data complete\")\n",
    "\n",
    "if subjects_data['subjectID'].dtype == 'int':\n",
    "    subjects_data['subjectID'] = [ str(int(np.float(f))) for f in subjects_data.subjectID ]\n",
    "    \n",
    "subjects_data['classID'] = [ 'hcp' for f in subjects_data['subjectID'] ]\n",
    "subjects_data['colors'] = [ 'blue' for f in subjects_data['subjectID'] ]\n",
    "\n",
    "colors = {}\n",
    "subjects = {}\n",
    "\n",
    "# loop through groups and identify subjects and set color schema for each group\n",
    "for g in range(len(groups)):\n",
    "    # set subjects array\n",
    "    subjects[groups[g]] =  subjects_data['subjectID']\n",
    "    subjects[groups[g]].sort_values()\n",
    "    \n",
    "    # set colors array\n",
    "    colors_name = colors_array[g]\n",
    "    colors[groups[g]] = colors_array[g]\n",
    "\n",
    "# create subjects color dictionary\n",
    "colors_dict = createColorDictionary(subjects_data,'subjectID','colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### structural and diffusion data in cortical and subcortical structures\n",
    "print(\"computing gray matter parcel analyses\")\n",
    "\n",
    "# create data structures\n",
    "# cortical measures - glasser\n",
    "if os.path.isfile(data_dir+'/cortical_glasser.csv'):\n",
    "    cortical_glasser = pd.read_csv(data_dir+'/cortical_glasser.csv')\n",
    "else:\n",
    "    cortical_glasser =  collectData('neuro/parc-stats',['cortex_mapping_stats'],['glasser'],'parc_MEAN.csv',data_dir+'/cortical_glasser.csv')\n",
    "    cortical_glasser = cortical_glasser.drop_duplicates()\n",
    "    cortical_glasser['subjectID'] = cortical_glasser.subjectID.astype(str)\n",
    "    cortical_glasser = pd.merge(cortical_glasser,subjects_data,on='subjectID')\n",
    "    cortical_glasser.to_csv(data_dir+'/cortical_glasser.csv',index=False)\n",
    "\n",
    "# replace vertex volume measure with overall parcel volume\n",
    "if os.path.isfile(data_dir+'/cortical_glasser_parcel_volume.csv'):\n",
    "    cortical_glasser_parcel_volume = pd.read_csv(data_dir+'/cortical_glasser_parcel_volume.csv')\n",
    "else:\n",
    "    cortical_glasser_parcel_volume = collectData('neuro/parc-stats',['surface'],['glasser'],'cortex.csv',data_dir+'/cortical_glasser_parcel_volume.csv')\n",
    "    cortical_glasser_parcel_volume = cortical_glasser_parcel_volume.drop_duplicates()\n",
    "    cortical_glasser_parcel_volume['subjectID'] = cortical_glasser_parcel_volume.subjectID.astype(str)\n",
    "    cortical_glasser_parcel_volume = cortical_glasser_parcel_volume[~cortical_glasser_parcel_volume['structureID'].isin([\"lh_unknown_0\",\"rh_unknown_0\",\"lh_???\",\"rh_???\"])]\n",
    "    cortical_glasser_parcel_volume['structureID'] = [ f.split('h_')[1] for f in cortical_glasser_parcel_volume['structureID'] ]\n",
    "    cortical_glasser_parcel_volume.to_csv(data_dir+'/cortical_glasser_parcel_volume.csv',index=False)\n",
    "\n",
    "cortical_glasser.drop(columns=['volume','thickness'],inplace=True)\n",
    "cortical_glasser = pd.merge(cortical_glasser,cortical_glasser_parcel_volume,on=['subjectID','structureID','nodeID','sessionID'])\n",
    "\n",
    "# cortical measures -  aparc.a2009s\n",
    "if os.path.isfile(data_dir+'/cortical_aparc.csv'):\n",
    "    cortical_aparc = pd.read_csv(data_dir+'/cortical_aparc.csv')\n",
    "else:\n",
    "    cortical_aparc =  collectData('neuro/parc-stats',['cortex_mapping_stats'],['glasser'],'aparc_MEAN.csv',data_dir+'/cortical_aparc.csv')\n",
    "    cortical_aparc = cortical_aparc.drop_duplicates()\n",
    "    cortical_aparc['subjectID'] = cortical_aparc.subjectID.astype(str)\n",
    "    cortical_aparc = pd.merge(cortical_aparc,subjects_data,on='subjectID')\n",
    "    cortical_aparc.to_csv(data_dir+'/cortical_aparc.csv',index=False)\n",
    "\n",
    "# freesurfer measures: aparc\n",
    "if os.path.isfile(data_dir+'/cortical_aparc_parcel_volume.csv'):\n",
    "    cortical_aparc_parcel_volume = pd.read_csv(data_dir+'/cortical_aparc_parcel_volume.csv')\n",
    "else:\n",
    "    cortical_aparc_parcel_volume = collectData('neuro/parc-stats',['freesurfer'],['aparc.a2009s'],'cortex.csv',data_dir+'/cortical_aparc_parcel_volume.csv')\n",
    "    cortical_aparc_parcel_volume = cortical_aparc_parcel_volume.drop_duplicates()\n",
    "    cortical_aparc_parcel_volume['subjectID'] = cortical_aparc_parcel_volume.subjectID.astype(str)\n",
    "    cortical_aparc_parcel_volume.to_csv(data_dir+'/cortical_aparc_parcel_volume.csv',index=False)\n",
    "\n",
    "cortical_aparc.drop(columns=['volume','thickness'],inplace=True)\n",
    "cortical_aparc = pd.merge(cortical_aparc,cortical_aparc_parcel_volume,on=['subjectID','structureID','nodeID','sessionID'])\n",
    "\n",
    "# concatenate glasser and aparc\n",
    "if os.path.isfile(data_dir+'/cortical.csv'):\n",
    "    cortical = pd.read_csv(data_dir+'/cortical.csv')\n",
    "else:\n",
    "    cortical_glasser['parcellation'] = [ 'glasser' for f in cortical_glasser['subjectID']]\n",
    "    cortical_aparc['parcellation'] = [ 'aparc.a2009s' for f in cortical_aparc['subjectID']]\n",
    "    cortical = pd.concat([cortical_glasser,cortical_aparc])\n",
    "    cortical.to_csv(data_dir+'/cortical.csv',index=False)\n",
    "\n",
    "# # # subcortical measures\n",
    "if os.path.isfile(data_dir+'/subcortical.csv'):\n",
    "    subcortical = pd.read_csv(data_dir+'/subcortical.csv')\n",
    "else:\n",
    "    subcortical =  collectData('neuro/parc-stats',['subcort_stats'],['tensor'],'aseg_nodes.csv',data_dir+'/subcortical.csv')\n",
    "    subcortical = subcortical.drop_duplicates()\n",
    "    subcortical['subjectID'] = subcortical.subjectID.astype(str)\n",
    "    subcortical = pd.merge(subcortical,subjects_data,on='subjectID')\n",
    "    subcortical.to_csv('subcortical.csv',index=False)\n",
    "\n",
    "# # clean and remove data that is outside physical bounds: cortical\n",
    "if os.path.isfile(data_dir+'/cortical_cleaned.csv'):\n",
    "    cortical = pd.read_csv(data_dir+'/cortical_cleaned.csv')\n",
    "else:\n",
    "    bad_cortical = pd.DataFrame()\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['ad'] > 3]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['ad'] < 0]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['fa'] > 1]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['fa'] < 0]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['md'] > 3]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['md'] < 0]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['rd'] > 3]])\n",
    "    bad_cortical = pd.concat([bad_cortical,cortical.loc[cortical['rd'] < 0]])\n",
    "    cortical = cortical.loc[~cortical.index.isin(bad_cortical.index)]\n",
    "    cortical.to_csv(data_dir+'/cortical_cleaned.csv',index=False)\n",
    "\n",
    "# # clean and remove data that is outside physical bounds: subcortical\n",
    "if os.path.isfile(data_dir+'/subcortical_cleaned.csv'):\n",
    "    subcortical = pd.read_csv(data_dir+'/subcortical_cleaned.csv')\n",
    "else:\n",
    "    bad_subcortical = pd.DataFrame()\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] > 1]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] < 0]])\n",
    "    subcortical = subcortical.loc[~subcortical.index.isin(bad_subcortical.index)]\n",
    "    subcortical.to_csv(data_dir+'/subcortical_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### diffusion snr\n",
    "# diffusion snr data\n",
    "if os.path.isfile(data_dir+'/diffusion-callosal-snr.csv'):\n",
    "    diffusion_snr_data = pd.read_csv(data_dir+'/diffusion-callosal-snr.csv')\n",
    "else:\n",
    "    diffusion_snr_data = collectData('neuro/snr-stats',['preprocessed'],['dwi_snr'],'snr.csv',data_dir+'/diffusion-callosal-snr.csv')\n",
    "    diffusion_snr_data['subjectID'] = diffusion_snr_data.subjectID.astype(str)\n",
    "    diffusion_snr_data = diffusion_snr_data.drop_duplicates()\n",
    "    diffusion_snr_data.rename(columns={'volumes': 'structureID'},inplace=True)\n",
    "    diffusion_snr_data.to_csv(data_dir+'/diffusion-callosal-snr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### structural and diffusion data in white matter tracts\n",
    "print(\"computing tractometry analyses\")\n",
    "\n",
    "# create data structures\n",
    "# tractmeasures data\n",
    "if os.path.isfile(data_dir+'/tractmeasures-total_nodes.csv'):\n",
    "    track_data = pd.read_csv(data_dir+'/tractmeasures-total_nodes.csv')\n",
    "else:\n",
    "    track_data = collectData('neuro/tractmeasures',['macro_micro'],['full_macro_stats'],'tractmeasures.csv',data_dir+'/tractmeasures-total_nodes.csv')\n",
    "    track_data['subjectID'] = track_data.subjectID.astype(str)\n",
    "    track_data = track_data.drop_duplicates()\n",
    "    track_data = track_data.loc[track_data['structureID'] != 'wbfg' ]\n",
    "    track_data.to_csv(data_dir+'/tractmeasures-total_nodes.csv',index=False)\n",
    "    \n",
    "# remove subjects with missing data\n",
    "subjs_with_nans = track_data[track_data.isna()['fa']].subjectID.unique()\n",
    "\n",
    "# check for missing individual tracts\n",
    "subjs_missing_tracts = [ f for f in track_data.subjectID.unique().tolist() if len(track_data[track_data['subjectID'] == f].structureID.unique()) < 61 ]\n",
    "subjs_missing_data = [*subjs_with_nans,*subjs_missing_tracts]\n",
    "\n",
    "# select only subjects that don't have missing data\n",
    "track_data = track_data.loc[~track_data['subjectID'].isin(subjs_missing_data)]\n",
    "\n",
    "# # merge subjects_data\n",
    "if os.path.isfile(data_dir+'/tractmeasures-total_nodes_merged.csv'):\n",
    "    track_data = pd.read_csv(data_dir+'/tractmeasures-total_nodes_merged.csv')\n",
    "    track_data = track_data.drop_duplicates()\n",
    "else:\n",
    "    track_data = pd.merge(track_data,subjects_data,on='subjectID')\n",
    "    track_data.to_csv(data_dir+'/tractmeasures-total_nodes_merged.csv')\n",
    "\n",
    "# # cut nodes\n",
    "if os.path.isfile(data_dir+'/tractmeasures-cut_nodes.csv'):\n",
    "    track_data_cut = pd.read_csv(data_dir+'/tractmeasures-cut_nodes.csv')\n",
    "else:\n",
    "    track_data_cut = cutNodes(track_data,180,data_dir,'tractmeasures','cut_nodes')\n",
    "    track_data_cut = pd.merge(track_data_cut,subjects_data,on=['subjectID','classID','age'])\n",
    "    track_data_cut.to_csv(data_dir+'/tractmeasures-cut_nodes.csv',index=False)\n",
    "\n",
    "# # compute mean data\n",
    "if os.path.isfile(data_dir+'/tractmeasures-mean_nodes.csv'):\n",
    "    track_data_mean = pd.read_csv(data_dir+'/tractmeasures-mean_nodes.csv')\n",
    "else:\n",
    "    track_data_mean = computeMeanData(data_dir,track_data_cut,'tractmeasures-mean_nodes')\n",
    "    track_data_mean = pd.merge(track_data_mean,subjects_data,on=['subjectID','classID'])\n",
    "    track_data_mean.to_csv(data_dir+'/tractmeasures-mean_nodes.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dmri glasser structural (density) networks - max node degree vs age replication\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'_dmri_glasser_preclean_adjacency.npy'):\n",
    "    dmri_glasser_networks_adjacency = np.load(data_dir+'/'+groups[0]+'_dmri_glasser_preclean_adjacency.npy',allow_pickle=True).item()\n",
    "else:\n",
    "    dmri_glasser_networks_adjacency = collectData('generic/network',['!measurements','networkmatrices','preprocessed'],['bl_paper','ga','structural','density'],'network.json.gz',data_dir+'/'+groups[0]+'_dmri_glasser_preclean_adjacency',True)\n",
    "\n",
    "# find binarized matrices\n",
    "bin_matrix = binarizeMatrices(dmri_glasser_networks_adjacency)\n",
    "\n",
    "# clean matrices\n",
    "dmri_glasser_networks_adjacency_cleaned = thresholdMatrices(dmri_glasser_networks_adjacency,bin_matrix,0.5)\n",
    "\n",
    "# compute node degree for each subject\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes = pd.DataFrame()\n",
    "subs = []\n",
    "degree = []\n",
    "nodes = []\n",
    "for i in dmri_glasser_networks_adjacency_cleaned.keys():\n",
    "    subs = np.append(subs,[ i.split('_')[0] for f in range(len(dmri_glasser_networks_adjacency_cleaned[i])) ])\n",
    "    nodes = np.append(nodes,[ str(f) for f in range(len(dmri_glasser_networks_adjacency_cleaned[i])) ])\n",
    "    degree = np.append(degree,bct.degrees_und(dmri_glasser_networks_adjacency_cleaned[i]))\n",
    "\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes['subjectID'] = subs\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes['node_degree'] = degree\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes['node'] = nodes\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes = pd.merge(dmri_glasser_networks_adjacency_cleaned_nodes,subjects_data.loc[subjects_data['subjectID'].isin(dmri_glasser_networks_adjacency_cleaned_nodes['subjectID'].unique())][['subjectID','age']],on='subjectID')\n",
    "\n",
    "bins = [18, 28, 38]\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes['bin'] = pd.cut(dmri_glasser_networks_adjacency_cleaned_nodes['age'],bins)\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes = dmri_glasser_networks_adjacency_cleaned_nodes.loc[~dmri_glasser_networks_adjacency_cleaned_nodes['node'].isin(['0','181'])]\n",
    "\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes_max = dmri_glasser_networks_adjacency_cleaned_nodes.groupby('subjectID').apply(lambda x: np.max(x['node_degree'])).reset_index()\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes_max.rename(columns={0: \"degree\"},inplace=True)\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes_max = pd.merge(dmri_glasser_networks_adjacency_cleaned_nodes_max,dmri_glasser_networks_adjacency_cleaned_nodes[['subjectID','age','bin']],on='subjectID')\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes_max['classID'] = [ groups[0] for f in range(len(dmri_glasser_networks_adjacency_cleaned_nodes_max)) ]\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes_max = dmri_glasser_networks_adjacency_cleaned_nodes_max.drop_duplicates()\n",
    "dmri_glasser_networks_adjacency_cleaned_nodes_max.to_csv(groups[0]+'-max-node-degree.csv',index=False)\n",
    "\n",
    "sns.heatmap(np.nan_to_num(np.log10(dmri_glasser_networks_adjacency_cleaned['103818_sess1']),neginf=-7),cmap='hot',vmin=-7,vmax=0)\n",
    "plt.savefig('103818_density_net.eps',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fmri yeo networks - max node degree vs age replication\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency.npy'):\n",
    "    fmri_yeo_networks_adjacency = np.load(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency.npy',allow_pickle=True).item()\n",
    "else:\n",
    "    fmri_yeo_networks_adjacency = collectData('generic/network',[\"time series\"],['yeo-mni','final'],'network.json.gz',data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency',True)\n",
    "\n",
    "# grab label indices\n",
    "networks = ['VisCent','VisPeri','SomMotA','SomMotB','DorsAttnA','DorsAttnB','SalVentAttnA','SalVentAttnB','Limbic','ContA','ContB','ContC','DefaultA','DefaultB','DefaultC']\n",
    "with open('label.json','r') as label_f:\n",
    "    labels = json.load(label_f)\n",
    "\n",
    "indices = {}\n",
    "for n in range(len(networks)):\n",
    "    indices[networks[n]] = []\n",
    "    for i in range(len(labels)):\n",
    "        netname = labels[i]['name']\n",
    "        if networks[n] in netname:\n",
    "                indices[networks[n]] = np.append(indices[networks[n]],int(i))\n",
    "\n",
    "# create average matrix\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency_mean.npy'):\n",
    "    fmri_yeo_networks_adjacency_mean = np.load(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency_mean.npy')\n",
    "else:\n",
    "    fmri_yeo_networks_adjacency_mean = fmri_yeo_networks_adjacency[list(fmri_yeo_networks.keys())[0]]\n",
    "\n",
    "    for i in fmri_yeo_networks_adjacency.keys():\n",
    "        fmri_yeo_networks_adjacency_mean = (fmri_yeo_networks_adjacency_mean + fmri_yeo_networks_adjacency[i]) / 2\n",
    "    np.save(data_dir+'/'+groups[0]+'_fmri_yeo_preclean_adjacency_mean.npy',fmri_yeo_networks_adjacency_mean)\n",
    "    \n",
    "# compute average functional connectivity (within network) for each network in yeo17\n",
    "fmri_yeo_networks_adjacency_mean_fc = computeMeanNetworkConnectivity(fmri_yeo_networks_adjacency,networks,indices,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating references for subcortical cortical and tractmeasures data\n",
      "anterioFrontalCC\n",
      "forcepsMajor\n",
      "forcepsMinor\n",
      "leftAnterioFrontoCerebellar\n",
      "leftArc\n",
      "leftAslant\n",
      "leftCST\n",
      "leftContraAnterioFrontoCerebellar\n",
      "leftContraMotorCerebellar\n",
      "leftIFOF\n",
      "leftILF\n",
      "leftMDLFang\n",
      "leftMDLFspl\n",
      "leftMotorCerebellar\n",
      "leftOccipitoCerebellar\n",
      "leftParietoCerebellar\n",
      "leftSLF1And2\n",
      "leftSLF3\n",
      "leftTPC\n",
      "leftThalamicoCerebellar\n",
      "leftUncinate\n",
      "leftVOF\n",
      "leftbaum\n",
      "leftcingulum\n",
      "leftfrontoThalamic\n",
      "leftmeyer\n",
      "leftmotorThalamic\n",
      "leftpArc\n",
      "leftparietoThalamic\n",
      "leftspinoThalamic\n",
      "lefttemporoThalamic\n",
      "middleFrontalCC\n",
      "parietalCC\n",
      "rightAnterioFrontoCerebellar\n",
      "rightArc\n",
      "rightAslant\n",
      "rightCST\n",
      "rightContraAnterioFrontoCerebellar\n",
      "rightContraMotorCerebellar\n",
      "rightIFOF\n",
      "rightILF\n",
      "rightMDLFang\n",
      "rightMDLFspl\n",
      "rightMotorCerebellar\n",
      "rightOccipitoCerebellar\n",
      "rightParietoCerebellar\n",
      "rightSLF1And2\n",
      "rightSLF3\n",
      "rightTPC\n",
      "rightThalamicoCerebellar\n",
      "rightUncinate\n",
      "rightVOF\n",
      "rightbaum\n",
      "rightcingulum\n",
      "rightfrontoThalamic\n",
      "rightmeyer\n",
      "rightmotorThalamic\n",
      "rightpArc\n",
      "rightparietoThalamic\n",
      "rightspinoThalamic\n",
      "righttemporoThalamic\n",
      "creating references complete\n"
     ]
    }
   ],
   "source": [
    "### creating reference datasets\n",
    "print(\"creating references for subcortical cortical and tractmeasures data\")\n",
    "\n",
    "if ~os.path.isdir(data_dir+'/references'):\n",
    "    os.mkdir(data_dir+'/references')\n",
    "\n",
    "# compute reference dataset for subcortical structures\n",
    "subcortical_structural_measures = ['number_of_voxels','volume']\n",
    "subcortical_structural_distances, subcortical_structural_outliers, subcortical_structural_reference_dataframe, subcortical_structural_reference_json = outlierDetection(subcortical,subcortical.structureID.unique(),'structureID',subcortical_structural_measures,95,'emd',True,False,'',groups[0],data_dir+'/references','subcortical_structural_reference_'+groups[0])\n",
    "subcortical_diffusion_measures = ['ad','fa','md','rd','ndi','odi','isovf']\n",
    "subcortical_diffusion_distances, subcortical_diffusion_outliers, subcortical_diffusion_reference_dataframe, subcortical_diffusion_reference_json = outlierDetection(subcortical,subcortical.structureID.unique(),'structureID',subcortical_diffusion_measures,95,'emd',True,False,'',groups[0],data_dir+'/references','subcortical_diffusion_reference_'+groups[0])\n",
    "\n",
    "# compute reference dataset for cortical structures\n",
    "cortical_diffusion_measures = ['ad','fa','md','rd','ndi','isovf','odi']    \n",
    "cortical_diffusion_distances, cortical_diffusion_outliers, cortical_diffusion_reference_dataframe, cortical_diffusion_reference_json = outlierDetection(cortical,cortical.structureID.unique(),'structureID',cortical_diffusion_measures,95,'emd',True,False,\"\",groups[0],data_dir+'/references','cortical_diffusion_reference_'+groups[0])\n",
    "cortical_structural_measures = ['number_of_vertices','surface_area_mm^2','gray_matter_volume_mm^3','average_thickness_mm','thickness_stddev_mm','integrated_rectified_mean_curvature_mm^-1','integrated_rectified_gaussian_curvature_mm^-2','folding_index','intrinsic_curvature_index']\n",
    "cortical_structural_distances, cortical_structural_outliers, cortical_structural_reference_dataframe, cortical_structural_reference_json = outlierDetection(cortical,cortical.structureID.unique(),'structureID',cortical_structural_measures,95,'emd',True,False,\"\",groups[0],data_dir+'/references','cortical_structural_reference_'+groups[0])\n",
    "\n",
    "# compute reference dataset for tractmeasures  \n",
    "track_structural_measures = [ f for f in track_data.keys() if f not in diff_micro_measures + subjects_data.keys().tolist() + ['structureID','sessionID','nodeID','ExponentialFitA','ExponentialFitB'] + track_data.columns[track_data.isna().any()].tolist() ]\n",
    "track_structural_distances, track_structural_outliers, track_structural_reference_dataframe, track_structural_reference_json = outlierDetection(track_data_mean,track_data_mean.structureID.unique(),'structureID',track_structural_measures,95,'emd',True,False,\"\",groups[0],data_dir+'/references','tractmeasures_structural_reference_'+groups[0])\n",
    "track_diffusion_measures = ['ad','fa','md','rd']\n",
    "track_diffusion_distances, track_diffusion_outliers, track_diffusion_reference_dataframe, track_diffusion_reference_json = outlierDetection(track_data,track_data.structureID.unique(),'nodeID',track_diffusion_measures,95,'euclidean',True,True,50,groups[0],data_dir+'/references','tractmeasures_diffusion_reference_'+groups[0]+'_tensor')\n",
    "track_diffusion_measures = ['ndi','odi','isovf']\n",
    "track_diffusion_distances, track_diffusion_outliers, track_diffusion_reference_dataframe, track_diffusion_reference_json = outlierDetection(track_data,track_data.structureID.unique(),'nodeID',track_diffusion_measures,95,'euclidean',True,True,50,groups[0],data_dir+'/references','tractmeasures_diffusion_reference_'+groups[0]+'_noddi')\n",
    "\n",
    "## compute reference dataset of fmri within network FC\n",
    "fmri_within_network_fc_measures = ['FC']\n",
    "fmri_within_network_fc_distances, fmri_within_network_fc_outliers, fmri_within_network_fc_reference_dataframe, fmri_within_network_fc_reference_json = outlierDetection(fmri_within_network_fc,subcortical.structureID.unique(),'structureID',subcortical_structural_measures,95,'emd',True,False,'',groups[0],data_dir+'/references','subcortical_structural_reference_'+groups[0])\n",
    "\n",
    "## compute reference dataset for diffusion b0 SNR - CC\n",
    "diffusion_snr_measures = ['snr']\n",
    "diffusion_snr_distances, diffusion_snr_outliers, diffusion_snr_reference_dataframe, diffusion_snr_reference_json = outlierDetection(diffusion_snr_data,['b0'],'structureID',diffusion_snr_measures,95,'emd',True,False,'',groups[0],data_dir+'/references','diffusion_snr_callosal_reference_'+groups[0])\n",
    "\n",
    "# merge reference datasets together for easier download and upload\n",
    "mergeStructuralDiffusionJson(cortical,data_dir+'/references/cortical_structural_reference_'+groups[0],data_dir+'/references/cortical_diffusion_reference_'+groups[0],data_dir+'/references/cortical_reference_'+groups[0])\n",
    "mergeStructuralDiffusionJson(subcortical,data_dir+'/references/subcortical_structural_reference_'+groups[0],data_dir+'/references/subcortical_diffusion_reference_'+groups[0],data_dir+'/references/subcortical_reference_'+groups[0])\n",
    "mergeStructuralDiffusionJson(track_data,data_dir+'/references/tractmeasures_diffusion_reference_'+groups[0]+'_tensor',data_dir+'/references/tractmeasures_diffusion_reference_'+groups[0]+'_noddi',data_dir+'/references/tractmeasures_diffusion_reference_'+groups[0])\n",
    "mergeStructuralDiffusionJson(track_data,data_dir+'/references/tractmeasures_structural_reference_'+groups[0],data_dir+'/references/tractmeasures_diffusion_reference_'+groups[0],data_dir+'/references/tractmeasures_reference_'+groups[0])\n",
    "\n",
    "print(\"creating references complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### generate figures documenting outlier detection capabilities\n",
    "## track data: example anterior frontal CC\n",
    "track_diffusion_reference_dataframe = pd.read_csv(data_dir+'/references/tractmeasures_reference_'+groups[0]+'.csv')\n",
    "track_diffusion_reference_dataframe['classID'] = [ 'abcd' for f in track_diffusion_reference_dataframe['subjectID'] ]\n",
    "track_diffusion_reference_dataframe['colors'] = [ 'green' for f in track_diffusion_reference_dataframe['subjectID'] ]\n",
    "\n",
    "# generate plot pre-outlier computation\n",
    "fig = plotProfiles(['anterioFrontalCC'],track_data,['fa'],'mean','std',\"\",\"\")\n",
    "plt.savefig('./img/anterior_frontal_cc_fa_pre_outliers_example.eps')\n",
    "plt.savefig('./img/anterior_frontal_cc_fa_pre_outliers_example.png')\n",
    "plt.close()\n",
    "\n",
    "# generate outliers and reference plot\n",
    "fig = plotProfiles(['anterioFrontalCC'],track_diffusion_reference_dataframe.loc[track_diffusion_reference_dataframe['structureID'] == 'anterioFrontalCC'],['fa'],'mean','std',\"\",\"\")\n",
    "sns.lineplot(x='nodeID',y='fa',data=track_data.loc[track_data['structureID'] == 'anterioFrontalCC'].loc[~track_data['subjectID'].isin(track_diffusion_reference_dataframe.loc[track_diffusion_reference_dataframe['structureID'] == 'anterioFrontalCC'].loc[~track_diffusion_reference_dataframe['fa'].isna()]['subjectID'].tolist())],hue='subjectID',palette=colors_dict,legend=False,linewidth=5)\n",
    "plt.savefig('./img/anterior_frontal_cc_fa_outliers_example.eps')\n",
    "plt.savefig('./img/anterior_frontal_cc_fa_outliers_example.png')\n",
    "plt.close()\n",
    "\n",
    "# generate reference plot\n",
    "fig = plotProfiles(['anterioFrontalCC'],track_diffusion_reference_dataframe.loc[track_diffusion_reference_dataframe['structureID'] == 'anterioFrontalCC'].loc[~track_diffusion_reference_dataframe['fa'].isna()],['fa'],'mean','std',img_dir,'anterior')\n",
    "plt.savefig('./img/anterior_frontal_cc_fa_reference_example.eps')\n",
    "plt.savefig('./img/anterior_frontal_cc_fa_reference_example.png')\n",
    "plt.close()\n",
    "\n",
    "## subcortical data: left hippocampus\n",
    "subcortical_structural_reference_dataframe = pd.read_csv(data_dir+'/references/subcortical_reference_'+groups[0]+'.csv')\n",
    "\n",
    "# generate distribution of volume pre outliers\n",
    "sns.violinplot(x='volume',data=subcortical.loc[subcortical['structureID'] == 'Left-Hippocampus'],scale='count',inner='points')\n",
    "plt.savefig('./img/left_hippocampus_volume_pre_outliers_example.png')\n",
    "plt.savefig('./img/left_hippocampus_volume_pre_outliers_example.eps')\n",
    "plt.close()\n",
    "\n",
    "# generate distribution of volume with outliers\n",
    "sns.violinplot(x='volume',data=subcortical_structural_reference_dataframe.loc[subcortical_structural_reference_dataframe['structureID'] == 'Left-Hippocampus'].loc[~subcortical_structural_reference_dataframe['volume'].isna()],scale='count',inner='points')\n",
    "sns.swarmplot(x='volume',data=subcortical.loc[subcortical['structureID'] == 'Left-Hippocampus'].loc[~subcortical['subjectID'].isin(subcortical_structural_reference_dataframe.loc[subcortical_structural_reference_dataframe['structureID'] == 'Left-Hippocampus'].loc[~subcortical_structural_reference_dataframe['volume'].isna()]['subjectID'].tolist())],color='red')\n",
    "plt.savefig('./img/left_hippocampus_volume_outliers_example.png')\n",
    "plt.savefig('./img/left_hippocampus_volume_outliers_example.eps')\n",
    "plt.close()\n",
    "\n",
    "# generate reference\n",
    "sns.violinplot(x='volume',data=subcortical_structural_reference_dataframe.loc[subcortical_structural_reference_dataframe['structureID'] == 'Left-Hippocampus'].loc[~subcortical_structural_reference_dataframe['volume'].isna()],scale='count',inner='points')\n",
    "plt.savefig('./img/left_hippocampus_volume_reference_example.png')\n",
    "plt.savefig('./img/left_hippocampus_volume_reference_example.eps')\n",
    "plt.close()\n",
    "\n",
    "## fmri networks - average degree: DMN\n",
    "grab default mode a network nodes and average\n",
    "if os.path.isfile(data_dir+'/'+groups[0]+'-within-network-default-a-preclean-fc.csv'):\n",
    "    default_a_preclean_df = pd.read_csv(data_dir+'/'+groups[0]+'-within-network-default-a-preclean-fc.csv')\n",
    "else:\n",
    "    default_a_preclean_df = fmri_yeo_networks_adjacency_mean_fc.loc[fmri_yeo_networks_adjacency_mean_fc['structureID'] == 'DefaultA']\n",
    "    default_a_preclean_df.to_csv(data_dir+'/'+groups[0]+'-within-network-default-a-preclean-fc.csv',index=False)\n",
    "\n",
    "# generate distribution of volume pre outliers\n",
    "sns.violinplot(x='FC',data=default_a_preclean_df,scale='count',inner='points')\n",
    "plt.savefig('./img/default_a_fc_pre_outliers_example.png')\n",
    "plt.savefig('./img/default_a_fc_pre_outliers_example.eps')\n",
    "plt.close()\n",
    "\n",
    "# generate outliers and reference plot\n",
    "default_a_distances, default_a_outliers, default_a_reference_dataframe, default_a_reference_json = outlierDetection(default_a_preclean_df,default_a_preclean_df.structureID.unique(),'structureID',['FC'],95,'emd',True,False,\"\",groups[0],data_dir+'/references','default_a_fc_references_'+groups[0])\n",
    "\n",
    "# generate distribution of node degree with outliers\n",
    "outliers_subs = default_a_outliers['subjectID'].unique()\n",
    "default_a_reference = default_a_preclean_df.loc[~default_a_preclean_df['subjectID'].isin(outliers_subs)]\n",
    "default_a_outliers = default_a_preclean_df.loc[default_a_preclean_df['subjectID'].isin(outliers_subs)]\n",
    "sns.violinplot(x='FC',data=default_a_reference,scale='count',inner='points')\n",
    "sns.swarmplot(x='FC',data=default_a_outliers,color='red')\n",
    "plt.savefig('./img/default_a_fc_outliers_example.png')\n",
    "plt.savefig('./img/default_a_fc_outliers_example.eps')\n",
    "plt.close()\n",
    "\n",
    "# generate reference\n",
    "sns.violinplot(x='FC',data=default_a_reference,scale='count',inner='points')\n",
    "plt.savefig('./img/default_a_fc_reference_example.png')\n",
    "plt.savefig('./img/default_a_fc_reference_example.eps')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replicate Fukutomi et al 2018 results\n",
    "# identify glasser structures with average snr > 17\n",
    "good_glasser_structures = [ f for f in cortical['structureID'].unique()[cortical.groupby('structureID',as_index=False).mean()['snr'] > 17] if not 'lh' in f if not 'rh' in f]\n",
    "\n",
    "# grab reference datasets\n",
    "cortical_ref = pd.read_csv(data_dir+'/references/cortical_reference_'+groups[0]+'.csv')\n",
    "\n",
    "# extract only the good structures\n",
    "cortical_ref_good_snr = cortical_ref.loc[cortical_ref['structureID'].isin(good_glasser_structures)]\n",
    "\n",
    "# plot data\n",
    "singleplotScatter(\"\",cortical_ref_good_snr.groupby(['structureID'],as_index=False).mean(),cortical_ref_good_snr.groupby(['structureID'],as_index=False).mean(),'average_thickness_mm','odi',False,'structureID','structureID','ravel','linreg',False,True,img_dir,\"study_replication_cortex_tissue_mapping_\"+groups[0]+\"_structure_average\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
