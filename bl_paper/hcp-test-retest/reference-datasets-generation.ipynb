{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subjects\n",
    "def collectSubjectData(topPath,dataPath,groups,subjects,colors):\n",
    "\n",
    "    # set up variables\n",
    "    data_columns = ['subjectID','classID','colors']\n",
    "    data =  pd.DataFrame([],columns=data_columns)\n",
    "\n",
    "    # populate structure\n",
    "    data['subjectID'] = [ f for g in groups for f in subjects[g] ]\n",
    "    data['classID'] = [ g for g in groups for f in range(len(subjects[g]))]\n",
    "    data['colors'] = [ colors[c] for c in colors for f in subjects[c]]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'subjects.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### color dictionary\n",
    "def createColorDictionary(data,measure,colorPalette):\n",
    "\n",
    "    keys = data[measure].unique()\n",
    "    values = sns.color_palette(colorPalette,len(keys))\n",
    "    values = values.as_hex()\n",
    "\n",
    "    colors_dict = dict(zip(keys,values))\n",
    "\n",
    "    return colors_dict\n",
    "\n",
    "### load parcellation stats data \n",
    "### load data \n",
    "def collectData(datatype,datatype_tags,tags,filename,subjects_data,colors,outPath):\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "    \n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    paths = []\n",
    "    \n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            if datatype_tags in obj['output']['datatype_tags']:\n",
    "                if tags in obj['output']['tags']:\n",
    "                    subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                    paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "    \n",
    "    # sort paths by subject order\n",
    "    paths = [x for _,x in sorted(zip(subjects,paths))]\n",
    "\n",
    "    for i in paths:\n",
    "        tmpdata = pd.read_csv(i)\n",
    "        if tmpdata.subjectID.dtypes != 'object':\n",
    "            tmpdata['subjectID'] = [ str(int(np.float(f))) for f in tmpdata.subjectID ]\n",
    "        if 'classID' in tmpdata.keys():\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on=['subjectID','classID'])\n",
    "        else:\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on='subjectID')\n",
    "        data = data.append(tmpdata,ignore_index=True)\n",
    "            \n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    # subjects.csv\n",
    "    data.to_csv(outPath,index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### cut nodes\n",
    "def cutNodes(data,num_nodes,dataPath,foldername,savename):\n",
    "\n",
    "    # identify inner n nodes based on num_nodes input\n",
    "    total_nodes = len(data['nodeID'].unique())\n",
    "    cut_nodes = int((total_nodes - num_nodes) / 2)\n",
    "\n",
    "    # remove cut_nodes from dataframe\n",
    "    data = data[data['nodeID'].between((cut_nodes)+1,(num_nodes+cut_nodes))]\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'/'+foldername+'-'+savename+'.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def computeMeanData(dataPath,data,outname):\n",
    "\n",
    "    # make mean data frame\n",
    "    data_mean =  data.groupby(['subjectID','classID','structureID']).mean().reset_index()\n",
    "    data_mean['nodeID'] = [ 1 for f in range(len(data_mean['nodeID'])) ]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data_mean.to_csv(dataPath+outname+'.csv',index=False)\n",
    "\n",
    "    return data_mean\n",
    "\n",
    "### rank order effect size calculator\n",
    "def computeRankOrderEffectSize(groups,subjects,tissue,measures,stat,measures_to_average,data_dir):\n",
    "\n",
    "    comparison_array = list(combinations(groups,2)) # 2 x 2 array; 2 different comparisons, with two pairs per comparison. comparison_array[0] = (\"run_1\",\"run_2\")\n",
    "    es = {}\n",
    "    roes = {}\n",
    "\n",
    "    # compute effect size\n",
    "    for compar in comparison_array:\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.DataFrame([])\n",
    "        tmp = pd.DataFrame([])\n",
    "        tmp['structureID'] = stat['structureID'].unique()\n",
    "        for m in measures:\n",
    "            diff = stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').mean() - stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').mean()\n",
    "            pooled_var = (np.sqrt((stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').std() ** 2 + stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').std() ** 2) / 2))\n",
    "            effectSize = diff / pooled_var\n",
    "            tmp[m+\"_effect_size\"] = list(effectSize[m])\n",
    "        tmp.to_csv(data_dir+tissue+\"_effect_sizes_\"+compar[0]+\"_\"+compar[1]+\".csv\",index=False)\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.concat([es[compar[0]+\"_\"+compar[1]],tmp],ignore_index=True)\n",
    "\n",
    "    # rank order structures\n",
    "    for ma in measures_to_average:\n",
    "        if ma == ['ad','fa','md','rd','ga','ak','mk','rk']:\n",
    "            model = 'tensor'\n",
    "        elif ma == ['ndi','isovf','odi']:\n",
    "            model = 'noddi'\n",
    "        else:\n",
    "            model = ma\n",
    "\n",
    "        tmpdata = pd.DataFrame([])\n",
    "        tmpdata['structureID'] = stat['structureID'].unique()\n",
    "        for compar in comparison_array:\n",
    "            if model == 'tensor':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ad_effect_size','fa_effect_size','md_effect_size','rd_effect_size']].abs().mean(axis=1).tolist()\n",
    "            elif model == 'noddi':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ndi_effect_size','isovf_effect_size','odi_effect_size']].abs().mean(axis=1).tolist()\n",
    "            else:\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][[ma+'_effect_size']].abs().mean(axis=1).tolist()\n",
    "\n",
    "        tmpdata[model+\"_average_effect_size\"] =  tmpdata.mean(axis=1).tolist()\n",
    "        tmpdata.to_csv(data_dir+model+\"_average_\"+tissue+\"_effect_sizes.csv\",index=False)\n",
    "        roes[model] = tmpdata.sort_values(by=model+\"_average_effect_size\")['structureID'].tolist()\n",
    "\n",
    "    return roes\n",
    "\n",
    "def combineCorticalSubcortical(dataPath,corticalData,subcorticalData):\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    corticalData = corticalData.drop(columns=['snr','thickness'])\n",
    "    subcorticalData = subcorticalData.drop(columns=['parcID','number_of_voxels'])\n",
    "\n",
    "    # merge data frames\n",
    "    data = pd.concat([corticalData,subcorticalData],sort=False)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'graymatter_nodes.csv',index=False)\n",
    "\n",
    "    # identify gray matter names\n",
    "    graymatter_names = list(data['structureID'].unique())\n",
    "\n",
    "    # output track names\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    with open((dataPath+'graymatter_list.json'),'w') as gm_listf:\n",
    "        json.dump(graymatter_names,gm_listf)\n",
    "\n",
    "    return [graymatter_names,data]\n",
    "\n",
    "# # def computeDistance(x,y,metric):\n",
    "\n",
    "# #     from sklearn.metrics.pairwise import euclidean_distances\n",
    "# #     from scipy.stats import wasserstein_distance\n",
    "\n",
    "# #     if metric == 'euclidean':\n",
    "# #         dist = euclidean_distances([x,y])[0][1]\n",
    "# #     else:\n",
    "# #         dist = wasserstein_distance(x,y)\n",
    "        \n",
    "# #     return dist\n",
    "\n",
    "# def computeDistance(x,y,metric):\n",
    "\n",
    "#     from sklearn.metrics.pairwise import euclidean_distances\n",
    "#     from scipy.stats import wasserstein_distance\n",
    "\n",
    "#     if metric == 'euclidean':\n",
    "#         dist = euclidean_distances([x,y])[0][1]\n",
    "#     else:\n",
    "#         dist = [ wasserstein_distance([x],[y[0]]) for x in x ]\n",
    "        \n",
    "#     return dist\n",
    "\n",
    "\n",
    "# def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "#     references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "#     references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "#     references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "#     return references_mean, references_sd\n",
    "\n",
    "# # def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "# #     dist = []\n",
    "# #     subj = []\n",
    "# #     meas = []\n",
    "# #     struc = []\n",
    "\n",
    "# #     for i in structures:\n",
    "# #         print(i)\n",
    "# #         subj_data = data.loc[data['structureID'] == i]\n",
    "# #         references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "# #         for m in measures:\n",
    "# #             for s in subj_data.subjectID.unique():\n",
    "# #                 x = list(subj_data.loc[subj_data['subjectID'] == s][m].values.tolist())\n",
    "# #                 y = list(references_data[0][m].values.tolist())\n",
    "# #                 dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "# #                 subj = np.append(subj,s)\n",
    "# #                 meas = np.append(meas,m)\n",
    "# #                 struc = np.append(struc,i)\n",
    "\n",
    "# #     dist_dataframe = pd.DataFrame()\n",
    "# #     dist_dataframe['subjectID'] = subj\n",
    "# #     dist_dataframe['structureID'] = struc\n",
    "# #     dist_dataframe['measures'] = meas\n",
    "# #     dist_dataframe['distance'] = dist\n",
    "    \n",
    "# #     return dist_dataframe\n",
    "\n",
    "# def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "#     dist = []\n",
    "#     subj = []\n",
    "#     meas = []\n",
    "#     struc = []\n",
    "\n",
    "#     for i in structures:\n",
    "#         print(i)\n",
    "#         subj_data = data.loc[data['structureID'] == i]\n",
    "#         references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "#         for m in measures:\n",
    "#             dist = np.append(dist,computeDistance(subj_data[m].values,references_data[0][m].values,'emd'))\n",
    "#             subj = np.append(subj,subj_data.subjectID.unique().tolist())\n",
    "#             meas = np.append(meas,[ m for f in range(len(subj_data[m])) ])\n",
    "#             struc = np.append(struc,[ i for f in range(len(subj_data[m])) ])\n",
    "\n",
    "#     dist_dataframe = pd.DataFrame()\n",
    "#     dist_dataframe['subjectID'] = subj\n",
    "#     dist_dataframe['structureID'] = struc\n",
    "#     dist_dataframe['measures'] = meas\n",
    "#     dist_dataframe['distance'] = dist\n",
    "    \n",
    "#     return dist_dataframe\n",
    "\n",
    "# def outputReferenceJson(ref_data,measures,data_dir,filename):\n",
    "    \n",
    "#     reference_json = []\n",
    "#     for st in ref_data.structureID.unique():\n",
    "#         i=0\n",
    "#         tmp = {}\n",
    "#         tmp['structurename'] = st\n",
    "#         tmp['source'] = \"HCP\"\n",
    "#         for meas in measures:\n",
    "#             tmp[meas] = {}\n",
    "#             tmp[meas]['mean'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().mean()\n",
    "#             tmp[meas]['min'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().min()\n",
    "#             tmp[meas]['max'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().max()\n",
    "#             tmp[meas]['sd'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().std()\n",
    "#         reference_json.append(tmp)\n",
    "\n",
    "#     with open(data_dir+'/'+filename,'w') as ref_out_f:\n",
    "#         json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "#     return reference_json\n",
    "\n",
    "# def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "    \n",
    "#     reference_data = pd.DataFrame()\n",
    "    \n",
    "#     for s in outliers.structureID.unique():\n",
    "#         for m in outliers.measures.unique():\n",
    "#             if profile:\n",
    "#                 tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID','nodeID',m]].reset_index(drop=True)\n",
    "#             else:\n",
    "#                 tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID',m]].reset_index(drop=True)\n",
    "#             reference_data = pd.concat([reference_data,tmpdata])\n",
    "#     if not profile:\n",
    "#         reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "    \n",
    "#     reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "    \n",
    "#     return reference_data\n",
    "\n",
    "# def computeOutliers(distances,threshold):\n",
    "    \n",
    "#     outliers = pd.DataFrame()\n",
    "    \n",
    "#     for i in distances.structureID.unique():\n",
    "#         for m in distances.measures.unique():\n",
    "#             tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "#             outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "#     return outliers\n",
    "\n",
    "# def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,data_dir,filename):\n",
    "    \n",
    "#     import numpy as np, pandas as pd\n",
    "\n",
    "#     outliers_subjects = []\n",
    "#     outliers_structures = []\n",
    "#     outliers_measures = []\n",
    "#     outliers_metrics = []\n",
    "\n",
    "#     distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "#     outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "#     if build_outliers:\n",
    "#         if 'neID' in data.columns:\n",
    "#             reference_dataframe = buildReferenceData(data,outliers_dataframe,True)\n",
    "#             reference_json = []\n",
    "#         else:\n",
    "#             reference_dataframe = buildReferenceData(data,outliers_dataframe,False)\n",
    "#             reference_json = outputReferenceJson(reference_dataframe,measures,data_dir,filename)\n",
    "#     else:\n",
    "#         reference_dataframe = []\n",
    "#         reference_json = []\n",
    "        \n",
    "#     return distances, outliers_dataframe, reference_dataframe, reference_json\n",
    "\n",
    "def computeDistance(data,references_data,measures,metric):\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    if metric == 'euclidean':\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: euclidean_distances([x[measures].values.tolist(),references_data[measures].values.tolist()])[0][1]).values\n",
    "    else:\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: wasserstein_distance(x[measures],[references_data[measures].values[0]]))\n",
    "\n",
    "    return dist\n",
    "\n",
    "def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "    references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "    references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "    references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "    return references_mean, references_sd\n",
    "\n",
    "def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    dist = []\n",
    "    subj = []\n",
    "    meas = []\n",
    "    struc = []\n",
    "\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        subj_data = data.loc[data['structureID'] == i]\n",
    "        references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "        for m in measures:\n",
    "            if dist_metric == 'euclidean':\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'euclidean'))\n",
    "            else:\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'emd'))\n",
    "            \n",
    "            subj = np.append(subj,subj_data.subjectID.unique().tolist())\n",
    "            meas = np.append(meas,[ m for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "            struc = np.append(struc,[ i for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "\n",
    "    dist_dataframe = pd.DataFrame()\n",
    "    dist_dataframe['subjectID'] = subj\n",
    "    dist_dataframe['structureID'] = struc\n",
    "    dist_dataframe['measures'] = meas\n",
    "    dist_dataframe['distance'] = dist\n",
    "    \n",
    "    return dist_dataframe\n",
    "\n",
    "# def outputReferenceJson(ref_data,measures,profile,resample_points,data_dir,filename):\n",
    "    \n",
    "#     import json\n",
    "#     from scipy.signal import resample\n",
    "\n",
    "#     reference_json = []\n",
    "#     for st in ref_data.structureID.unique():\n",
    "#         tmp = {}\n",
    "#         tmp['structurename'] = st\n",
    "#         tmp['source'] = \"camcan\"\n",
    "#         for meas in measures:\n",
    "#             tmp[meas] = {}\n",
    "#             if profile:\n",
    "#                 gb_frame = ref_data.loc[ref_data['structureID'] == st][['nodeID',meas]].dropna().groupby('nodeID')[meas]\n",
    "#             else:\n",
    "#                 gb_frame = ref_data.loc[ref_data['structureID'] == st][[meas]].dropna()[meas]\n",
    "            \n",
    "#             if resample_points:\n",
    "#                 mean_tmp = resample(gb_frame.mean().values.tolist(),resample_points).tolist()\n",
    "#                 min_tmp = resample(gb_frame.min().values.tolist(),resample_points).tolist()\n",
    "#                 max_tmp = resample(gb_frame.max().values.tolist(),resample_points).tolist()\n",
    "#                 sd_tmp = resample(gb_frame.std().values.tolist(),resample_points).tolist()\n",
    "#             else:\n",
    "#                 mean_tmp = gb_frame.mean()\n",
    "#                 min_tmp = gb_frame.min()\n",
    "#                 max_tmp = gb_frame.max()\n",
    "#                 sd_tmp = gb_frame.std()\n",
    "\n",
    "#             tmp[meas]['mean'] = mean_tmp\n",
    "#             tmp[meas]['min'] = min_tmp\n",
    "#             tmp[meas]['max'] = max_tmp\n",
    "#             tmp[meas]['sd'] = sd_tmp\n",
    "#         reference_json.append(tmp)\n",
    "\n",
    "#     with open(data_dir+'/'+filename+'.json','w') as ref_out_f:\n",
    "#         json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "#     return reference_json\n",
    "\n",
    "def outputReferenceJson(ref_data,measures,profile,resample_points,data_dir,filename):\n",
    "    \n",
    "    import json\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    for st in ref_data.structureID.unique():\n",
    "        reference_json = []\n",
    "        tmp = {}\n",
    "        tmp['structurename'] = st\n",
    "        tmp['source'] = \"camcan\"\n",
    "        for meas in measures:\n",
    "            tmp[meas] = {}\n",
    "            if profile:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][['nodeID',meas]].dropna().groupby('nodeID')[meas]\n",
    "            else:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][[meas]].dropna()[meas]\n",
    "            \n",
    "            data_tmp = []\n",
    "            if resample_points:                \n",
    "                mean_tmp = resample(gb_frame.mean().values.tolist(),resample_points).tolist()\n",
    "                min_tmp = resample(gb_frame.min().values.tolist(),resample_points).tolist()\n",
    "                max_tmp = resample(gb_frame.max().values.tolist(),resample_points).tolist()\n",
    "                sd_tmp = resample(gb_frame.std().values.tolist(),resample_points).tolist()\n",
    "                five_tmp = resample(gb_frame.quantile(q=.05).values.tolist(),resample_points).tolist()\n",
    "                twofive_tmp = resample(gb_frame.quantile(q=.25).values.tolist(),resample_points).tolist()\n",
    "                sevenfive_tmp = resample(gb_frame.quantile(q=.75).values.tolist(),resample_points).tolist()\n",
    "                ninefive_tmp = resample(gb_frame.quantile(q=.95).values.tolist(),resample_points).tolist()\n",
    "                tmp[meas]['mean'] = mean_tmp\n",
    "                tmp[meas]['min'] = min_tmp\n",
    "                tmp[meas]['max'] = max_tmp\n",
    "                tmp[meas]['sd'] = sd_tmp\n",
    "                tmp[meas]['5_percentile'] = five_tmp\n",
    "                tmp[meas]['25_percentile'] = twofive_tmp\n",
    "                tmp[meas]['75_percentile'] = sevenfive_tmp\n",
    "                tmp[meas]['95_percentile'] = ninefive_tmp\n",
    "            else:\n",
    "                data_tmp = gb_frame.values.tolist()\n",
    "                tmp[meas]['data'] = data_tmp\n",
    "        reference_json.append(tmp)\n",
    "\n",
    "        with open(data_dir+'/'+filename+'_'+st+'.json','w') as ref_out_f:\n",
    "            json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "    return reference_json\n",
    "\n",
    "def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "    \n",
    "    reference_data = pd.DataFrame()\n",
    "    \n",
    "    for s in outliers.structureID.unique():\n",
    "        for m in outliers.measures.unique():\n",
    "            if profile:\n",
    "                tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID','nodeID',m]].reset_index(drop=True)\n",
    "            else:\n",
    "                tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID',m]].reset_index(drop=True)\n",
    "            reference_data = pd.concat([reference_data,tmpdata])\n",
    "    if not profile:\n",
    "        reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "    \n",
    "    reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "    \n",
    "    return reference_data\n",
    "\n",
    "def computeOutliers(distances,threshold):\n",
    "    \n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    for i in distances.structureID.unique():\n",
    "        for m in distances.measures.unique():\n",
    "            tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "            outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "    return outliers\n",
    "\n",
    "def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,profile,resample_points,data_dir,filename):\n",
    "    \n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    outliers_subjects = []\n",
    "    outliers_structures = []\n",
    "    outliers_measures = []\n",
    "    outliers_metrics = []\n",
    "\n",
    "    distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "    outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "    if build_outliers:\n",
    "        reference_dataframe = buildReferenceData(data,outliers_dataframe,profile,data_dir,filename)\n",
    "        reference_json = outputReferenceJson(reference_dataframe,measures,profile,resample_points,data_dir,filename)\n",
    "    else:\n",
    "        reference_dataframe = []\n",
    "        reference_json = []\n",
    "        \n",
    "    return distances, outliers_dataframe, reference_dataframe, reference_json\n",
    "\n",
    "\n",
    "def profileFlipCheck(data,subjects,structures,test_measure,flip_measures,dist_metric,threshold,outPath):\n",
    "    \n",
    "    flipped_subjects = []\n",
    "    flipped_structures = []\n",
    "    distance = []\n",
    "    flipped_distance = []\n",
    "    \n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        struc_data = data.loc[data['structureID'] == i]\n",
    "        references_data = computeReferences(struc_data,'nodeID','nodeID',flip_measures)\n",
    "        differences = []\n",
    "        dist = []\n",
    "        dist_flipped = []\n",
    "\n",
    "        for s in subjects:\n",
    "            subj_data = struc_data.loc[data['subjectID'] == s]\n",
    "            x = list(subj_data[test_measure].values.tolist())\n",
    "            y = list(references_data[0][test_measure].values.tolist())\n",
    "            dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "            dist_flipped = np.append(dist_flipped,computeDistance(list(np.flip(x)),y,dist_metric))\n",
    "            differences =  np.append(differences,(dist[-1]-dist_flipped[-1]))\n",
    "        \n",
    "        percentile_threshold = np.percentile(differences,threshold)\n",
    "#         print(percentile_threshold)\n",
    "        for m in range(len(differences)):\n",
    "            if differences[m] > 0 and differences[m] > percentile_threshold:\n",
    "#             if differences[m] > percentile_threshold:\n",
    "#                 print(subjects[m])\n",
    "                flipped_subjects = np.append(flipped_subjects,subjects[m])\n",
    "                flipped_structures = np.append(flipped_structures,i)\n",
    "                distance = np.append(distance,dist[m])\n",
    "                flipped_distance = np.append(flipped_distance,dist_flipped[m])\n",
    "    \n",
    "    output_summary = pd.DataFrame()\n",
    "    output_summary['flipped_subjects'] = flipped_subjects\n",
    "    output_summary['flipped_structures'] = flipped_structures\n",
    "    output_summary['distance'] = distance\n",
    "    output_summary['flipped_distance'] = flipped_distance\n",
    "    \n",
    "    if outPath:\n",
    "        output_summary.to_csv(outPath+'_flipped_profiles.csv',index=False)\n",
    "    \n",
    "    return output_summary\n",
    "\n",
    "### scatter plot related scripts\n",
    "# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure\n",
    "def averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X = x_stat.groupby(measure).mean()[x_measure].tolist()\n",
    "    Y = y_stat.groupby(measure).mean()[y_measure].tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure\n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X,Y = [np.array([]),np.array([])]\n",
    "    for i in range(len(x_stat[measure].unique())):\n",
    "        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]\n",
    "        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]\n",
    "\n",
    "        if np.isnan(x).any() or np.isnan(y).any():\n",
    "            print(\"skipping %s due to nan\" %x_stat[measure].unique()[i])\n",
    "        else:\n",
    "            # checks to make sure the same data\n",
    "            if len(x) == len(y):\n",
    "                X = np.append(X,x)\n",
    "                Y = np.append(Y,y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject\n",
    "def ravelNetwork(x_stat,y_stat):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    X = np.ravel(x_stat).tolist()\n",
    "    Y = np.ravel(y_stat).tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel. \n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def ravelNonNetwork(x_stat,y_stat,x_measure,y_measure):\n",
    "\n",
    "    X = x_stat[x_measure].to_list()\n",
    "    Y = y_stat[y_measure].to_list()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# wrapper function to call either of the above scripts based on user input\n",
    "def setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,isnetwork,measure):\n",
    "\n",
    "    x_stat = x_data\n",
    "    y_stat = y_data\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        X,Y = averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'append':\n",
    "        X,Y = appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'ravel':\n",
    "        if isnetwork == True:\n",
    "            X,Y = ravelNetwork(x_stat,y_stat)\n",
    "        else:\n",
    "            X,Y = ravelNonNetwork(x_stat,y_stat,x_measure,y_measure)\n",
    "\n",
    "    return x_stat,y_stat,X,Y\n",
    "\n",
    "# function to shuffle data and colors\n",
    "def shuffleDataAlg(X,Y,hues):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    X,Y,hues = shuffle(X,Y,hues)\n",
    "\n",
    "    return X,Y,hues\n",
    "\n",
    "# simple display or figure save function\n",
    "def saveOrShowImg(dir_out,x_measure,y_measure,img_name):\n",
    "    import os,sys \n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    " \n",
    "        # save or show plot\n",
    "        if dir_out:\n",
    "            if not os.path.exists(dir_out):\n",
    "                os.mkdir(dir_out)\n",
    "\n",
    "            if x_measure == y_measure:\n",
    "                img_name_eps = img_name+'_'+x_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'.svg'\n",
    "            else:\n",
    "                img_name_eps = img_name+'_'+x_measure+'_vs_'+y_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'_vs_'+y_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'_vs_'+y_measure+'.svg'\n",
    "\n",
    "            plt.savefig(os.path.join(dir_out, img_name_eps),transparent=True)\n",
    "            plt.savefig(os.path.join(dir_out, img_name_png))     \n",
    "    #         plt.savefig(os.path.join(dir_out, img_name_svg))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results\n",
    "# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def relplotScatter(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,column_wrap,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data: CANNOT BE AVERAGE\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,'ravel',False,hue_measure)\n",
    "\n",
    "    p = sns.relplot(x=X,y=Y,col=x_stat[column_measure],hue=x_stat[hue_measure],kind=\"scatter\",s=100,col_wrap=column_wrap)\n",
    "\n",
    "    # setting counter. looping through axes to add important info and regression lines\n",
    "    i = 0\n",
    "    for ax in p.axes.flat:\n",
    "        x_lim,y_lim = [ax.get_xlim(),ax.get_ylim()]\n",
    "\n",
    "        if trendline == 'equality':\n",
    "            ax.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        elif trendline == 'linreg':\n",
    "            m,b = np.polyfit(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'],1)\n",
    "            ax.plot(ax.get_xticks(),m*ax.get_xticks() + b)\n",
    "            plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(x_measure)\n",
    "        ax.set_ylabel(y_measure)\n",
    "\n",
    "        # compute correlation for each subject and add to plots\n",
    "        corr = np.corrcoef(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'])[1][0]\n",
    "        plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # compute rmse for each subject and add to plots\n",
    "        rmse = np.sqrt(mean_squared_error(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y']))\n",
    "        plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # update counter\n",
    "        i = i+1\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def singleplotScatter(colors_dict,x_data,y_data,x_measure,y_measure,logX,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "    colors = sns.color_palette('colorblind',len(x_stat[hue_measure]))\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    if logX == True:\n",
    "        X = np.log10(X)\n",
    "\n",
    "    if colors_dict:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    else:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100)\t\t\t\t\t\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    if x_measure == y_measure:\n",
    "        p.axes.axis('square')\n",
    "        y_ticks = p.axes.get_yticks()\n",
    "        p.axes.set_xticks(y_ticks)\n",
    "        p.axes.set_yticks(p.axes.get_xticks())\n",
    "        p.axes.set_ylim(p.axes.get_xlim())\n",
    "        p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "    elif trendline == 'groupreg':\n",
    "        for g in range(len(groups)):\n",
    "            if stat_name == 'volume':\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            else:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure] for network correlations. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized.\n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def networkScatter(colors_dict,hues,groups,subjects,x_data,y_data,network_measure,shuffleData,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,\"\",\"\",\"ravel\",True,\"\")\n",
    "\n",
    "    # additional network setup\n",
    "    # hues = sns.color_palette(colormap,len(X))\n",
    "    # hues = hues.as_hex()\n",
    "    # keys = [ i for i in range(len(X)) ]\n",
    "    # colors_dict = dict(zip(hues,hues))\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    # if colors_dict:\n",
    "        # p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    # else:\n",
    "    p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    p.axes.axis('square')\n",
    "    y_ticks = p.axes.get_yticks()\n",
    "    p.axes.set_xticks(y_ticks)\n",
    "    p.axes.set_yticks(p.axes.get_xticks())\n",
    "    p.axes.set_ylim(p.axes.get_xlim())\n",
    "    p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s %s vs %s' %(network_measure,groups[0],groups[1]),fontsize=20)\n",
    "    plt.xlabel(groups[0],fontsize=18)\n",
    "    plt.ylabel(groups[1],fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,network_measure+'_'+groups[0],network_measure+'_'+groups[1],img_name)\n",
    "\n",
    "# uses matplotlib.pyplot's hist2d function to plot data from x_data[x_measure] and y_data[y_measure]. useful for supplementary figure or debugging or publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plot2dHist(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    plt.hist2d(x=X,y=Y,cmin=1,density=False,bins=(len(X)/10),cmap='magma',vmax=(len(X)/10))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # set title and x and y labels\n",
    "\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # # remove top and right spines from plot\n",
    "    # p.axes.spines[\"top\"].set_visible(False)\n",
    "    # p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "### tract profile data\n",
    "# uses matplotlib.pyplot's plot and fill_between functions to plot tract profile data from stat. useful for publication worthy figure\n",
    "# requires stat to be formatted in way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the track in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotProfiles(structures,stat,diffusion_measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os,sys\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # loop through all structures\n",
    "    for t in structures:\n",
    "        print(t)\n",
    "        # loop through all measures\n",
    "        for dm in diffusion_measures:\n",
    "            print(dm)\n",
    "\n",
    "            imgname=img_name+\"_\"+t+\"_\"+dm\n",
    "\n",
    "            # generate figures\n",
    "            fig = plt.figure(figsize=(15,15))\n",
    "#             fig = plt.figure()\n",
    "            fig.patch.set_visible(False)\n",
    "            p = plt.subplot()\n",
    "\n",
    "            # set title and catch array for legend handle\n",
    "            plt.title(\"%s Profiles %s: %s\" %(summary_method,t,dm),fontsize=20)\n",
    "\n",
    "            # loop through groups and plot profile data\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is nodes\n",
    "                x = stat['nodeID'].unique()\n",
    "\n",
    "                # y is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).mean()[dm][t]\n",
    "                elif summary_method == 'median':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).median()[dm][t]\n",
    "                elif summary_method == 'max':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).max()[dm][t]\n",
    "                elif summary_method == 'min':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).min()[dm][t]\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t] / np.sqrt(len(stat[stat['classID'] == stat.classID.unique()[g]]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t]\n",
    "\n",
    "                # plot summary\n",
    "                plt.plot(x,y,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],linewidth=5,label=stat.classID.unique()[g])\n",
    "\n",
    "                # plot shaded error\n",
    "                plt.fill_between(x,y-err,y+err,alpha=0.4,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='1 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "                plt.fill_between(x,y-(2*err),y+(2*err),alpha=0.2,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='2 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "\n",
    "            # set up labels and ticks\n",
    "            plt.xlabel('Location',fontsize=18)\n",
    "            plt.ylabel(dm,fontsize=18)\n",
    "            plt.xticks([x[0],x[-1]],['Begin','End'],fontsize=16)\n",
    "            plt.legend(fontsize=12)\n",
    "            y_lim = plt.ylim()\n",
    "            plt.yticks([np.round(y_lim[0],2),np.mean(y_lim),np.round(y_lim[1],2)],fontsize=16)\n",
    "\n",
    "            # remove top and right spines from plot\n",
    "            p.axes.spines[\"top\"].set_visible(False)\n",
    "            p.axes.spines[\"right\"].set_visible(False)\n",
    "            ax = plt.gca()\n",
    "\n",
    "            # save image or show image\n",
    "#             saveOrShowImg(dir_out,dm,dm,imgname)\n",
    "    return fig\n",
    "\n",
    "### generic data plots\n",
    "## structure average\n",
    "# uses matplotlib.pyplot's errobar function to plot group average data for each structure with errorbars. useful for publication worthy figure\n",
    "# requires stat to be formatted in similar way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the structure in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotGroupStructureAverage(structures,tissue,stat,measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "\n",
    "    for dm in measures:\n",
    "        print(dm)\n",
    "\n",
    "        # generate figures\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "        fig.patch.set_visible(False)\n",
    "        p = plt.subplot()\n",
    "\n",
    "        # set y range\n",
    "        p.set_ylim([0,(len(structures)*len(stat.classID.unique()))+len(stat.classID.unique())])\n",
    "\n",
    "        # set spines and ticks, and labels\n",
    "        p.yaxis.set_ticks_position('left')\n",
    "        p.xaxis.set_ticks_position('bottom')\n",
    "        p.set_xlabel(dm,fontsize=18)\n",
    "        p.set_ylabel(\"Structures\",fontsize=18)\n",
    "        if len(stat.classID.unique()) < 3:\n",
    "            if len(stat.classID.unique()) == 2:\n",
    "                p.set_yticks(np.arange(1.5,(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "            else:\n",
    "                p.set_yticks(np.arange(1,len(structures)+1,step=1))\n",
    "        else:\n",
    "            p.set_yticks(np.arange((len(stat.classID.unique())-1),(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "        p.set_yticklabels(structures,fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "\n",
    "        # set title\n",
    "        plt.title(\"%s Group-Summary: %s\" %(summary_method,dm),fontsize=20)\n",
    "\n",
    "        # loop through structures\n",
    "        for t in range(len(structures)):\n",
    "            # loop through groups\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).mean()[dm][structures[t]]\n",
    "                elif summary_method == 'median':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).median()[dm][structures[t]]\n",
    "                elif summary_method == 'max':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).max()[dm][structures[t]]\n",
    "                elif summary_method == 'min':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).min()[dm][structures[t]]\n",
    "\n",
    "                # y is location on y axis\n",
    "                y = (len(stat.classID.unique())*(t+1)-len(stat.classID.unique()))+(g+1)\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]] / np.sqrt(len(stat[stat.classID.str.contains(stat.classID.unique()[g])]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]]\n",
    "\n",
    "                # plot data\n",
    "                if t == 0:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10,label=stat.classID.unique()[g])\n",
    "                else:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10)\n",
    "\n",
    "        # add legend\n",
    "        plt.legend(fontsize=16)\n",
    "\n",
    "        # remove top and right spines from plot\n",
    "        p.axes.spines[\"top\"].set_visible(False)\n",
    "        p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "        # save image or show image\n",
    "        saveOrShowImg(dir_out,dm,dm,img_name)\n",
    "\n",
    "def violinPlots(x_measure,y_measure,hue_measure,data,summary,scale,inner,cmap,dir_out,img_name):\n",
    "    \n",
    "    if hue_measure:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,hue=hue_measure,scale=scale,inner=inner,orientation='horizontal')\n",
    "    else:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,scale=scale,inner=inner,orientation='horizontal')\n",
    "\n",
    "    if summary == 'mean':\n",
    "        summary = data.groupby([x_measure])[y_measure].mean()\n",
    "    elif summary == 'median':\n",
    "        summary = data.groupby([x_measure])[y_measure].median()\n",
    "    elif summary == 'mode':\n",
    "        summary = data.groupby([x_measure])[y_measure].mode()\n",
    "    elif summary == 'max':\n",
    "        summary = data.groupby([x_measure])[y_measure].max()\n",
    "    elif summary == 'min':\n",
    "        summary = data.groupby([x_measure])[y_measure].min()\n",
    "\n",
    "#     for xtick in violin.get_xticks():\n",
    "#         violin.text(xtick,summary[xtick],np.round(summary[xtick],3),horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "  \n",
    "#     saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "    return violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up variables\n",
      "setting up variables complete\n",
      "grabbing demographic data\n",
      "grabbing demographic data complete\n"
     ]
    }
   ],
   "source": [
    "### setting up variables and adding paths\n",
    "print(\"setting up variables\")\n",
    "topPath = \"./\"\n",
    "os.chdir(topPath)\n",
    "data_dir = topPath+'/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "img_dir = topPath+'/img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "groups = ['hcp']\n",
    "colors_array = ['orange']\n",
    "diff_micro_measures = ['ad','fa','md','rd','ndi','isovf','odi']\n",
    "diff_macro_measures = ['length','volume','count']\n",
    "print(\"setting up variables complete\")\n",
    "\n",
    "### create subjects.csv\n",
    "# print(\"creating subjects.csv\")\n",
    "# from compile_data import collectSubjectData\n",
    "# subjects_data = collectSubjectData(topPath,data_dir,groups,subjects,colors)\n",
    "# print(\"creating subjects.csv complete\")\n",
    "\n",
    "### grabbing subjects demographic data\n",
    "print(\"grabbing demographic data\")\n",
    "subjects_data = pd.read_csv('./subjects_demo.csv')\n",
    "print(\"grabbing demographic data complete\")\n",
    "colors = {}\n",
    "subjects = {}\n",
    "\n",
    "# loop through groups and identify subjects and set color schema for each group\n",
    "for g in range(len(groups)):\n",
    "    # set subjects array\n",
    "    subjects[groups[g]] =  subjects_data['subjectID']\n",
    "    subjects[groups[g]].sort_values()\n",
    "    # update subjects with HCP prefix to make plotting easier\n",
    "\n",
    "    # set colors array\n",
    "    colors_name = colors_array[g]\n",
    "    colors[groups[g]] = colors_array[g]\n",
    "\n",
    "\n",
    "### merge demo and subjects data\n",
    "# subjects_data = pd.merge(subjects_data,subjects_demo,on='subjectID')\n",
    "subjects_data['classID'] = [ 'camcan' for f in range(len(subjects_data))]\n",
    "subjects_data = subjects_data.drop(columns={'gender','age'})\n",
    "# subjects_data.to_csv(data_dir+'/subjects_data_demo.csv')\n",
    "\n",
    "# create subjects color dictionary\n",
    "colors_dict = createColorDictionary(subjects_data,'subjectID','colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating gray matter references\n",
      "Left-Cerebral-White-Matter\n",
      "Left-Cerebral-Cortex\n",
      "Left-Inf-Lat-Vent\n",
      "Left-Cerebellum-White-Matter\n",
      "Left-Cerebellum-Cortex\n",
      "Left-Thalamus-Proper\n",
      "Left-Caudate\n",
      "Left-Putamen\n",
      "Left-Pallidum\n",
      "Brain-Stem\n",
      "Left-Hippocampus\n",
      "Left-Amygdala\n",
      "Left-Accumbens-area\n",
      "Left-VentralDC\n",
      "Left-vessel\n",
      "Right-Cerebral-White-Matter\n",
      "Right-Cerebral-Cortex\n",
      "Right-Cerebellum-White-Matter\n",
      "Right-Cerebellum-Cortex\n",
      "Right-Thalamus-Proper\n",
      "Right-Caudate\n",
      "Right-Putamen\n",
      "Right-Pallidum\n",
      "Right-Hippocampus\n",
      "Right-Amygdala\n",
      "Right-Accumbens-area\n",
      "Right-VentralDC\n",
      "Right-vessel\n",
      "WM-hypointensities\n",
      "Optic-Chiasm\n",
      "CC_Posterior\n",
      "CC_Mid_Posterior\n",
      "CC_Central\n",
      "CC_Mid_Anterior\n",
      "CC_Anterior\n",
      "Right-choroid-plexus\n",
      "Right-Inf-Lat-Vent\n",
      "CSF\n",
      "Left-choroid-plexus\n",
      "5th-Ventricle\n",
      "non-WM-hypointensities\n",
      "3rd-Ventricle\n",
      "4th-Ventricle\n",
      "Left-Lateral-Ventricle\n",
      "Right-Lateral-Ventricle\n",
      "Left-Cerebral-White-Matter\n",
      "Left-Cerebral-Cortex\n",
      "Left-Inf-Lat-Vent\n",
      "Left-Cerebellum-White-Matter\n",
      "Left-Cerebellum-Cortex\n",
      "Left-Thalamus-Proper\n",
      "Left-Caudate\n",
      "Left-Putamen\n",
      "Left-Pallidum\n",
      "Brain-Stem\n",
      "Left-Hippocampus\n",
      "Left-Amygdala\n",
      "Left-Accumbens-area\n",
      "Left-VentralDC\n",
      "Left-vessel\n",
      "Right-Cerebral-White-Matter\n",
      "Right-Cerebral-Cortex\n",
      "Right-Cerebellum-White-Matter\n",
      "Right-Cerebellum-Cortex\n",
      "Right-Thalamus-Proper\n",
      "Right-Caudate\n",
      "Right-Putamen\n",
      "Right-Pallidum\n",
      "Right-Hippocampus\n",
      "Right-Amygdala\n",
      "Right-Accumbens-area\n",
      "Right-VentralDC\n",
      "Right-vessel\n",
      "WM-hypointensities\n",
      "Optic-Chiasm\n",
      "CC_Posterior\n",
      "CC_Mid_Posterior\n",
      "CC_Central\n",
      "CC_Mid_Anterior\n",
      "CC_Anterior\n",
      "Right-choroid-plexus\n",
      "Right-Inf-Lat-Vent\n",
      "CSF\n",
      "Left-choroid-plexus\n",
      "5th-Ventricle\n",
      "non-WM-hypointensities\n",
      "3rd-Ventricle\n",
      "4th-Ventricle\n",
      "Left-Lateral-Ventricle\n",
      "Right-Lateral-Ventricle\n"
     ]
    }
   ],
   "source": [
    "#### gray matter mapping reference\n",
    "print(\"creating gray matter references\")\n",
    "\n",
    "# create data structures\n",
    "# subcortical measures\n",
    "if os.path.isfile(data_dir+'/subcortical_reference_preclean.csv'):\n",
    "    subcortical = pd.read_csv(data_dir+'/subcortical_reference_preclean.csv')\n",
    "else:\n",
    "    subcortical =  collectData('neuro/parc-stats','subcort_stats','noddi','aseg_nodes.csv',subjects_data,colors,data_dir+'/subcortical_reference_preclean.csv')\n",
    "    subcortical = subcortical.drop_duplicates()\n",
    "    subcortical['subjectID'] = subcortical.subjectID.astype(str)\n",
    "    bad_subcortical = pd.DataFrame()\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] > 1]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] < 0]])\n",
    "    subcortical = subcortical.loc[~subcortical.index.isin(bad_subcortical.index)]\n",
    "    subcortical.rename(columns={'volume': 'gray_matter_volume_mm^3'},inplace=True)\n",
    "    subcortical.to_csv(data_dir+'/subcortical_reference_preclean.csv',index=False)\n",
    "\n",
    "# compute reference dataset for subcortical structures\n",
    "subcortical_structural_measures = ['number_of_voxels','gray_matter_volume_mm^3']\n",
    "subcortical_structural_distances, subcortical_structural_outliers, subcortical_structural_reference_dataframe, subcortical_structural_reference_json = outlierDetection(subcortical,subcortical.structureID.unique(),'structureID',subcortical_structural_measures,95,'emd',True,False,'',data_dir+'/references','subcortical_structural_reference_camcan')\n",
    "\n",
    "subcortical_diffusion_measures = ['ad','fa','md','rd','ndi','odi','isovf']\n",
    "subcortical_diffusion_distances, subcortical_diffusion_outliers, subcortical_diffusion_reference_dataframe, subcortical_diffusion_reference_json = outlierDetection(subcortical,subcortical.structureID.unique(),'structureID',subcortical_diffusion_measures,95,'emd',True,False,'',data_dir+'/references','subcortical_diffusion_reference_camcan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lh_G&S_frontomargin\n",
      "lh_G&S_occipital_inf\n",
      "lh_G&S_paracentral\n",
      "lh_G&S_subcentral\n",
      "lh_G&S_transv_frontopol\n",
      "lh_G&S_cingul-Ant\n",
      "lh_G&S_cingul-Mid-Ant\n",
      "lh_G&S_cingul-Mid-Post\n",
      "lh_G_cingul-Post-dorsal\n",
      "lh_G_cingul-Post-ventral\n",
      "lh_G_cuneus\n",
      "lh_G_front_inf-Opercular\n",
      "lh_G_front_inf-Orbital\n",
      "lh_G_front_inf-Triangul\n",
      "lh_G_front_middle\n",
      "lh_G_front_sup\n",
      "lh_G_Ins_lg&S_cent_ins\n",
      "lh_G_insular_short\n",
      "lh_G_occipital_middle\n",
      "lh_G_occipital_sup\n",
      "lh_G_oc-temp_lat-fusifor\n",
      "lh_G_oc-temp_med-Lingual\n",
      "lh_G_oc-temp_med-Parahip\n",
      "lh_G_orbital\n",
      "lh_G_pariet_inf-Angular\n",
      "lh_G_pariet_inf-Supramar\n",
      "lh_G_parietal_sup\n",
      "lh_G_postcentral\n",
      "lh_G_precentral\n",
      "lh_G_precuneus\n",
      "lh_G_rectus\n",
      "lh_G_subcallosal\n",
      "lh_G_temp_sup-G_T_transv\n",
      "lh_G_temp_sup-Lateral\n",
      "lh_G_temp_sup-Plan_polar\n",
      "lh_G_temp_sup-Plan_tempo\n",
      "lh_G_temporal_inf\n",
      "lh_G_temporal_middle\n",
      "lh_Lat_Fis-ant-Horizont\n",
      "lh_Lat_Fis-ant-Vertical\n",
      "lh_Lat_Fis-post\n",
      "lh_Pole_occipital\n",
      "lh_Pole_temporal\n",
      "lh_S_calcarine\n",
      "lh_S_central\n",
      "lh_S_cingul-Marginalis\n",
      "lh_S_circular_insula_ant\n",
      "lh_S_circular_insula_inf\n",
      "lh_S_circular_insula_sup\n",
      "lh_S_collat_transv_ant\n",
      "lh_S_collat_transv_post\n",
      "lh_S_front_inf\n",
      "lh_S_front_middle\n",
      "lh_S_front_sup\n",
      "lh_S_interm_prim-Jensen\n",
      "lh_S_intrapariet&P_trans\n",
      "lh_S_oc_middle&Lunatus\n",
      "lh_S_oc_sup&transversal\n",
      "lh_S_occipital_ant\n",
      "lh_S_oc-temp_lat\n",
      "lh_S_oc-temp_med&Lingual\n",
      "lh_S_orbital_lateral\n",
      "lh_S_orbital_med-olfact\n",
      "lh_S_orbital-H_Shaped\n",
      "lh_S_parieto_occipital\n",
      "lh_S_pericallosal\n",
      "lh_S_postcentral\n",
      "lh_S_precentral-inf-part\n",
      "lh_S_precentral-sup-part\n",
      "lh_S_suborbital\n",
      "lh_S_subparietal\n",
      "lh_S_temporal_inf\n",
      "lh_S_temporal_sup\n",
      "lh_S_temporal_transverse\n",
      "rh_G&S_frontomargin\n",
      "rh_G&S_occipital_inf\n",
      "rh_G&S_paracentral\n",
      "rh_G&S_subcentral\n",
      "rh_G&S_transv_frontopol\n",
      "rh_G&S_cingul-Ant\n",
      "rh_G&S_cingul-Mid-Ant\n",
      "rh_G&S_cingul-Mid-Post\n",
      "rh_G_cingul-Post-dorsal\n",
      "rh_G_cingul-Post-ventral\n",
      "rh_G_cuneus\n",
      "rh_G_front_inf-Opercular\n",
      "rh_G_front_inf-Orbital\n",
      "rh_G_front_inf-Triangul\n",
      "rh_G_front_middle\n",
      "rh_G_front_sup\n",
      "rh_G_Ins_lg&S_cent_ins\n",
      "rh_G_insular_short\n",
      "rh_G_occipital_middle\n",
      "rh_G_occipital_sup\n",
      "rh_G_oc-temp_lat-fusifor\n",
      "rh_G_oc-temp_med-Lingual\n",
      "rh_G_oc-temp_med-Parahip\n",
      "rh_G_orbital\n",
      "rh_G_pariet_inf-Angular\n",
      "rh_G_pariet_inf-Supramar\n",
      "rh_G_parietal_sup\n",
      "rh_G_postcentral\n",
      "rh_G_precentral\n",
      "rh_G_precuneus\n",
      "rh_G_rectus\n",
      "rh_G_subcallosal\n",
      "rh_G_temp_sup-G_T_transv\n",
      "rh_G_temp_sup-Lateral\n",
      "rh_G_temp_sup-Plan_polar\n",
      "rh_G_temp_sup-Plan_tempo\n",
      "rh_G_temporal_inf\n",
      "rh_G_temporal_middle\n",
      "rh_Lat_Fis-ant-Horizont\n",
      "rh_Lat_Fis-ant-Vertical\n",
      "rh_Lat_Fis-post\n",
      "rh_Pole_occipital\n",
      "rh_Pole_temporal\n",
      "rh_S_calcarine\n",
      "rh_S_central\n",
      "rh_S_cingul-Marginalis\n",
      "rh_S_circular_insula_ant\n",
      "rh_S_circular_insula_inf\n",
      "rh_S_circular_insula_sup\n",
      "rh_S_collat_transv_ant\n",
      "rh_S_collat_transv_post\n",
      "rh_S_front_inf\n",
      "rh_S_front_middle\n",
      "rh_S_front_sup\n",
      "rh_S_interm_prim-Jensen\n",
      "rh_S_intrapariet&P_trans\n",
      "rh_S_oc_middle&Lunatus\n",
      "rh_S_oc_sup&transversal\n",
      "rh_S_occipital_ant\n",
      "rh_S_oc-temp_lat\n",
      "rh_S_oc-temp_med&Lingual\n",
      "rh_S_orbital_lateral\n",
      "rh_S_orbital_med-olfact\n",
      "rh_S_orbital-H_Shaped\n",
      "rh_S_parieto_occipital\n",
      "rh_S_pericallosal\n",
      "rh_S_postcentral\n",
      "rh_S_precentral-inf-part\n",
      "rh_S_precentral-sup-part\n",
      "rh_S_suborbital\n",
      "rh_S_subparietal\n",
      "rh_S_temporal_inf\n",
      "rh_S_temporal_sup\n",
      "rh_S_temporal_transverse\n"
     ]
    }
   ],
   "source": [
    "# cortical measures: aparc\n",
    "if os.path.isfile(data_dir+'/cortical_aparc_reference_preclean.csv'):\n",
    "    cortical_aparc = pd.read_csv(data_dir+'/cortical_aparc_reference_preclean.csv')\n",
    "else:\n",
    "    cortical_aparc =  collectData('neuro/parc-stats','cortex_mapping_stats','glasser','aparc_MEAN.csv',subjects_data,colors,data_dir+'/cortical_aparc_reference_preclean.csv')\n",
    "    cortical_aparc = cortical_aparc.drop_duplicates()\n",
    "    cortical_aparc['subjectID'] = cortical_aparc.subjectID.astype(str)\n",
    "    cortical_aparc = cortical_aparc.drop(columns={'volume','thickness','snr'})\n",
    "    bad_cortical_aparc = pd.DataFrame()\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['ad'] > 3]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['ad'] < 0]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['fa'] > 1]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['fa'] < 0]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['md'] > 3]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['md'] < 0]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['rd'] > 3]])\n",
    "    bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['rd'] < 0]])\n",
    "    cortical_aparc = cortical_aparc.loc[~cortical_aparc.index.isin(bad_cortical_aparc.index)]\n",
    "    cortical_aparc.to_csv(data_dir+'/cortical_aparc_reference_preclean.csv',index=False)\n",
    "\n",
    "cortical_diffusion_measures = ['ad','fa','md','rd','ndi','odi','isovf']    \n",
    "# cortical_aparc_diffusion_distances, cortical_aparc_diffusion_outliers, cortical_aparc_diffusion_reference_dataframe, cortical_aparc_diffusion_reference_json = outlierDetection(cortical_aparc,cortical_aparc.structureID.unique(),'structureID',cortical_diffusion_measures,95,'emd',True,False,\"\",data_dir+'/references','cortical_aparc_diffusion_reference_camcan')\n",
    "\n",
    "# freesurfer measures: aparc\n",
    "if os.path.isfile(data_dir+'/cortical_aparc_volume_reference_preclean.csv'):\n",
    "    cortical_aparc_parcel_volume = pd.read_csv(data_dir+'/cortical_aparc_volume_reference_preclean.csv')\n",
    "else:\n",
    "    cortical_aparc_parcel_volume = collectData('neuro/parc-stats','freesurfer','aparc.a2009s','cortex.csv',subjects_data,colors,data_dir+'/cortical_aparc_volume_reference_preclean.csv')\n",
    "    cortical_aparc_parcel_volume = cortical_aparc_parcel_volume.drop_duplicates()\n",
    "    cortical_aparc_parcel_volume['subjectID'] = cortical_aparc_parcel_volume.subjectID.astype(str)\n",
    "    cortical_aparc_parcel_volume.to_csv(data_dir+'/cortical_aparc_volume_reference_preclean.csv',index=False)\n",
    "\n",
    "# cortical_aparc_reference = pd.merge(cortical_aparc,cortical_aparc_parcel_volume,on=['subjectID','structureID','classID','nodeID'])\n",
    "# cortical_measures = [ f for f in cortical_aparc_reference.keys() if f not in ['subjectID','structureID','nodeID','classID','parcID'] ]\n",
    "\n",
    "cortical_structural_measures = ['number_of_vertices','surface_area_mm^2','gray_matter_volume_mm^3','average_thickness_mm','thickness_stddev_mm','integrated_rectified_mean_curvature_mm^-1','integrated_rectified_gaussian_curvature_mm^-2','folding_index','intrinsic_curvature_index']\n",
    "cortical_aparc_structural_distances, cortical_aparc_structural_outliers, cortical_aparc_structural_reference_dataframe, cortical_aparc_structural_reference_json = outlierDetection(cortical_aparc_parcel_volume,cortical_aparc_parcel_volume.structureID.unique(),'structureID',cortical_structural_measures,95,'emd',True,False,\"\",data_dir+'/references','cortical_aparc_structural_reference_camcan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_V1_ROI\n",
      "L_MST_ROI\n",
      "L_V6_ROI\n",
      "L_V2_ROI\n",
      "L_V3_ROI\n",
      "L_V4_ROI\n",
      "L_V8_ROI\n",
      "L_4_ROI\n",
      "L_3b_ROI\n",
      "L_FEF_ROI\n",
      "L_PEF_ROI\n",
      "L_55b_ROI\n",
      "L_V3A_ROI\n",
      "L_RSC_ROI\n",
      "L_POS2_ROI\n",
      "L_V7_ROI\n",
      "L_IPS1_ROI\n",
      "L_FFC_ROI\n",
      "L_V3B_ROI\n",
      "L_LO1_ROI\n",
      "L_LO2_ROI\n",
      "L_PIT_ROI\n",
      "L_MT_ROI\n",
      "L_A1_ROI\n",
      "L_PSL_ROI\n",
      "L_SFL_ROI\n",
      "L_PCV_ROI\n",
      "L_STV_ROI\n",
      "L_7Pm_ROI\n",
      "L_7m_ROI\n",
      "L_POS1_ROI\n",
      "L_23d_ROI\n",
      "L_v23ab_ROI\n",
      "L_d23ab_ROI\n",
      "L_31pv_ROI\n",
      "L_5m_ROI\n",
      "L_5mv_ROI\n",
      "L_23c_ROI\n",
      "L_5L_ROI\n",
      "L_24dd_ROI\n",
      "L_24dv_ROI\n",
      "L_7AL_ROI\n",
      "L_SCEF_ROI\n",
      "L_6ma_ROI\n",
      "L_7Am_ROI\n",
      "L_7PL_ROI\n",
      "L_7PC_ROI\n",
      "L_LIPv_ROI\n",
      "L_VIP_ROI\n",
      "L_MIP_ROI\n",
      "L_1_ROI\n",
      "L_2_ROI\n",
      "L_3a_ROI\n",
      "L_6d_ROI\n",
      "L_6mp_ROI\n",
      "L_6v_ROI\n",
      "L_p24pr_ROI\n",
      "L_33pr_ROI\n",
      "L_a24pr_ROI\n",
      "L_p32pr_ROI\n",
      "L_a24_ROI\n",
      "L_d32_ROI\n",
      "L_8BM_ROI\n",
      "L_p32_ROI\n",
      "L_10r_ROI\n",
      "L_47m_ROI\n",
      "L_8Av_ROI\n",
      "L_8Ad_ROI\n",
      "L_9m_ROI\n",
      "L_8BL_ROI\n",
      "L_9p_ROI\n",
      "L_8C_ROI\n",
      "L_44_ROI\n",
      "L_45_ROI\n",
      "L_47l_ROI\n",
      "L_a47r_ROI\n",
      "L_6r_ROI\n",
      "L_IFJa_ROI\n",
      "L_IFJp_ROI\n",
      "L_IFSp_ROI\n",
      "L_IFSa_ROI\n",
      "L_p9-46v_ROI\n",
      "L_46_ROI\n",
      "L_a9-46v_ROI\n",
      "L_9-46d_ROI\n",
      "L_9a_ROI\n",
      "L_10v_ROI\n",
      "L_a10p_ROI\n",
      "L_10pp_ROI\n",
      "L_11l_ROI\n",
      "L_13l_ROI\n",
      "L_OFC_ROI\n",
      "L_47s_ROI\n",
      "L_LIPd_ROI\n",
      "L_6a_ROI\n",
      "L_i6-8_ROI\n",
      "L_s6-8_ROI\n",
      "L_43_ROI\n",
      "L_OP4_ROI\n",
      "L_OP1_ROI\n",
      "L_OP2-3_ROI\n",
      "L_52_ROI\n",
      "L_RI_ROI\n",
      "L_PFcm_ROI\n",
      "L_PoI2_ROI\n",
      "L_TA2_ROI\n",
      "L_FOP4_ROI\n",
      "L_MI_ROI\n",
      "L_Pir_ROI\n",
      "L_AVI_ROI\n",
      "L_AAIC_ROI\n",
      "L_FOP1_ROI\n",
      "L_FOP3_ROI\n",
      "L_FOP2_ROI\n",
      "L_PFt_ROI\n",
      "L_AIP_ROI\n",
      "L_EC_ROI\n",
      "L_PreS_ROI\n",
      "L_ProS_ROI\n",
      "L_PeEc_ROI\n",
      "L_STGa_ROI\n",
      "L_PBelt_ROI\n",
      "L_A5_ROI\n",
      "L_PHA1_ROI\n",
      "L_PHA3_ROI\n",
      "L_STSda_ROI\n",
      "L_STSdp_ROI\n",
      "L_STSvp_ROI\n",
      "L_TGd_ROI\n",
      "L_TE1a_ROI\n",
      "L_TE1p_ROI\n",
      "L_TE2a_ROI\n",
      "L_TF_ROI\n",
      "L_TE2p_ROI\n",
      "L_PHT_ROI\n",
      "L_PH_ROI\n",
      "L_TPOJ1_ROI\n",
      "L_TPOJ2_ROI\n",
      "L_TPOJ3_ROI\n",
      "L_DVT_ROI\n",
      "L_PGp_ROI\n",
      "L_IP2_ROI\n",
      "L_IP1_ROI\n",
      "L_IP0_ROI\n",
      "L_PFop_ROI\n",
      "L_PF_ROI\n",
      "L_PFm_ROI\n",
      "L_PGi_ROI\n",
      "L_PGs_ROI\n",
      "L_V6A_ROI\n",
      "L_VMV1_ROI\n",
      "L_VMV3_ROI\n",
      "L_PHA2_ROI\n",
      "L_V4t_ROI\n",
      "L_FST_ROI\n",
      "L_V3CD_ROI\n",
      "L_LO3_ROI\n",
      "L_VMV2_ROI\n",
      "L_31pd_ROI\n",
      "L_31a_ROI\n",
      "L_VVC_ROI\n",
      "L_25_ROI\n",
      "L_s32_ROI\n",
      "L_pOFC_ROI\n",
      "L_PoI1_ROI\n",
      "L_Ig_ROI\n",
      "L_FOP5_ROI\n",
      "L_p10p_ROI\n",
      "L_p47r_ROI\n",
      "L_TGv_ROI\n",
      "L_MBelt_ROI\n",
      "L_LBelt_ROI\n",
      "L_A4_ROI\n",
      "L_STSva_ROI\n",
      "L_TE1m_ROI\n",
      "L_PI_ROI\n",
      "L_a32pr_ROI\n",
      "L_p24_ROI\n",
      "R_V1_ROI\n",
      "R_MST_ROI\n",
      "R_V6_ROI\n",
      "R_V2_ROI\n",
      "R_V3_ROI\n",
      "R_V4_ROI\n",
      "R_V8_ROI\n",
      "R_4_ROI\n",
      "R_3b_ROI\n",
      "R_FEF_ROI\n",
      "R_PEF_ROI\n",
      "R_55b_ROI\n",
      "R_V3A_ROI\n",
      "R_RSC_ROI\n",
      "R_POS2_ROI\n",
      "R_V7_ROI\n",
      "R_IPS1_ROI\n",
      "R_FFC_ROI\n",
      "R_V3B_ROI\n",
      "R_LO1_ROI\n",
      "R_LO2_ROI\n",
      "R_PIT_ROI\n",
      "R_MT_ROI\n",
      "R_A1_ROI\n",
      "R_PSL_ROI\n",
      "R_SFL_ROI\n",
      "R_PCV_ROI\n",
      "R_STV_ROI\n",
      "R_7Pm_ROI\n",
      "R_7m_ROI\n",
      "R_POS1_ROI\n",
      "R_23d_ROI\n",
      "R_v23ab_ROI\n",
      "R_d23ab_ROI\n",
      "R_31pv_ROI\n",
      "R_5m_ROI\n",
      "R_5mv_ROI\n",
      "R_23c_ROI\n",
      "R_5L_ROI\n",
      "R_24dd_ROI\n",
      "R_24dv_ROI\n",
      "R_7AL_ROI\n",
      "R_SCEF_ROI\n",
      "R_6ma_ROI\n",
      "R_7Am_ROI\n",
      "R_7PL_ROI\n",
      "R_7PC_ROI\n",
      "R_LIPv_ROI\n",
      "R_VIP_ROI\n",
      "R_MIP_ROI\n",
      "R_1_ROI\n",
      "R_2_ROI\n",
      "R_3a_ROI\n",
      "R_6d_ROI\n",
      "R_6mp_ROI\n",
      "R_6v_ROI\n",
      "R_p24pr_ROI\n",
      "R_33pr_ROI\n",
      "R_a24pr_ROI\n",
      "R_p32pr_ROI\n",
      "R_a24_ROI\n",
      "R_d32_ROI\n",
      "R_8BM_ROI\n",
      "R_p32_ROI\n",
      "R_10r_ROI\n",
      "R_47m_ROI\n",
      "R_8Av_ROI\n",
      "R_8Ad_ROI\n",
      "R_9m_ROI\n",
      "R_8BL_ROI\n",
      "R_9p_ROI\n",
      "R_10d_ROI\n",
      "R_8C_ROI\n",
      "R_44_ROI\n",
      "R_45_ROI\n",
      "R_47l_ROI\n",
      "R_a47r_ROI\n",
      "R_6r_ROI\n",
      "R_IFJa_ROI\n",
      "R_IFJp_ROI\n",
      "R_IFSp_ROI\n",
      "R_IFSa_ROI\n",
      "R_p9-46v_ROI\n",
      "R_46_ROI\n",
      "R_a9-46v_ROI\n",
      "R_9-46d_ROI\n",
      "R_9a_ROI\n",
      "R_10v_ROI\n",
      "R_a10p_ROI\n",
      "R_11l_ROI\n",
      "R_13l_ROI\n",
      "R_OFC_ROI\n",
      "R_47s_ROI\n",
      "R_LIPd_ROI\n",
      "R_6a_ROI\n",
      "R_i6-8_ROI\n",
      "R_s6-8_ROI\n",
      "R_43_ROI\n",
      "R_OP4_ROI\n",
      "R_OP1_ROI\n",
      "R_OP2-3_ROI\n",
      "R_52_ROI\n",
      "R_RI_ROI\n",
      "R_PFcm_ROI\n",
      "R_PoI2_ROI\n",
      "R_TA2_ROI\n",
      "R_FOP4_ROI\n",
      "R_MI_ROI\n",
      "R_Pir_ROI\n",
      "R_AVI_ROI\n",
      "R_AAIC_ROI\n",
      "R_FOP1_ROI\n",
      "R_FOP3_ROI\n",
      "R_FOP2_ROI\n",
      "R_PFt_ROI\n",
      "R_AIP_ROI\n",
      "R_EC_ROI\n",
      "R_PreS_ROI\n",
      "R_ProS_ROI\n",
      "R_PeEc_ROI\n",
      "R_STGa_ROI\n",
      "R_PBelt_ROI\n",
      "R_A5_ROI\n",
      "R_PHA1_ROI\n",
      "R_PHA3_ROI\n",
      "R_STSda_ROI\n",
      "R_STSdp_ROI\n",
      "R_STSvp_ROI\n",
      "R_TGd_ROI\n",
      "R_TE1a_ROI\n",
      "R_TE1p_ROI\n",
      "R_TE2a_ROI\n",
      "R_TF_ROI\n",
      "R_TE2p_ROI\n",
      "R_PHT_ROI\n",
      "R_PH_ROI\n",
      "R_TPOJ1_ROI\n",
      "R_TPOJ2_ROI\n",
      "R_TPOJ3_ROI\n",
      "R_DVT_ROI\n",
      "R_PGp_ROI\n",
      "R_IP2_ROI\n",
      "R_IP1_ROI\n",
      "R_IP0_ROI\n",
      "R_PFop_ROI\n",
      "R_PF_ROI\n",
      "R_PFm_ROI\n",
      "R_PGi_ROI\n",
      "R_PGs_ROI\n",
      "R_V6A_ROI\n",
      "R_VMV1_ROI\n",
      "R_VMV3_ROI\n",
      "R_PHA2_ROI\n",
      "R_V4t_ROI\n",
      "R_FST_ROI\n",
      "R_V3CD_ROI\n",
      "R_LO3_ROI\n",
      "R_VMV2_ROI\n",
      "R_31pd_ROI\n",
      "R_31a_ROI\n",
      "R_VVC_ROI\n",
      "R_25_ROI\n",
      "R_s32_ROI\n",
      "R_pOFC_ROI\n",
      "R_PoI1_ROI\n",
      "R_Ig_ROI\n",
      "R_FOP5_ROI\n",
      "R_p10p_ROI\n",
      "R_p47r_ROI\n",
      "R_MBelt_ROI\n",
      "R_LBelt_ROI\n",
      "R_A4_ROI\n",
      "R_STSva_ROI\n",
      "R_TE1m_ROI\n",
      "R_PI_ROI\n",
      "R_a32pr_ROI\n",
      "R_p24_ROI\n",
      "L_10d_ROI\n",
      "R_10pp_ROI\n",
      "R_TGv_ROI\n",
      "L_V1_ROI\n",
      "L_MST_ROI\n",
      "L_V6_ROI\n",
      "L_V2_ROI\n",
      "L_V3_ROI\n",
      "L_V4_ROI\n",
      "L_V8_ROI\n",
      "L_4_ROI\n",
      "L_3b_ROI\n",
      "L_FEF_ROI\n",
      "L_PEF_ROI\n",
      "L_55b_ROI\n",
      "L_V3A_ROI\n",
      "L_RSC_ROI\n",
      "L_POS2_ROI\n",
      "L_V7_ROI\n",
      "L_IPS1_ROI\n",
      "L_FFC_ROI\n",
      "L_V3B_ROI\n",
      "L_LO1_ROI\n",
      "L_LO2_ROI\n",
      "L_PIT_ROI\n",
      "L_MT_ROI\n",
      "L_A1_ROI\n",
      "L_PSL_ROI\n",
      "L_SFL_ROI\n",
      "L_PCV_ROI\n",
      "L_STV_ROI\n",
      "L_7Pm_ROI\n",
      "L_7m_ROI\n",
      "L_POS1_ROI\n",
      "L_23d_ROI\n",
      "L_v23ab_ROI\n",
      "L_d23ab_ROI\n",
      "L_31pv_ROI\n",
      "L_5m_ROI\n",
      "L_5mv_ROI\n",
      "L_23c_ROI\n",
      "L_5L_ROI\n",
      "L_24dd_ROI\n",
      "L_24dv_ROI\n",
      "L_7AL_ROI\n",
      "L_SCEF_ROI\n",
      "L_6ma_ROI\n",
      "L_7Am_ROI\n",
      "L_7PL_ROI\n",
      "L_7PC_ROI\n",
      "L_LIPv_ROI\n",
      "L_VIP_ROI\n",
      "L_MIP_ROI\n",
      "L_1_ROI\n",
      "L_2_ROI\n",
      "L_3a_ROI\n",
      "L_6d_ROI\n",
      "L_6mp_ROI\n",
      "L_6v_ROI\n",
      "L_p24pr_ROI\n",
      "L_33pr_ROI\n",
      "L_a24pr_ROI\n",
      "L_p32pr_ROI\n",
      "L_a24_ROI\n",
      "L_d32_ROI\n",
      "L_8BM_ROI\n",
      "L_p32_ROI\n",
      "L_10r_ROI\n",
      "L_47m_ROI\n",
      "L_8Av_ROI\n",
      "L_8Ad_ROI\n",
      "L_9m_ROI\n",
      "L_8BL_ROI\n",
      "L_9p_ROI\n",
      "L_10d_ROI\n",
      "L_8C_ROI\n",
      "L_44_ROI\n",
      "L_45_ROI\n",
      "L_47l_ROI\n",
      "L_a47r_ROI\n",
      "L_6r_ROI\n",
      "L_IFJa_ROI\n",
      "L_IFJp_ROI\n",
      "L_IFSp_ROI\n",
      "L_IFSa_ROI\n",
      "L_p9-46v_ROI\n",
      "L_46_ROI\n",
      "L_a9-46v_ROI\n",
      "L_9-46d_ROI\n",
      "L_9a_ROI\n",
      "L_10v_ROI\n",
      "L_a10p_ROI\n",
      "L_10pp_ROI\n",
      "L_11l_ROI\n",
      "L_13l_ROI\n",
      "L_OFC_ROI\n",
      "L_47s_ROI\n",
      "L_LIPd_ROI\n",
      "L_6a_ROI\n",
      "L_i6-8_ROI\n",
      "L_s6-8_ROI\n",
      "L_43_ROI\n",
      "L_OP4_ROI\n",
      "L_OP1_ROI\n",
      "L_OP2-3_ROI\n",
      "L_52_ROI\n",
      "L_RI_ROI\n",
      "L_PFcm_ROI\n",
      "L_PoI2_ROI\n",
      "L_TA2_ROI\n",
      "L_FOP4_ROI\n",
      "L_MI_ROI\n",
      "L_Pir_ROI\n",
      "L_AVI_ROI\n",
      "L_AAIC_ROI\n",
      "L_FOP1_ROI\n",
      "L_FOP3_ROI\n",
      "L_FOP2_ROI\n",
      "L_PFt_ROI\n",
      "L_AIP_ROI\n",
      "L_EC_ROI\n",
      "L_PreS_ROI\n",
      "L_H_ROI\n",
      "L_ProS_ROI\n",
      "L_PeEc_ROI\n",
      "L_STGa_ROI\n",
      "L_PBelt_ROI\n",
      "L_A5_ROI\n",
      "L_PHA1_ROI\n",
      "L_PHA3_ROI\n",
      "L_STSda_ROI\n",
      "L_STSdp_ROI\n",
      "L_STSvp_ROI\n",
      "L_TGd_ROI\n",
      "L_TE1a_ROI\n",
      "L_TE1p_ROI\n",
      "L_TE2a_ROI\n",
      "L_TF_ROI\n",
      "L_TE2p_ROI\n",
      "L_PHT_ROI\n",
      "L_PH_ROI\n",
      "L_TPOJ1_ROI\n",
      "L_TPOJ2_ROI\n",
      "L_TPOJ3_ROI\n",
      "L_DVT_ROI\n",
      "L_PGp_ROI\n",
      "L_IP2_ROI\n",
      "L_IP1_ROI\n",
      "L_IP0_ROI\n",
      "L_PFop_ROI\n",
      "L_PF_ROI\n",
      "L_PFm_ROI\n",
      "L_PGi_ROI\n",
      "L_PGs_ROI\n",
      "L_V6A_ROI\n",
      "L_VMV1_ROI\n",
      "L_VMV3_ROI\n",
      "L_PHA2_ROI\n",
      "L_V4t_ROI\n",
      "L_FST_ROI\n",
      "L_V3CD_ROI\n",
      "L_LO3_ROI\n",
      "L_VMV2_ROI\n",
      "L_31pd_ROI\n",
      "L_31a_ROI\n",
      "L_VVC_ROI\n",
      "L_25_ROI\n",
      "L_s32_ROI\n",
      "L_pOFC_ROI\n",
      "L_PoI1_ROI\n",
      "L_Ig_ROI\n",
      "L_FOP5_ROI\n",
      "L_p10p_ROI\n",
      "L_p47r_ROI\n",
      "L_TGv_ROI\n",
      "L_MBelt_ROI\n",
      "L_LBelt_ROI\n",
      "L_A4_ROI\n",
      "L_STSva_ROI\n",
      "L_TE1m_ROI\n",
      "L_PI_ROI\n",
      "L_a32pr_ROI\n",
      "L_p24_ROI\n",
      "R_V1_ROI\n",
      "R_MST_ROI\n",
      "R_V6_ROI\n",
      "R_V2_ROI\n",
      "R_V3_ROI\n",
      "R_V4_ROI\n",
      "R_V8_ROI\n",
      "R_4_ROI\n",
      "R_3b_ROI\n",
      "R_FEF_ROI\n",
      "R_PEF_ROI\n",
      "R_55b_ROI\n",
      "R_V3A_ROI\n",
      "R_RSC_ROI\n",
      "R_POS2_ROI\n",
      "R_V7_ROI\n",
      "R_IPS1_ROI\n",
      "R_FFC_ROI\n",
      "R_V3B_ROI\n",
      "R_LO1_ROI\n",
      "R_LO2_ROI\n",
      "R_PIT_ROI\n",
      "R_MT_ROI\n",
      "R_A1_ROI\n",
      "R_PSL_ROI\n",
      "R_SFL_ROI\n",
      "R_PCV_ROI\n",
      "R_STV_ROI\n",
      "R_7Pm_ROI\n",
      "R_7m_ROI\n",
      "R_POS1_ROI\n",
      "R_23d_ROI\n",
      "R_v23ab_ROI\n",
      "R_d23ab_ROI\n",
      "R_31pv_ROI\n",
      "R_5m_ROI\n",
      "R_5mv_ROI\n",
      "R_23c_ROI\n",
      "R_5L_ROI\n",
      "R_24dd_ROI\n",
      "R_24dv_ROI\n",
      "R_7AL_ROI\n",
      "R_SCEF_ROI\n",
      "R_6ma_ROI\n",
      "R_7Am_ROI\n",
      "R_7PL_ROI\n",
      "R_7PC_ROI\n",
      "R_LIPv_ROI\n",
      "R_VIP_ROI\n",
      "R_MIP_ROI\n",
      "R_1_ROI\n",
      "R_2_ROI\n",
      "R_3a_ROI\n",
      "R_6d_ROI\n",
      "R_6mp_ROI\n",
      "R_6v_ROI\n",
      "R_p24pr_ROI\n",
      "R_33pr_ROI\n",
      "R_a24pr_ROI\n",
      "R_p32pr_ROI\n",
      "R_a24_ROI\n",
      "R_d32_ROI\n",
      "R_8BM_ROI\n",
      "R_p32_ROI\n",
      "R_10r_ROI\n",
      "R_47m_ROI\n",
      "R_8Av_ROI\n",
      "R_8Ad_ROI\n",
      "R_9m_ROI\n",
      "R_8BL_ROI\n",
      "R_9p_ROI\n",
      "R_10d_ROI\n",
      "R_8C_ROI\n",
      "R_44_ROI\n",
      "R_45_ROI\n",
      "R_47l_ROI\n",
      "R_a47r_ROI\n",
      "R_6r_ROI\n",
      "R_IFJa_ROI\n",
      "R_IFJp_ROI\n",
      "R_IFSp_ROI\n",
      "R_IFSa_ROI\n",
      "R_p9-46v_ROI\n",
      "R_46_ROI\n",
      "R_a9-46v_ROI\n",
      "R_9-46d_ROI\n",
      "R_9a_ROI\n",
      "R_10v_ROI\n",
      "R_a10p_ROI\n",
      "R_10pp_ROI\n",
      "R_11l_ROI\n",
      "R_13l_ROI\n",
      "R_OFC_ROI\n",
      "R_47s_ROI\n",
      "R_LIPd_ROI\n",
      "R_6a_ROI\n",
      "R_i6-8_ROI\n",
      "R_s6-8_ROI\n",
      "R_43_ROI\n",
      "R_OP4_ROI\n",
      "R_OP1_ROI\n",
      "R_OP2-3_ROI\n",
      "R_52_ROI\n",
      "R_RI_ROI\n",
      "R_PFcm_ROI\n",
      "R_PoI2_ROI\n",
      "R_TA2_ROI\n",
      "R_FOP4_ROI\n",
      "R_MI_ROI\n",
      "R_Pir_ROI\n",
      "R_AVI_ROI\n",
      "R_AAIC_ROI\n",
      "R_FOP1_ROI\n",
      "R_FOP3_ROI\n",
      "R_FOP2_ROI\n",
      "R_PFt_ROI\n",
      "R_AIP_ROI\n",
      "R_EC_ROI\n",
      "R_PreS_ROI\n",
      "R_H_ROI\n",
      "R_ProS_ROI\n",
      "R_PeEc_ROI\n",
      "R_STGa_ROI\n",
      "R_PBelt_ROI\n",
      "R_A5_ROI\n",
      "R_PHA1_ROI\n",
      "R_PHA3_ROI\n",
      "R_STSda_ROI\n",
      "R_STSdp_ROI\n",
      "R_STSvp_ROI\n",
      "R_TGd_ROI\n",
      "R_TE1a_ROI\n",
      "R_TE1p_ROI\n",
      "R_TE2a_ROI\n",
      "R_TF_ROI\n",
      "R_TE2p_ROI\n",
      "R_PHT_ROI\n",
      "R_PH_ROI\n",
      "R_TPOJ1_ROI\n",
      "R_TPOJ2_ROI\n",
      "R_TPOJ3_ROI\n",
      "R_DVT_ROI\n",
      "R_PGp_ROI\n",
      "R_IP2_ROI\n",
      "R_IP1_ROI\n",
      "R_IP0_ROI\n",
      "R_PFop_ROI\n",
      "R_PF_ROI\n",
      "R_PFm_ROI\n",
      "R_PGi_ROI\n",
      "R_PGs_ROI\n",
      "R_V6A_ROI\n",
      "R_VMV1_ROI\n",
      "R_VMV3_ROI\n",
      "R_PHA2_ROI\n",
      "R_V4t_ROI\n",
      "R_FST_ROI\n",
      "R_V3CD_ROI\n",
      "R_LO3_ROI\n",
      "R_VMV2_ROI\n",
      "R_31pd_ROI\n",
      "R_31a_ROI\n",
      "R_VVC_ROI\n",
      "R_25_ROI\n",
      "R_s32_ROI\n",
      "R_pOFC_ROI\n",
      "R_PoI1_ROI\n",
      "R_Ig_ROI\n",
      "R_FOP5_ROI\n",
      "R_p10p_ROI\n",
      "R_p47r_ROI\n",
      "R_TGv_ROI\n",
      "R_MBelt_ROI\n",
      "R_LBelt_ROI\n",
      "R_A4_ROI\n",
      "R_STSva_ROI\n",
      "R_TE1m_ROI\n",
      "R_PI_ROI\n",
      "R_a32pr_ROI\n",
      "R_p24_ROI\n",
      "creating gray matter references complete\n"
     ]
    }
   ],
   "source": [
    "# cortical measures: glasser\n",
    "if os.path.isfile(data_dir+'/cortical_glasser_reference_preclean.csv'):\n",
    "    cortical_glasser = pd.read_csv(data_dir+'/cortical_glasser_reference_preclean.csv')\n",
    "else:\n",
    "    cortical_glasser =  collectData('neuro/parc-stats','cortex_mapping_stats','glasser','parc_MEAN.csv',subjects_data,colors,data_dir+'/cortical_glasser_reference_preclean.csv')\n",
    "    cortical_glasser = cortical_glasser.drop_duplicates()\n",
    "    cortical_glasser['subjectID'] = cortical_glasser.subjectID.astype(str)\n",
    "    cortical_glasser = cortical_glasser.drop(columns={'volume','thickness','snr'})\n",
    "    bad_cortical_glasser = pd.DataFrame()\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['ad'] > 3]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['ad'] < 0]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['fa'] > 1]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['fa'] < 0]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['md'] > 3]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['md'] < 0]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['rd'] > 3]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['rd'] < 0]])\n",
    "    cortical_glasser = cortical_glasser.loc[~cortical_glasser.index.isin(bad_cortical_glasser.index)]\n",
    "    cortical_glasser.to_csv(data_dir+'/cortical_glasser_reference_preclean.csv',index=False)\n",
    "    \n",
    "cortical_glasser_diffusion_distances, cortical_glasser_diffusion_outliers, cortical_glasser_diffusion_reference_dataframe, cortical_glasser_diffusion_reference_json = outlierDetection(cortical_glasser,cortical_glasser.structureID.unique(),'structureID',cortical_diffusion_measures,95,'emd',True,False,\"\",data_dir+'/references','cortical_glasser_diffusion_reference_camcan')\n",
    "\n",
    "# # # freesurfer measures: glasser\n",
    "if os.path.isfile(data_dir+'/cortical_glasser_volume_reference_preclean.csv'):\n",
    "    cortical_glasser_parcel_volume = pd.read_csv(data_dir+'/cortical_glasser_volume_reference_preclean.csv')\n",
    "else:\n",
    "    cortical_glasser_parcel_volume = collectData('neuro/parc-stats','surface','glasser','cortex.csv',subjects_data,colors,data_dir+'/cortical_glasser_volume_reference_preclean.csv')\n",
    "    cortical_glasser_parcel_volume = cortical_glasser_parcel_volume.drop_duplicates()\n",
    "    cortical_glasser_parcel_volume['subjectID'] = cortical_glasser_parcel_volume.subjectID.astype(str)\n",
    "    cortical_glasser_parcel_volume = cortical_glasser_parcel_volume.loc[~cortical_glasser_parcel_volume['structureID'].isin([\"lh_unknown_0\",\"rh_unknown_0\",\"lh_???\",\"rh_???\"])]\n",
    "    cortical_glasser_parcel_volume['structureID'] = [ f.split('h_')[1] for f in cortical_glasser_parcel_volume['structureID'] ]\n",
    "    cortical_glasser_parcel_volume.to_csv(data_dir+'/cortical_glasser_volume_reference_preclean.csv',index=False)\n",
    "    \n",
    "# cortical_glasser_reference = pd.merge(cortical_glasser,cortical_glasser_parcel_volume,on=['subjectID','structureID','classID','nodeID'])\n",
    "# cortical_glasser_distances, cortical_glasser_outliers, cortical_glasser_reference_dataframe, cortical_glasser_reference_json = outlierDetection(cortical_glasser_reference,cortical_glasser_reference.structureID.unique(),'structureID',cortical_measures,95,'emd',True,False,\"\",data_dir,'cortical_glasser_reference_hcp')\n",
    "\n",
    "cortical_glasser_structural_distances, cortical_glasser_structural_outliers, cortical_glasser_structural_reference_dataframe, cortical_glasser_structural_reference_json = outlierDetection(cortical_glasser_parcel_volume,cortical_glasser_parcel_volume.structureID.unique(),'structureID',cortical_structural_measures,95,'emd',True,False,\"\",data_dir+'/references','cortical_glasser_structural_reference_camcan')\n",
    "\n",
    "\n",
    "# combine glasser and aparc\n",
    "# cortical_reference_pre = pd.concat([cortical_aparc_reference,cortical_glasser_reference],ignore_index=True)\n",
    "\n",
    "# # generate cortical reference using glasser and aparc\n",
    "# cortical_distances, cortical_outliers, cortical_reference_dataframe, cortical_reference_json = outlierDetection(cortical_reference_pre,cortical_reference_pre.structureID.unique(),'structureID',cortical_measures,95,'emd',True,False,\"\",data_dir,'cortical_reference_hcp')\n",
    "\n",
    "print(\"creating gray matter references complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating gray matter references\n",
      "creating gray matter references complete\n"
     ]
    }
   ],
   "source": [
    "# #### gray matter mapping reference\n",
    "# print(\"creating gray matter references\")\n",
    "\n",
    "# # create data structures\n",
    "# # subcortical measures\n",
    "# if os.path.isfile(data_dir+'/subcortical_reference_preclean.csv'):\n",
    "#     subcortical = pd.read_csv(data_dir+'/subcortical_reference_preclean.csv')\n",
    "# else:\n",
    "#     subcortical =  collectData('neuro/parc-stats','subcort_stats','noddi','aseg_nodes.csv',subjects_data,colors,data_dir+'/subcortical_reference_preclean.csv')\n",
    "#     subcortical = subcortical.drop_duplicates()\n",
    "#     bad_subcortical = pd.DataFrame()\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] > 3]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] < 0]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] > 1]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] < 0]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] > 3]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] < 0]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] > 3]])\n",
    "#     bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] < 0]])\n",
    "#     subcortical = subcortical.loc[~subcortical.index.isin(bad_subcortical.index)]\n",
    "#     subcortical.rename(columns={'volume': 'gray_matter_volume_mm^3'},inplace=True)\n",
    "#     subcortical.to_csv(data_dir+'/subcortical_reference_preclean.csv',index=False)\n",
    "\n",
    "# # # # compute reference dataset for subcortical structures\n",
    "# subcortical_measures = [ f for f in subcortical.keys() if f not in ['subjectID','structureID','nodeID','classID','parcID','number_of_voxels'] ]\n",
    "# # subcortical_distances, subcortical_outliers, subcortical_reference_dataframe, subcortical_reference_json = outlierDetection(subcortical,subcortical.structureID.unique(),'structureID',subcortical_measures,99,'emd',True,False,\"\",data_dir,'subcortical_reference_camcan')\n",
    "\n",
    "# # cortical measures: aparc\n",
    "# # if os.path.isfile(data_dir+'/cortical_aparc_reference_preclean.csv'):\n",
    "# #     cortical_aparc = pd.read_csv(data_dir+'/cortical_aparc_reference_preclean.csv')\n",
    "# # else:\n",
    "# #     cortical_aparc =  collectData('neuro/parc-stats','cortex_mapping_stats','glasser','aparc_MEAN.csv',subjects_data,colors,data_dir+'/cortical_aparc_reference_preclean.csv')\n",
    "# #     cortical_aparc = cortical_aparc.drop_duplicates()\n",
    "# #     cortical_aparc = cortical_aparc.drop(columns={'volume','thickness','snr'})\n",
    "# #     bad_cortical_aparc = pd.DataFrame()\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['ad'] > 3]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['ad'] < 0]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['fa'] > 1]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['fa'] < 0]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['md'] > 3]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['md'] < 0]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['rd'] > 3]])\n",
    "# #     bad_cortical_aparc = pd.concat([bad_cortical_aparc,cortical_aparc.loc[cortical_aparc['rd'] < 0]])\n",
    "# #     cortical_aparc = cortical_aparc.loc[~cortical_aparc.index.isin(bad_cortical_aparc.index)]\n",
    "# #     cortical_aparc.to_csv(data_dir+'/cortical_aparc_reference_preclean.csv',index=False)\n",
    "    \n",
    "# # # freesurfer measures: aparc\n",
    "# # if os.path.isfile(data_dir+'/cortical_aparc_volume_reference_preclean.csv'):\n",
    "# #     cortical_aparc_parcel_volume = pd.read_csv(data_dir+'/cortical_aparc_volume_reference_preclean.csv')\n",
    "# # else:\n",
    "# #     cortical_aparc_parcel_volume = collectData('neuro/parc-stats','freesurfer','aparc.a2009s','cortex.csv',subjects_data,colors,data_dir+'/cortical_aparc_volume_reference_preclean.csv')\n",
    "# #     cortical_aparc_parcel_volume = cortical_aparc_parcel_volume.drop_duplicates()\n",
    "# #     cortical_aparc_parcel_volume.to_csv(data_dir+'/cortical_aparc_volume_reference_preclean.csv',index=False)\n",
    "\n",
    "# # cortical_aparc_reference = pd.merge(cortical_aparc,cortical_aparc_parcel_volume,on=['subjectID','structureID','classID','nodeID'])\n",
    "# # cortical_measures = [ f for f in cortical_aparc_reference.keys() if f not in ['subjectID','structureID','nodeID','classID','parcID'] ]\n",
    "# # cortical_aparc_distances, cortical_aparc_outliers, cortical_aparc_reference_dataframe, cortical_aparc_reference_json = outlierDetection(cortical_aparc_reference,cortical_aparc_reference.structureID.unique(),'structureID',cortical_measures,95,'emd',True,False,\"\",data_dir,'cortical_aparc_reference_camcan')\n",
    "\n",
    "# # # cortical measures: glasser\n",
    "# # if os.path.isfile(data_dir+'/cortical_glasser_reference_preclean.csv'):\n",
    "# #     cortical_glasser = pd.read_csv(data_dir+'/cortical_glasser_reference_preclean.csv')\n",
    "# # else:\n",
    "# #     cortical_glasser =  collectData('neuro/parc-stats','cortex_mapping_stats','glasser','parc_MEAN.csv',subjects_data,colors,data_dir+'/cortical_glasser_reference_preclean.csv')\n",
    "# #     cortical_glasser = cortical_glasser.drop_duplicates()\n",
    "# #     cortical_glasser = cortical_glasser.drop(columns={'volume','thickness','snr'})\n",
    "# #     bad_cortical_glasser = pd.DataFrame()\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['ad'] > 3]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['ad'] < 0]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['fa'] > 1]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['fa'] < 0]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['md'] > 3]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['md'] < 0]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['rd'] > 3]])\n",
    "# #     bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['rd'] < 0]])\n",
    "# #     cortical_glasser = cortical_glasser.loc[~cortical_glasser.index.isin(bad_cortical_glasser.index)]\n",
    "# #     cortical_glasser.to_csv(data_dir+'/cortical_glasser_reference_preclean.csv',index=False)\n",
    "    \n",
    "# # # # # freesurfer measures: glasser\n",
    "# # if os.path.isfile(data_dir+'/cortical_glasser_volume_reference_preclean.csv'):\n",
    "# #     cortical_glasser_parcel_volume = pd.read_csv(data_dir+'/cortical_glasser_volume_reference_preclean.csv')\n",
    "# # else:\n",
    "# #     cortical_glasser_parcel_volume = collectData('neuro/parc-stats','surface','glasser','cortex.csv',subjects_data,colors,data_dir+'/cortical_glasser_volume_reference_preclean.csv')\n",
    "# #     cortical_glasser_parcel_volume = cortical_glasser_parcel_volume.drop_duplicates()\n",
    "# #     cortical_glasser_parcel_volume = cortical_glasser_parcel_volume.loc[~cortical_glasser_parcel_volume['structureID'].isin([\"lh_unknown_0\",\"rh_unknown_0\",\"lh_???\",\"rh_???\"])]\n",
    "# #     cortical_glasser_parcel_volume['structureID'] = [ f.split('h_')[1] for f in cortical_glasser_parcel_volume['structureID'] ]\n",
    "# #     cortical_glasser_parcel_volume.to_csv(data_dir+'/cortical_glasser_volume_reference_preclean.csv',index=False)\n",
    "    \n",
    "# # cortical_glasser_reference = pd.merge(cortical_glasser,cortical_glasser_parcel_volume,on=['subjectID','structureID','classID','nodeID'])\n",
    "# # cortical_glasser_distances, cortical_glasser_outliers, cortical_glasser_reference_dataframe, cortical_glasser_reference_json = outlierDetection(cortical_glasser_reference,cortical_glasser_reference.structureID.unique(),'structureID',cortical_measures,95,'emd',True,False,\"\",data_dir,'cortical_glasser_reference_camcan')\n",
    "\n",
    "# # combine glasser and aparc\n",
    "# # cortical_reference_pre = pd.concat([cortical_aparc_reference,cortical_glasser_reference],ignore_index=True)\n",
    "\n",
    "# # generate cortical reference using glasser and aparc\n",
    "# # cortical_distances, cortical_outliers, cortical_reference_dataframe, cortical_reference_json = outlierDetection(cortical_reference_pre,cortical_reference_pre.structureID.unique(),'structureID',cortical_measures,95,'emd',True,False,\"\",data_dir,'cortical_reference_camcan')\n",
    "\n",
    "# print(\"creating gray matter references complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tract profile references\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fa'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-44e130af41c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrack_reference_pre\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcollectData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neuro/tractmeasures'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'macro_micro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dti'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'output_FiberStats.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubjects_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/tractmeasures-reference_preclean.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrack_reference_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_reference_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrack_reference_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_reference_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtrack_reference_pre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrack_reference_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/tractmeasures-reference_preclean.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fa'"
     ]
    }
   ],
   "source": [
    "#### tractometry reference generation\n",
    "print(\"creating tract profile references\")\n",
    "\n",
    "# create data structures\n",
    "# tractmeasures data\n",
    "if os.path.isfile(data_dir+'/tractmeasures-reference_preclean.csv'):\n",
    "    track_reference_pre = pd.read_csv(data_dir+'/tractmeasures-reference_preclean.csv')\n",
    "else:\n",
    "    track_reference_pre =  collectData('neuro/tractmeasures','macro_micro','dti','output_FiberStats.csv',subjects_data,colors,data_dir+'/tractmeasures-reference_preclean.csv')\n",
    "    track_reference_pre = track_reference_pre.drop_duplicates()\n",
    "    track_reference_pre = track_reference_pre.loc[~track_reference_pre['fa'].isna()]\n",
    "    track_reference_pre.to_csv(data_dir+'/tractmeasures-reference_preclean.csv',index=False)\n",
    "    \n",
    "# track_measures = [ f for f in track_reference_pre.keys() if f not in ['subjectID','structureID','nodeID','classID','sex'] ]\n",
    "track_structural_measures = ['length','volume','count']\n",
    "track_diffusion_measures = ['ad','fa','md','rd','ndi','odi','isovf']\n",
    "\n",
    "# generate tractmeasures reference and outliers\n",
    "track_structural_distances, track_structural_outliers, track_structural_reference_dataframe, track_structural_reference_json = outlierDetection(track_reference_pre,track_reference_pre.structureID.unique(),'nodeID',track_structural_measures,95,'euclidean',True,False,\"\",data_dir+'/references','tractmeasures_structural_reference_camcan')\n",
    "track_diffusion_distances, track_diffusion_outliers, track_diffusion_reference_dataframe, track_diffusion_reference_json = outlierDetection(track_reference_pre,track_reference_pre.structureID.unique(),'nodeID',track_diffusion_measures,95,'euclidean',True,True,50,data_dir+'/references','tractmeasures_diffusion_reference_camcan')\n",
    "\n",
    "print(\"creating tract profile references complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_reference_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating tract profile references\n",
      "creating tract profile references complete\n"
     ]
    }
   ],
   "source": [
    "#### tractometry reference generation\n",
    "print(\"creating tract profile references\")\n",
    "\n",
    "# create data structures\n",
    "# tractmeasures data\n",
    "# if os.path.isfile(data_dir+'/tractmeasures-reference_preclean.csv'):\n",
    "track_reference_pre = pd.read_csv(data_dir+'/tractmeasures-reference_preclean.csv')\n",
    "# else:\n",
    "# track_reference_pre =  collectData('neuro/tractmeasures','macro_micro','noddi','output_FiberStats.csv',subjects_data,colors,data_dir+'/tractmeasures-reference_preclean.csv')\n",
    "# track_reference_pre = track_reference_pre.drop_duplicates()\n",
    "# track_reference_pre['subjectID'] = track_reference_pre.subjectID.astype(str)\n",
    "# track_reference_pre = track_reference_pre.loc[~track_reference_pre['fa'].isna()]\n",
    "# for i in track_reference_pre.structureID.unique():\n",
    "#     subj_data = track_reference_pre.loc[track_reference_pre['structureID'] == i]\n",
    "#     incorrect_node_count = subj_data.groupby('subjectID',sort=False).apply(lambda x: False if len(x['nodeID'].values) < 200 else True).reset_index()\n",
    "#     track_reference_pre.loc[track_reference_pre['structureID'] == i] = track_reference_pre.loc[track_reference_pre['structureID'] == i].loc[track_reference_pre['subjectID'].isin(incorrect_node_count.loc[incorrect_node_count[0] == True]['subjectID'].tolist())]\n",
    "# track_reference_pre.to_csv(data_dir+'/tractmeasures-reference_preclean.csv',index=False)\n",
    "\n",
    "# track_structural_measures = ['length','volume','count']\n",
    "# track_diffusion_measures = ['ad','fa','md','rd','ndi','odi','isovf']\n",
    "\n",
    "# # generate tractmeasures reference and outliers\n",
    "# track_structural_distances, track_structural_outliers, track_structural_reference_dataframe, track_structural_reference_json = outlierDetection(track_reference_pre,track_reference_pre.structureID.unique(),'nodeID',track_structural_measures,95,'euclidean',True,False,\"\",data_dir+'/references','tractmeasures_structural_reference_camcan')\n",
    "# track_diffusion_distances, track_diffusion_outliers, track_diffusion_reference_dataframe, track_diffusion_reference_json = outlierDetection(track_reference_pre,track_reference_pre.structureID.unique(),'nodeID',track_diffusion_measures,95,'euclidean',True,True,50,data_dir+'/references','tractmeasures_diffusion_reference_camcan')\n",
    "  \n",
    "# track_measures = [ f for f in track_reference_pre.keys() if f not in ['subjectID','structureID','nodeID','classID'] ]\n",
    "    \n",
    "# # generate tractmeasures reference and outliers\n",
    "# track_distances, track_outliers, track_reference_dataframe, track_reference_json = outlierDetection(track_reference_pre,track_reference_pre.structureID.unique(),'nodeID',track_measures,95,'euclidean',True,True,50,data_dir,'tractmeasures_reference_camcan')\n",
    "\n",
    "print(\"creating tract profile references complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>structureID</th>\n",
       "      <th>nodeID</th>\n",
       "      <th>ad</th>\n",
       "      <th>fa</th>\n",
       "      <th>md</th>\n",
       "      <th>rd</th>\n",
       "      <th>ndi</th>\n",
       "      <th>isovf</th>\n",
       "      <th>odi</th>\n",
       "      <th>count</th>\n",
       "      <th>length</th>\n",
       "      <th>volume</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subjectID, structureID, nodeID, ad, fa, md, rd, ndi, isovf, odi, count, length, volume, classID]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_reference_pre = track_reference_pre.loc[track_reference_pre['structureID'] != np.nan]\n",
    "track_reference_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect_node_count['subjectID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
