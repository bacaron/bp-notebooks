{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subjects\n",
    "def collectSubjectData(topPath,dataPath,groups,subjects,colors):\n",
    "\n",
    "    # set up variables\n",
    "    data_columns = ['subjectID','classID','colors']\n",
    "    data =  pd.DataFrame([],columns=data_columns)\n",
    "\n",
    "    # populate structure\n",
    "    data['subjectID'] = [ f for g in groups for f in subjects[g] ]\n",
    "    data['classID'] = [ g for g in groups for f in range(len(subjects[g]))]\n",
    "    data['colors'] = [ colors[c] for c in colors for f in subjects[c]]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'subjects.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### color dictionary\n",
    "def createColorDictionary(data,measure,colorPalette):\n",
    "\n",
    "    keys = data[measure].unique()\n",
    "    values = sns.color_palette(colorPalette,len(keys))\n",
    "    values = values.as_hex()\n",
    "\n",
    "    colors_dict = dict(zip(keys,values))\n",
    "\n",
    "    return colors_dict\n",
    "\n",
    "### load parcellation stats data \n",
    "### load data \n",
    "def collectData(datatype,datatype_tags,tags,filename,subjects_data,colors,outPath):\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "\n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    paths = []\n",
    "\n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            if datatype_tags in obj['output']['datatype_tags']:\n",
    "    #                 if tags in obj['output']['tags']:\n",
    "                if set(tags).issubset(obj['output']['tags']):\n",
    "\n",
    "                    subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                    paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "                elif '!' in str(tags):\n",
    "                    tag = [ f for f in tags if '!' in str(f) ]\n",
    "                    tag_drop = [ f for f in tags if f not in tag ]\n",
    "                    if not set([ f.replace('!','') for f in tag]).issubset(obj['output']['tags']):\n",
    "                        if set(tag_drop).issubset(obj['output']['tags']):\n",
    "                            subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                            paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "\n",
    "    paths = [y for _,y in sorted(zip(subjects,paths))]\n",
    "    subjects = [x for x,_ in sorted(zip(subjects,paths))]\n",
    "\n",
    "    for i in paths:\n",
    "        if '.json.gz' in filename:\n",
    "            tmpdata = pd.read_json(i,orient='index').reset_index(drop=True)\n",
    "            tmpdata['subjectID'] = [ str(subjects[f]) for f in range(len(subjects)) if i == paths[f]]\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data)\n",
    "        else:\n",
    "            tmpdata = pd.read_csv(i)\n",
    "        if tmpdata.subjectID.dtypes != 'object':\n",
    "            tmpdata['subjectID'] = [ str(int(np.float(f))) for f in tmpdata.subjectID ]\n",
    "    #         if 'classID' in tmpdata.keys():\n",
    "    #             tmpdata = pd.merge(tmpdata,subjects_data,on=['subjectID','classID'])\n",
    "    #         else:\n",
    "    #             tmpdata = pd.merge(tmpdata,subjects_data,on='subjectID')\n",
    "        data = data.append(tmpdata,ignore_index=True)\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # drop duplicates\n",
    "#     data = data.drop_duplicates('subjectID')\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    # subjects.csv\n",
    "    data.to_csv(outPath,index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def collectNetworkData(datatype,datatype_tags,tags,corr_filename,labels_filename,subjects_data,colors,outPath):\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "\n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    csv_paths = []\n",
    "    label_paths = []\n",
    "\n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            if datatype_tags in obj['output']['datatype_tags']:\n",
    "                if tags in obj['output']['tags']:\n",
    "                    subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                    csv_paths = np.append(csv_paths,\"input/\"+obj[\"path\"]+\"/\"+corr_filename)\n",
    "                    label_paths = np.append(label_paths,\"input/\"+obj[\"path\"]+\"/\"+labels_filename)\n",
    "\n",
    "    # sort paths by subject order\n",
    "    subjects = [x for _,x in sorted(zip(subjects,subjects))]\n",
    "    csv_paths = [x for _,x in sorted(zip(subjects,csv_paths))]\n",
    "    label_paths = [x for _,x in sorted(zip(subjects,label_paths))]\n",
    "\n",
    "    for i in range(len(csv_paths)):\n",
    "        tmplabel = pd.read_json(label_paths[i])\n",
    "        label_names = [ f for f in tmplabel['name'] if f not in ['self-loop'] ]\n",
    "        tmpdata = pd.read_csv(csv_paths[i],names=label_names)\n",
    "        tmpdata.index = label_names\n",
    "        tmpdata['subjectID'] = [ subjects[i] for f in range(len(tmpdata)) ]\n",
    "        if tmpdata.subjectID.dtypes != 'object':\n",
    "            tmpdata['subjectID'] = [ str(int(np.float(f))) for f in tmpdata.subjectID ]\n",
    "        if 'classID' in tmpdata.keys():\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on=['subjectID','classID'],right_index=True)\n",
    "        else:\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on='subjectID',right_index=True)\n",
    "        data = data.append(tmpdata)\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    # subjects.csv\n",
    "    data.to_csv(outPath)\n",
    "\n",
    "    return data\n",
    "\n",
    "### cut nodes\n",
    "def cutNodes(data,num_nodes,dataPath,foldername,savename):\n",
    "\n",
    "    # identify inner n nodes based on num_nodes input\n",
    "    total_nodes = len(data['nodeID'].unique())\n",
    "    cut_nodes = int((total_nodes - num_nodes) / 2)\n",
    "\n",
    "    # remove cut_nodes from dataframe\n",
    "    data = data[data['nodeID'].between((cut_nodes)+1,(num_nodes+cut_nodes))]\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'/'+foldername+'-'+savename+'.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def computeMeanData(dataPath,data,outname):\n",
    "\n",
    "    # make mean data frame\n",
    "    data_mean =  data.groupby(['subjectID','classID','structureID']).mean().reset_index()\n",
    "    data_mean['nodeID'] = [ 1 for f in range(len(data_mean['nodeID'])) ]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data_mean.to_csv(dataPath+outname+'.csv',index=False)\n",
    "\n",
    "    return data_mean\n",
    "\n",
    "### rank order effect size calculator\n",
    "def computeRankOrderEffectSize(groups,subjects,tissue,measures,stat,measures_to_average,data_dir):\n",
    "\n",
    "    comparison_array = list(combinations(groups,2)) # 2 x 2 array; 2 different comparisons, with two pairs per comparison. comparison_array[0] = (\"run_1\",\"run_2\")\n",
    "    es = {}\n",
    "    roes = {}\n",
    "\n",
    "    # compute effect size\n",
    "    for compar in comparison_array:\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.DataFrame([])\n",
    "        tmp = pd.DataFrame([])\n",
    "        tmp['structureID'] = stat['structureID'].unique()\n",
    "        for m in measures:\n",
    "            diff = stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').mean() - stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').mean()\n",
    "            pooled_var = (np.sqrt((stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').std() ** 2 + stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').std() ** 2) / 2))\n",
    "            effectSize = diff / pooled_var\n",
    "            tmp[m+\"_effect_size\"] = list(effectSize[m])\n",
    "        tmp.to_csv(data_dir+tissue+\"_effect_sizes_\"+compar[0]+\"_\"+compar[1]+\".csv\",index=False)\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.concat([es[compar[0]+\"_\"+compar[1]],tmp],ignore_index=True)\n",
    "\n",
    "    # rank order structures\n",
    "    for ma in measures_to_average:\n",
    "        if ma == ['ad','fa','md','rd','ga','ak','mk','rk']:\n",
    "            model = 'tensor'\n",
    "        elif ma == ['ndi','isovf','odi']:\n",
    "            model = 'noddi'\n",
    "        else:\n",
    "            model = ma\n",
    "\n",
    "        tmpdata = pd.DataFrame([])\n",
    "        tmpdata['structureID'] = stat['structureID'].unique()\n",
    "        for compar in comparison_array:\n",
    "            if model == 'tensor':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ad_effect_size','fa_effect_size','md_effect_size','rd_effect_size']].abs().mean(axis=1).tolist()\n",
    "            elif model == 'noddi':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ndi_effect_size','isovf_effect_size','odi_effect_size']].abs().mean(axis=1).tolist()\n",
    "            else:\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][[ma+'_effect_size']].abs().mean(axis=1).tolist()\n",
    "\n",
    "        tmpdata[model+\"_average_effect_size\"] =  tmpdata.mean(axis=1).tolist()\n",
    "        tmpdata.to_csv(data_dir+model+\"_average_\"+tissue+\"_effect_sizes.csv\",index=False)\n",
    "        roes[model] = tmpdata.sort_values(by=model+\"_average_effect_size\")['structureID'].tolist()\n",
    "\n",
    "    return roes\n",
    "\n",
    "def combineCorticalSubcortical(dataPath,corticalData,subcorticalData):\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    corticalData = corticalData.drop(columns=['snr','thickness'])\n",
    "    subcorticalData = subcorticalData.drop(columns=['parcID','number_of_voxels'])\n",
    "\n",
    "    # merge data frames\n",
    "    data = pd.concat([corticalData,subcorticalData],sort=False)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'graymatter_nodes.csv',index=False)\n",
    "\n",
    "    # identify gray matter names\n",
    "    graymatter_names = list(data['structureID'].unique())\n",
    "\n",
    "    # output track names\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    with open((dataPath+'graymatter_list.json'),'w') as gm_listf:\n",
    "        json.dump(graymatter_names,gm_listf)\n",
    "\n",
    "    return [graymatter_names,data]\n",
    "\n",
    "def computeDistance(x,y,metric):\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    if metric == 'euclidean':\n",
    "        dist = euclidean_distances([x,y])[0][1]\n",
    "    else:\n",
    "        dist = wasserstein_distance(x,y)\n",
    "        \n",
    "    return dist\n",
    "\n",
    "def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "    references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "    references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "    references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "    return references_mean, references_sd\n",
    "\n",
    "def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "    dist = []\n",
    "    subj = []\n",
    "    meas = []\n",
    "    struc = []\n",
    "\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        subj_data = data.loc[data['structureID'] == i]\n",
    "        references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "        for m in measures:\n",
    "            for s in subj_data.subjectID.unique():\n",
    "                x = list(subj_data.loc[subj_data['subjectID'] == s][m].values.tolist())\n",
    "                y = list(references_data[0][m].values.tolist())\n",
    "                dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "                subj = np.append(subj,s)\n",
    "                meas = np.append(meas,m)\n",
    "                struc = np.append(struc,i)\n",
    "\n",
    "    dist_dataframe = pd.DataFrame()\n",
    "    dist_dataframe['subjectID'] = subj\n",
    "    dist_dataframe['structureID'] = struc\n",
    "    dist_dataframe['measures'] = meas\n",
    "    dist_dataframe['distance'] = dist\n",
    "    \n",
    "    return dist_dataframe\n",
    "\n",
    "def buildReferenceData(data,outliers,profile):\n",
    "    \n",
    "    reference_data = pd.DataFrame()\n",
    "    \n",
    "    for s in outliers.structureID.unique():\n",
    "        for m in outliers.measures.unique():\n",
    "            if profile:\n",
    "                tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID','nodeID',m]].reset_index(drop=True)\n",
    "            else:\n",
    "                tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID',m]].reset_index(drop=True)\n",
    "            reference_data = pd.concat([reference_data,tmpdata])\n",
    "    if not profile:\n",
    "        reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "    \n",
    "    return reference_data\n",
    "\n",
    "def computeOutliers(distances,threshold):\n",
    "    \n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    for i in distances.structureID.unique():\n",
    "        for m in distances.measures.unique():\n",
    "            tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "            outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "    return outliers\n",
    "\n",
    "def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers):\n",
    "    \n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    outliers_subjects = []\n",
    "    outliers_structures = []\n",
    "    outliers_measures = []\n",
    "    outliers_metrics = []\n",
    "\n",
    "    distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "    outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "    if build_outliers:\n",
    "        if 'nodeID' in data.columns:\n",
    "            reference_dataframe = buildReferenceData(data,outliers_dataframe,True)\n",
    "        else:\n",
    "            reference_dataframe = buildReferenceData(data,outliers_dataframe,False)\n",
    "    else:\n",
    "        reference_dataframe = []\n",
    "        \n",
    "    return distances, outliers_dataframe, reference_dataframe\n",
    "\n",
    "def profileFlipCheck(data,subjects,structures,test_measure,flip_measures,dist_metric,threshold,outPath):\n",
    "    \n",
    "    flipped_subjects = []\n",
    "    flipped_structures = []\n",
    "    distance = []\n",
    "    flipped_distance = []\n",
    "    \n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        struc_data = data.loc[data['structureID'] == i]\n",
    "        references_data = computeReferences(struc_data,'nodeID','nodeID',flip_measures)\n",
    "        differences = []\n",
    "        dist = []\n",
    "        dist_flipped = []\n",
    "\n",
    "        for s in subjects:\n",
    "            subj_data = struc_data.loc[data['subjectID'] == s]\n",
    "            x = list(subj_data[test_measure].values.tolist())\n",
    "            y = list(references_data[0][test_measure].values.tolist())\n",
    "            dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "            dist_flipped = np.append(dist_flipped,computeDistance(list(np.flip(x)),y,dist_metric))\n",
    "            differences =  np.append(differences,(dist[-1]-dist_flipped[-1]))\n",
    "        \n",
    "        percentile_threshold = np.percentile(differences,threshold)\n",
    "#         print(percentile_threshold)\n",
    "        for m in range(len(differences)):\n",
    "            if differences[m] > 0 and differences[m] > percentile_threshold:\n",
    "#             if differences[m] > percentile_threshold:\n",
    "#                 print(subjects[m])\n",
    "                flipped_subjects = np.append(flipped_subjects,subjects[m])\n",
    "                flipped_structures = np.append(flipped_structures,i)\n",
    "                distance = np.append(distance,dist[m])\n",
    "                flipped_distance = np.append(flipped_distance,dist_flipped[m])\n",
    "    \n",
    "    output_summary = pd.DataFrame()\n",
    "    output_summary['flipped_subjects'] = flipped_subjects\n",
    "    output_summary['flipped_structures'] = flipped_structures\n",
    "    output_summary['distance'] = distance\n",
    "    output_summary['flipped_distance'] = flipped_distance\n",
    "    \n",
    "    if outPath:\n",
    "        output_summary.to_csv(outPath+'_flipped_profiles.csv',index=False)\n",
    "    \n",
    "    return output_summary\n",
    "\n",
    "### scatter plot related scripts\n",
    "# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure\n",
    "def averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X = x_stat.groupby(measure).mean()[x_measure].tolist()\n",
    "    Y = y_stat.groupby(measure).mean()[y_measure].tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure\n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X,Y = [np.array([]),np.array([])]\n",
    "    for i in range(len(x_stat[measure].unique())):\n",
    "        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]\n",
    "        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]\n",
    "\n",
    "        if np.isnan(x).any() or np.isnan(y).any():\n",
    "            print(\"skipping %s due to nan\" %x_stat[measure].unique()[i])\n",
    "        else:\n",
    "            # checks to make sure the same data\n",
    "            if len(x) == len(y):\n",
    "                X = np.append(X,x)\n",
    "                Y = np.append(Y,y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject\n",
    "def ravelNetwork(x_stat,y_stat):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    X = np.ravel(x_stat).tolist()\n",
    "    Y = np.ravel(y_stat).tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel. \n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def ravelNonNetwork(x_stat,y_stat,x_measure,y_measure):\n",
    "\n",
    "    X = x_stat[x_measure].to_list()\n",
    "    Y = y_stat[y_measure].to_list()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# wrapper function to call either of the above scripts based on user input\n",
    "def setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,isnetwork,measure):\n",
    "\n",
    "    x_stat = x_data\n",
    "    y_stat = y_data\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        X,Y = averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'append':\n",
    "        X,Y = appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'ravel':\n",
    "        if isnetwork == True:\n",
    "            X,Y = ravelNetwork(x_stat,y_stat)\n",
    "        else:\n",
    "            X,Y = ravelNonNetwork(x_stat,y_stat,x_measure,y_measure)\n",
    "\n",
    "    return x_stat,y_stat,X,Y\n",
    "\n",
    "# function to shuffle data and colors\n",
    "def shuffleDataAlg(X,Y,hues):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    X,Y,hues = shuffle(X,Y,hues)\n",
    "\n",
    "    return X,Y,hues\n",
    "\n",
    "# simple display or figure save function\n",
    "def saveOrShowImg(dir_out,x_measure,y_measure,img_name):\n",
    "    import os,sys \n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    " \n",
    "        # save or show plot\n",
    "        if dir_out:\n",
    "            if not os.path.exists(dir_out):\n",
    "                os.mkdir(dir_out)\n",
    "\n",
    "            if x_measure == y_measure:\n",
    "                img_name_eps = img_name+'_'+x_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'.svg'\n",
    "            else:\n",
    "                img_name_eps = img_name+'_'+x_measure+'_vs_'+y_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'_vs_'+y_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'_vs_'+y_measure+'.svg'\n",
    "\n",
    "            plt.savefig(os.path.join(dir_out, img_name_eps),transparent=True)\n",
    "            plt.savefig(os.path.join(dir_out, img_name_png))     \n",
    "    #         plt.savefig(os.path.join(dir_out, img_name_svg))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results\n",
    "# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def relplotScatter(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,column_wrap,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data: CANNOT BE AVERAGE\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,'ravel',False,hue_measure)\n",
    "\n",
    "    p = sns.relplot(x=X,y=Y,col=x_stat[column_measure],hue=x_stat[hue_measure],kind=\"scatter\",s=100,col_wrap=column_wrap)\n",
    "\n",
    "    # setting counter. looping through axes to add important info and regression lines\n",
    "    i = 0\n",
    "    for ax in p.axes.flat:\n",
    "        x_lim,y_lim = [ax.get_xlim(),ax.get_ylim()]\n",
    "\n",
    "        if trendline == 'equality':\n",
    "            ax.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        elif trendline == 'linreg':\n",
    "            m,b = np.polyfit(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'],1)\n",
    "            ax.plot(ax.get_xticks(),m*ax.get_xticks() + b)\n",
    "            plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(x_measure)\n",
    "        ax.set_ylabel(y_measure)\n",
    "\n",
    "        # compute correlation for each subject and add to plots\n",
    "        corr = np.corrcoef(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'])[1][0]\n",
    "        plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # compute rmse for each subject and add to plots\n",
    "        rmse = np.sqrt(mean_squared_error(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y']))\n",
    "        plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # update counter\n",
    "        i = i+1\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def singleplotScatter(colors_dict,x_data,y_data,x_measure,y_measure,logX,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "    colors = sns.color_palette('colorblind',len(x_stat[hue_measure]))\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    if logX == True:\n",
    "        X = np.log10(X)\n",
    "\n",
    "    if colors_dict:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    else:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100)\t\t\t\t\t\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    if x_measure == y_measure:\n",
    "        p.axes.axis('square')\n",
    "        y_ticks = p.axes.get_yticks()\n",
    "        p.axes.set_xticks(y_ticks)\n",
    "        p.axes.set_yticks(p.axes.get_xticks())\n",
    "        p.axes.set_ylim(p.axes.get_xlim())\n",
    "        p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "        ax = plt.gca()\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    elif trendline == 'groupreg':\n",
    "        for g in range(len(groups)):\n",
    "            if stat_name == 'volume':\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            else:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure] for network correlations. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized.\n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def networkScatter(colors_dict,hues,groups,subjects,x_data,y_data,network_measure,shuffleData,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    # grab data\n",
    "#     [x_stat,y_stat,X,Y] = setupData(x_data,y_data,\"\",\"\",\"ravel\",True,\"\")\n",
    "\n",
    "    # additional network setup\n",
    "    # hues = sns.color_palette(colormap,len(X))\n",
    "    # hues = hues.as_hex()\n",
    "    # keys = [ i for i in range(len(X)) ]\n",
    "    # colors_dict = dict(zip(hues,hues))\n",
    "\n",
    "#     if shuffleData == True:\n",
    "#         X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    # if colors_dict:\n",
    "        # p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    # else:\n",
    "    \n",
    "    p = sns.scatterplot(x=x_data,y=y_data,s=100,palette=colors_dict,legend=False)\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    p.axes.axis('square')\n",
    "    y_ticks = p.axes.get_yticks()\n",
    "    p.axes.set_xticks(y_ticks)\n",
    "    p.axes.set_yticks(p.axes.get_xticks())\n",
    "    p.axes.set_ylim(p.axes.get_xlim())\n",
    "    p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(x_data,y_data,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(x_data,y_data)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(x_data,y_data))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s %s vs %s' %(network_measure,groups[0],groups[1]),fontsize=20)\n",
    "    plt.xlabel(groups[0],fontsize=18)\n",
    "    plt.ylabel(groups[1],fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,network_measure+'_'+groups[0],network_measure+'_'+groups[1],img_name)\n",
    "\n",
    "# uses matplotlib.pyplot's hist2d function to plot data from x_data[x_measure] and y_data[y_measure]. useful for supplementary figure or debugging or publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plot2dHist(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    plt.hist2d(x=X,y=Y,cmin=1,density=False,bins=(len(X)/10),cmap='magma',vmax=(len(X)/10))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # set title and x and y labels\n",
    "\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # # remove top and right spines from plot\n",
    "    # p.axes.spines[\"top\"].set_visible(False)\n",
    "    # p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "### tract profile data\n",
    "# uses matplotlib.pyplot's plot and fill_between functions to plot tract profile data from stat. useful for publication worthy figure\n",
    "# requires stat to be formatted in way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the track in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotProfiles(structures,stat,diffusion_measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os,sys\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # loop through all structures\n",
    "    for t in structures:\n",
    "        print(t)\n",
    "        # loop through all measures\n",
    "        for dm in diffusion_measures:\n",
    "            print(dm)\n",
    "\n",
    "            imgname=img_name+\"_\"+t+\"_\"+dm\n",
    "\n",
    "            # generate figures\n",
    "            fig = plt.figure(figsize=(15,15))\n",
    "#             fig = plt.figure()\n",
    "            fig.patch.set_visible(False)\n",
    "            p = plt.subplot()\n",
    "\n",
    "            # set title and catch array for legend handle\n",
    "            plt.title(\"%s Profiles %s: %s\" %(summary_method,t,dm),fontsize=20)\n",
    "\n",
    "            # loop through groups and plot profile data\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is nodes\n",
    "                x = stat['nodeID'].unique()\n",
    "\n",
    "                # y is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).mean()[dm][t]\n",
    "                elif summary_method == 'median':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).median()[dm][t]\n",
    "                elif summary_method == 'max':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).max()[dm][t]\n",
    "                elif summary_method == 'min':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).min()[dm][t]\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t] / np.sqrt(len(stat[stat['classID'] == stat.classID.unique()[g]]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t]\n",
    "\n",
    "                # plot summary\n",
    "                plt.plot(x,y,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],linewidth=5,label=stat.classID.unique()[g])\n",
    "\n",
    "                # plot shaded error\n",
    "                plt.fill_between(x,y-err,y+err,alpha=0.4,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='1 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "                plt.fill_between(x,y-(2*err),y+(2*err),alpha=0.2,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='2 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "\n",
    "            # set up labels and ticks\n",
    "            plt.xlabel('Location',fontsize=18)\n",
    "            plt.ylabel(dm,fontsize=18)\n",
    "            plt.xticks([x[0],x[-1]],['Begin','End'],fontsize=16)\n",
    "            plt.legend(fontsize=12)\n",
    "            y_lim = plt.ylim()\n",
    "            plt.yticks([np.round(y_lim[0],2),np.mean(y_lim),np.round(y_lim[1],2)],fontsize=16)\n",
    "\n",
    "            # remove top and right spines from plot\n",
    "            p.axes.spines[\"top\"].set_visible(False)\n",
    "            p.axes.spines[\"right\"].set_visible(False)\n",
    "            ax = plt.gca()\n",
    "\n",
    "            # save image or show image\n",
    "            saveOrShowImg(dir_out,dm,dm,imgname)\n",
    "\n",
    "### generic data plots\n",
    "## structure average\n",
    "# uses matplotlib.pyplot's errobar function to plot group average data for each structure with errorbars. useful for publication worthy figure\n",
    "# requires stat to be formatted in similar way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the structure in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotGroupStructureAverage(structures,tissue,stat,measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "\n",
    "    for dm in measures:\n",
    "        print(dm)\n",
    "\n",
    "        # generate figures\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "        fig.patch.set_visible(False)\n",
    "        p = plt.subplot()\n",
    "\n",
    "        # set y range\n",
    "        p.set_ylim([0,(len(structures)*len(stat.classID.unique()))+len(stat.classID.unique())])\n",
    "\n",
    "        # set spines and ticks, and labels\n",
    "        p.yaxis.set_ticks_position('left')\n",
    "        p.xaxis.set_ticks_position('bottom')\n",
    "        p.set_xlabel(dm,fontsize=18)\n",
    "        p.set_ylabel(\"Structures\",fontsize=18)\n",
    "        if len(stat.classID.unique()) < 3:\n",
    "            if len(stat.classID.unique()) == 2:\n",
    "                p.set_yticks(np.arange(1.5,(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "            else:\n",
    "                p.set_yticks(np.arange(1,len(structures)+1,step=1))\n",
    "        else:\n",
    "            p.set_yticks(np.arange((len(stat.classID.unique())-1),(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "        p.set_yticklabels(structures,fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "\n",
    "        # set title\n",
    "        plt.title(\"%s Group-Summary: %s\" %(summary_method,dm),fontsize=20)\n",
    "\n",
    "        # loop through structures\n",
    "        for t in range(len(structures)):\n",
    "            # loop through groups\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).mean()[dm][structures[t]]\n",
    "                elif summary_method == 'median':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).median()[dm][structures[t]]\n",
    "                elif summary_method == 'max':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).max()[dm][structures[t]]\n",
    "                elif summary_method == 'min':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).min()[dm][structures[t]]\n",
    "\n",
    "                # y is location on y axis\n",
    "                y = (len(stat.classID.unique())*(t+1)-len(stat.classID.unique()))+(g+1)\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]] / np.sqrt(len(stat[stat.classID.str.contains(stat.classID.unique()[g])]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]]\n",
    "\n",
    "                # plot data\n",
    "                if t == 0:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10,label=stat.classID.unique()[g])\n",
    "                else:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10)\n",
    "\n",
    "        # add legend\n",
    "        plt.legend(fontsize=16)\n",
    "\n",
    "        # remove top and right spines from plot\n",
    "        p.axes.spines[\"top\"].set_visible(False)\n",
    "        p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "        # save image or show image\n",
    "        saveOrShowImg(dir_out,dm,dm,img_name)\n",
    "\n",
    "def violinPlots(x_measure,y_measure,hue_measure,data,summary,scale,inner,cmap,dir_out,img_name):\n",
    "    \n",
    "    if hue_measure:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,hue=hue_measure,scale=scale,inner=inner,palette=cmap)\n",
    "    else:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,scale=scale,inner=inner,palette=cmap,orientation='horizontal')\n",
    "\n",
    "    if summary == 'mean':\n",
    "        summary = data.groupby([x_measure])[y_measure].mean()\n",
    "    elif summary == 'median':\n",
    "        summary = data.groupby([x_measure])[y_measure].median()\n",
    "    elif summary == 'mode':\n",
    "        summary = data.groupby([x_measure])[y_measure].mode()\n",
    "    elif summary == 'max':\n",
    "        summary = data.groupby([x_measure])[y_measure].max()\n",
    "    elif summary == 'min':\n",
    "        summary = data.groupby([x_measure])[y_measure].min()\n",
    "\n",
    "#     for xtick in violin.get_xticks():\n",
    "#         violin.text(xtick,summary[xtick],np.round(summary[xtick],3),horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "  \n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up variables\n",
      "setting up variables complete\n",
      "grabbing demographic data\n"
     ]
    }
   ],
   "source": [
    "### setting up variables and adding paths\n",
    "print(\"setting up variables\")\n",
    "topPath = \"./\"\n",
    "os.chdir(topPath)\n",
    "data_dir = topPath+'/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "img_dir = topPath+'/img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "groups = ['run_1','run_2']\n",
    "colors_array = ['blue','gray']\n",
    "diff_micro_measures = ['ad','fa','md','rd','ndi','isovf','odi']\n",
    "diff_macro_measures = ['length','volume','count']\n",
    "print(\"setting up variables complete\")\n",
    "\n",
    "### grabbing subjects demographic data\n",
    "print(\"grabbing demographic data\")\n",
    "# if os.path.isfile('./data/subjects_data.csv'):\n",
    "#     subjects_data = pd.read_csv('./data/subjects_data.csv')\n",
    "# else:\n",
    "subjects_data = pd.read_csv('./subjects_demo.csv')\n",
    "subjects_data['subjectID'] = [ str(int(np.float(f.strip('HCP')))) for f in subjects_data['subjectID']]\n",
    "subjects_data.to_csv('./data/subjects_data.csv',index=False)\n",
    "# print(\"grabbing demographic data complete\")\n",
    "\n",
    "colors = {}\n",
    "subjects = {}\n",
    "\n",
    "# # loop through groups and identify subjects and set color schema for each group\n",
    "for g in range(len(groups)):\n",
    "#     # set subjects array\n",
    "    subjects[groups[g]] =  subjects_data['subjectID']\n",
    "    subjects[groups[g]].sort_values()\n",
    "#     # update subjects with HCP prefix to make plotting easier\n",
    "\n",
    "#     # set colors array\n",
    "    colors_name = colors_array[g]\n",
    "    colors[groups[g]] = colors_array[g]\n",
    "\n",
    "### merge demo and subjects data\n",
    "# subjects_data = pd.merge(subjects_data,subjects_demo,on='subjectID')\n",
    "# subjects_data['classID'] = [ 'hcp' for f in range(len(subjects_data))]\n",
    "# subjects_data.to_csv(data_dir+'/subjects_data_demo.csv')\n",
    "\n",
    "# create subjects color dictionary\n",
    "colors_dict = createColorDictionary(subjects_data,'subjectID','colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_data['subjectID'] =  [ str(int(np.float(f))) for f in subjects_data['subjectID']]\n",
    "subjects_data.drop(columns={\"Unnamed: 0\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>classID</th>\n",
       "      <th>colors</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_range</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103818</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103818</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105923</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105923</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111312</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>861456</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>877168</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>877168</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>917255</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>917255</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subjectID classID  colors gender age_range  age\n",
       "0     103818    test  orange      F     31-35   31\n",
       "1     103818  retest    blue      F     31-35   31\n",
       "2     105923    test  orange      F     31-35   31\n",
       "3     105923  retest    blue      F     31-35   31\n",
       "4     111312    test  orange      F     31-35   31\n",
       "..       ...     ...     ...    ...       ...  ...\n",
       "83    861456  retest    blue      F     31-35   31\n",
       "84    877168    test  orange      F     31-35   31\n",
       "85    877168  retest    blue      F     31-35   31\n",
       "86    917255    test  orange      M     31-35   31\n",
       "87    917255  retest    blue      M     31-35   31\n",
       "\n",
       "[88 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_data = subjects_data.loc[subjects_data['classID'] == 'test']\n",
    "subjects_data.drop(columns={'classID'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>classID</th>\n",
       "      <th>colors</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_range</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103818</td>\n",
       "      <td>run_1</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103818</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105923</td>\n",
       "      <td>run_1</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105923</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111312</td>\n",
       "      <td>run_1</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>861456</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>877168</td>\n",
       "      <td>run_1</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>877168</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>917255</td>\n",
       "      <td>run_1</td>\n",
       "      <td>orange</td>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>917255</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subjectID classID  colors gender age_range  age\n",
       "0     103818   run_1  orange      F     31-35   31\n",
       "1     103818   run_2    blue      F     31-35   31\n",
       "2     105923   run_1  orange      F     31-35   31\n",
       "3     105923   run_2    blue      F     31-35   31\n",
       "4     111312   run_1  orange      F     31-35   31\n",
       "..       ...     ...     ...    ...       ...  ...\n",
       "83    861456   run_2    blue      F     31-35   31\n",
       "84    877168   run_1  orange      F     31-35   31\n",
       "85    877168   run_2    blue      F     31-35   31\n",
       "86    917255   run_1  orange      M     31-35   31\n",
       "87    917255   run_2    blue      M     31-35   31\n",
       "\n",
       "[88 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subjects_data.drop(columns={\"Unnamed: 0\"},inplace=True)\n",
    "# subjects_data['classID'] = [ groups[0] if f == 'test' else groups[1] for f in subjects_data['classID'] ]\n",
    "subjects_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### app-validitiy\n",
    "run_1 = collectData('neuro/tractmeasures','profiles','!validity',\"output_FiberStats.csv\",subjects_data,colors,data_dir+\"/tract_profiles_app_validity_run1.csv\")\n",
    "# run_2 = collectData('neuro/tractmeasures','',['test','aparc.a2009s','bl_generated'],\"tractmeasures.csv\",subjects_data,colors,data_dir+\"/tract_profiles_app_validity_run2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['861456', '185442', '114823', '287248', '111312', '125525',\n",
       "       '137128', '137128'], dtype='<U32')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating brainlife: anatomical\n"
     ]
    }
   ],
   "source": [
    "### freesurfer validation analysis: hcp freesurfer vs bl freesurfer\n",
    "print(\"validating brainlife: anatomical\")\n",
    "# grab data: desikan killany\n",
    "bl_generated_aparc_test = collectData('neuro/parc-stats','freesurfer',['test','aparc.a2009s','bl_generated'],\"cortex.csv\",subjects_data,colors,data_dir+\"/bl_generated_freesurfer_aparc_test.csv\")\n",
    "hcp_provided_aparc_test = collectData('neuro/parc-stats','freesurfer',['test','aparc.a2009s','hcp_provided'],\"cortex.csv\",subjects_data,colors,data_dir+\"/hcp_provided_freesurfer_aparc_test.csv\")\n",
    "bl_generated_aparc_test['classID'] = [ 'bl_generated' for f in bl_generated_aparc_test['subjectID']]\n",
    "hcp_provided_aparc_test['classID'] = [ 'hcp_provided' for f in hcp_provided_aparc_test['subjectID']]\n",
    "aparc_validity = pd.concat([bl_generated_aparc_test,hcp_provided_aparc_test])\n",
    "aparc_validity = aparc_validity.drop_duplicates()\n",
    "aparc_validity.to_csv('desikian_killany_aparc_validity_hcp_test_data.csv',index=False)\n",
    "\n",
    "# plot data\n",
    "singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'average_thickness_mm','average_thickness_mm',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_thickness')\n",
    "singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'surface_area_mm^2','surface_area_mm^2',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_surface_area')\n",
    "singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'gray_matter_volume_mm^3','gray_matter_volume_mm^3',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_volume')\n",
    "\n",
    "# grab data: hcp-mmp\n",
    "bl_generated_glasser_test = collectData('neuro/parc-stats','surface',['test','glasser','bl_generated'],\"cortex.csv\",subjects_data,colors,data_dir+\"/bl_generated_freesurfer_glasser_test.csv\")\n",
    "hcp_provided_glasser_test = collectData('neuro/parc-stats','surface',['test','glasser','hcp_provided'],\"cortex.csv\",subjects_data,colors,data_dir+\"/hcp_provided_freesurfer_glasser_test.csv\")\n",
    "bl_generated_glasser_test['classID'] = [ 'bl_generated' for f in bl_generated_glasser_test['subjectID']]\n",
    "hcp_provided_glasser_test['classID'] = [ 'hcp_provided' for f in hcp_provided_glasser_test['subjectID']]\n",
    "\n",
    "# concat and clean\n",
    "glasser_validity = pd.concat([bl_generated_glasser_test,hcp_provided_glasser_test])\n",
    "glasser_validity = glasser_validity[glasser_validity['structureID'] != 'lh_???']\n",
    "glasser_validity = glasser_validity[glasser_validity['structureID'] != 'rh_???']\n",
    "glasser_validity = glasser_validity[~glasser_validity.structureID.str.contains('unknown')]\n",
    "glasser_validity = glasser_validity[~glasser_validity.structureID.str.contains('_H_ROI')]\n",
    "glasser_validity = glasser_validity.drop_duplicates()\n",
    "glasser_validity.to_csv('hcp_mmp_glasser_validity_hcp_test_data.csv',index=False)\n",
    "\n",
    "# plot data\n",
    "singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'average_thickness_mm','average_thickness_mm',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_thickness')\n",
    "singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'surface_area_mm^2','surface_area_mm^2',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_surface_area')\n",
    "singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'gray_matter_volume_mm^3','gray_matter_volume_mm^3',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_volume')\n",
    "\n",
    "print(\"validating brainlife complete: anatomical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "### diffusion validation analysis: hcp preprocessed dwi vs bl preprocessed dwi\n",
    "print(\"validating brainlife: diffusion\")\n",
    "# grab data: desikan killany\n",
    "# bl_generated_aparc_test = collectData('neuro/parc-stats','freesurfer',['test','aparc.a2009s','bl_generated'],\"cortex.csv\",subjects_data,colors,data_dir+\"/bl_generated_freesurfer_aparc_test.csv\")\n",
    "# hcp_provided_aparc_test = collectData('neuro/parc-stats','freesurfer',['test','aparc.a2009s','hcp_provided'],\"cortex.csv\",subjects_data,colors,data_dir+\"/hcp_provided_freesurfer_aparc_test.csv\")\n",
    "# bl_generated_aparc_test['classID'] = [ 'bl_generated' for f in bl_generated_aparc_test['subjectID']]\n",
    "# hcp_provided_aparc_test['classID'] = [ 'hcp_provided' for f in hcp_provided_aparc_test['subjectID']]\n",
    "# aparc_validity = pd.concat([bl_generated_aparc_test,hcp_provided_aparc_test])\n",
    "# aparc_validity = aparc_validity.drop_duplicates()\n",
    "# aparc_validity.to_csv('desikian_killany_aparc_validity_hcp_test_data.csv',index=False)\n",
    "\n",
    "# # plot data\n",
    "# singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'average_thickness_mm','average_thickness_mm',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_thickness')\n",
    "# singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'surface_area_mm^2','surface_area_mm^2',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_surface_area')\n",
    "# singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'gray_matter_volume_mm^3','gray_matter_volume_mm^3',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_volume')\n",
    "\n",
    "# # grab data: hcp-mmp\n",
    "# bl_generated_glasser_test = collectData('neuro/parc-stats','surface',['test','glasser','bl_generated'],\"cortex.csv\",subjects_data,colors,data_dir+\"/bl_generated_freesurfer_glasser_test.csv\")\n",
    "# hcp_provided_glasser_test = collectData('neuro/parc-stats','surface',['test','glasser','hcp_provided'],\"cortex.csv\",subjects_data,colors,data_dir+\"/hcp_provided_freesurfer_glasser_test.csv\")\n",
    "# bl_generated_glasser_test['classID'] = [ 'bl_generated' for f in bl_generated_glasser_test['subjectID']]\n",
    "# hcp_provided_glasser_test['classID'] = [ 'hcp_provided' for f in hcp_provided_glasser_test['subjectID']]\n",
    "\n",
    "# # concat and clean\n",
    "# glasser_validity = pd.concat([bl_generated_glasser_test,hcp_provided_glasser_test])\n",
    "# glasser_validity = glasser_validity[glasser_validity['structureID'] != 'lh_???']\n",
    "# glasser_validity = glasser_validity[glasser_validity['structureID'] != 'rh_???']\n",
    "# glasser_validity = glasser_validity[~glasser_validity.structureID.str.contains('unknown')]\n",
    "# glasser_validity = glasser_validity[~glasser_validity.structureID.str.contains('_H_ROI')]\n",
    "# glasser_validity = glasser_validity.drop_duplicates()\n",
    "# glasser_validity.to_csv('hcp_mmp_glasser_validity_hcp_test_data.csv',index=False)\n",
    "\n",
    "# # plot data\n",
    "# singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'average_thickness_mm','average_thickness_mm',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_thickness')\n",
    "# singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'surface_area_mm^2','surface_area_mm^2',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_surface_area')\n",
    "# singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'gray_matter_volume_mm^3','gray_matter_volume_mm^3',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_volume')\n",
    "\n",
    "print(\"validating brainlife complete: diffusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### diffusion validation analysis: hcp preprocessed fmri vs bl preprocessed fmri\n",
    "print(\"validating brainlife: fmri\")\n",
    "# grab data: desikan killany\n",
    "# bl_generated_aparc_test = collectData('neuro/parc-stats','freesurfer',['test','aparc.a2009s','bl_generated'],\"cortex.csv\",subjects_data,colors,data_dir+\"/bl_generated_freesurfer_aparc_test.csv\")\n",
    "# hcp_provided_aparc_test = collectData('neuro/parc-stats','freesurfer',['test','aparc.a2009s','hcp_provided'],\"cortex.csv\",subjects_data,colors,data_dir+\"/hcp_provided_freesurfer_aparc_test.csv\")\n",
    "# bl_generated_aparc_test['classID'] = [ 'bl_generated' for f in bl_generated_aparc_test['subjectID']]\n",
    "# hcp_provided_aparc_test['classID'] = [ 'hcp_provided' for f in hcp_provided_aparc_test['subjectID']]\n",
    "# aparc_validity = pd.concat([bl_generated_aparc_test,hcp_provided_aparc_test])\n",
    "# aparc_validity = aparc_validity.drop_duplicates()\n",
    "# aparc_validity.to_csv('desikian_killany_aparc_validity_hcp_test_data.csv',index=False)\n",
    "\n",
    "# # plot data\n",
    "# singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'average_thickness_mm','average_thickness_mm',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_thickness')\n",
    "# singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'surface_area_mm^2','surface_area_mm^2',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_surface_area')\n",
    "# singleplotScatter(\"\",aparc_validity.loc[aparc_validity['classID'] == 'hcp_provided'],aparc_validity.loc[aparc_validity['classID'] == 'bl_generated'],'gray_matter_volume_mm^3','gray_matter_volume_mm^3',False,'structureID','structureID','ravel','linreg',True,img_dir,'aparc_validity_hcp_test_volume')\n",
    "\n",
    "# # grab data: hcp-mmp\n",
    "# bl_generated_glasser_test = collectData('neuro/parc-stats','surface',['test','glasser','bl_generated'],\"cortex.csv\",subjects_data,colors,data_dir+\"/bl_generated_freesurfer_glasser_test.csv\")\n",
    "# hcp_provided_glasser_test = collectData('neuro/parc-stats','surface',['test','glasser','hcp_provided'],\"cortex.csv\",subjects_data,colors,data_dir+\"/hcp_provided_freesurfer_glasser_test.csv\")\n",
    "# bl_generated_glasser_test['classID'] = [ 'bl_generated' for f in bl_generated_glasser_test['subjectID']]\n",
    "# hcp_provided_glasser_test['classID'] = [ 'hcp_provided' for f in hcp_provided_glasser_test['subjectID']]\n",
    "\n",
    "# # concat and clean\n",
    "# glasser_validity = pd.concat([bl_generated_glasser_test,hcp_provided_glasser_test])\n",
    "# glasser_validity = glasser_validity[glasser_validity['structureID'] != 'lh_???']\n",
    "# glasser_validity = glasser_validity[glasser_validity['structureID'] != 'rh_???']\n",
    "# glasser_validity = glasser_validity[~glasser_validity.structureID.str.contains('unknown')]\n",
    "# glasser_validity = glasser_validity[~glasser_validity.structureID.str.contains('_H_ROI')]\n",
    "# glasser_validity = glasser_validity.drop_duplicates()\n",
    "# glasser_validity.to_csv('hcp_mmp_glasser_validity_hcp_test_data.csv',index=False)\n",
    "\n",
    "# # plot data\n",
    "# singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'average_thickness_mm','average_thickness_mm',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_thickness')\n",
    "# singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'surface_area_mm^2','surface_area_mm^2',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_surface_area')\n",
    "# singleplotScatter(\"\",glasser_validity.loc[glasser_validity['classID'] == 'hcp_provided'],glasser_validity.loc[glasser_validity['classID'] == 'bl_generated'],'gray_matter_volume_mm^3','gray_matter_volume_mm^3',False,'structureID','structureID','ravel','linreg',True,img_dir,'glasser_validity_hcp_test_volume')\n",
    "\n",
    "print(\"validating brainlife complete: fmri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>classID</th>\n",
       "      <th>colors</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_range</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103818</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103818</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105923</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105923</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111312</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>861456</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>877168</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>877168</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>917255</td>\n",
       "      <td>test</td>\n",
       "      <td>orange</td>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>917255</td>\n",
       "      <td>retest</td>\n",
       "      <td>blue</td>\n",
       "      <td>M</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subjectID classID  colors gender age_range  age\n",
       "0      103818    test  orange      F     31-35   31\n",
       "1      103818  retest    blue      F     31-35   31\n",
       "2      105923    test  orange      F     31-35   31\n",
       "3      105923  retest    blue      F     31-35   31\n",
       "4      111312    test  orange      F     31-35   31\n",
       "..        ...     ...     ...    ...       ...  ...\n",
       "83     861456  retest    blue      F     31-35   31\n",
       "84     877168    test  orange      F     31-35   31\n",
       "85     877168  retest    blue      F     31-35   31\n",
       "86     917255    test  orange      M     31-35   31\n",
       "87     917255  retest    blue      M     31-35   31\n",
       "\n",
       "[88 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### cortical data reliability: test-retest with hcp-freesurfer\n",
    "print(\"grabbing aparc a2009s data\")\n",
    "\n",
    "### freesurfer annotation statistics - bl generated\n",
    "print(\"grabbing bl generated aparc a2009s data\")\n",
    "# grab data\n",
    "aparc_stats_bl = collectData(topPath,\"parc-freesurfer-bl\",\"cortex.csv\",data_dir,groups,subjects,'generated-a2009s')\n",
    "\n",
    "# plot data\n",
    "for measures in structural_measures:\n",
    "    print(measures)\n",
    "    singleplotScatter(colors_dict,aparc_stats_bl[aparc_stats_bl['classID'] == groups[0]],aparc_stats_bl[aparc_stats_bl['classID'] == groups[1]],measures,measures,'structureID','subjectID','ravel','linreg',True,img_dir,\"aparc_stats_bl_scatter\")\n",
    "\n",
    "### glasser annotation statistics\n",
    "print(\"grabbing glasser data\")\n",
    "# grab data\n",
    "glasser_stats_bl = collectData(topPath,\"parc-glasser-bl\",\"cortex.csv\",data_dir,groups,subjects,'bl-generated')\n",
    "\n",
    "# clean up data\n",
    "glasser_stats_bl = glasser_stats_bl[glasser_stats_bl['structureID'] != 'lh_???']\n",
    "glasser_stats_bl = glasser_stats_bl[glasser_stats_bl['structureID'] != 'rh_???'] \n",
    "glasser_stats_bl = glasser_stats_bl[~glasser_stats_bl.structureID.str.contains('unknown')]\n",
    "structureList = []\n",
    "for i in glasser_stats_bl['structureID'].unique():\n",
    "    if len(glasser_stats_bl[glasser_stats_bl['structureID'] == i].subjectID) < len(groups) * len(glasser_stats_bl['subjectID'].unique()):\n",
    "        print('%s is missing in all subjects' %i)\n",
    "    else:\n",
    "        structureList = np.append(structureList,i)\n",
    "glasser_stats_bl = glasser_stats_bl.loc[glasser_stats_bl['structureID'].isin(structureList)]\n",
    "glasser_stats_bl.to_csv(data_dir+'/parc-glasser-bl-cleaned.csv')\n",
    "\n",
    "# plot data\n",
    "for measures in structural_measures:\n",
    "    print(measures)\n",
    "    singleplotScatter(colors_dict,glasser_stats_bl[glasser_stats_bl['classID'] == groups[0]],glasser_stats_bl[glasser_stats_bl['classID'] == groups[1]],measures,measures,'structureID','subjectID','ravel','linreg',True,img_dir,\"glasser_stats_bl_scatter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmri validity\n",
    "bl_generated_fmri = collectData('generic/network','measurements',['test','bl_generated'],'network.json.gz',subjects_data,colors,data_dir+\"/bl_generated_fmri_yeo_validity_test.csv\")\n",
    "bl_generated_fmri.drop(columns={'colors'},inplace=True)\n",
    "bl_generated_fmri.drop_duplicates('subjectID',inplace=True)\n",
    "\n",
    "hcp_provided_fmri = collectData('generic/network','measurements',['test','bl'],'network.json.gz',subjects_data,colors,data_dir+\"/hcp_provided_fmri_yeo_validity_test.csv\")\n",
    "hcp_provided_fmri['classID'] = ['run_2' for f in hcp_provided_fmri['classID']]\n",
    "hcp_provided_fmri.drop(columns={'colors'},inplace=True)\n",
    "hcp_provided_fmri.drop_duplicates('subjectID',inplace=True)\n",
    "\n",
    "# bl_generated_fmri_avg_degree = [ bl_generated_fmri.loc[bl_generated_fmri['subjectID'] == f]['metadata'].reset_index(drop=True)[0]['Avg. Degree'] for f in bl_generated_fmri['subjectID'].unique()] \n",
    "# hcp_provided_fmri_avg_degree = [ hcp_provided_fmri.loc[hcp_provided_fmri['subjectID'] == f]['metadata'].reset_index(drop=True)[0]['Avg. Degree'] for f in hcp_provided_fmri['subjectID'].unique()] \n",
    "\n",
    "# fmri_average_degree = pd.DataFrame()\n",
    "# fmri_average_degree['subjectID'] = bl_generated_fmri.subjectID.unique().tolist() + hcp_provided_fmri.subjectID.unique().tolist()\n",
    "# fmri_average_degree['classID'] = bl_generated_fmri.classID.tolist() + hcp_provided_fmri.classID.tolist()\n",
    "\n",
    "# fmri_average_degree['degree'] = bl_generated_fmri_avg_degree + hcp_provided_fmri_avg_degree\n",
    "# singleplotScatter('',fmri_average_degree.loc[fmri_average_degree['classID'] == 'run_1'],fmri_average_degree.loc[fmri_average_degree['classID'] == 'run_2'],'degree','degree',False,'classID','classID','ravel','linreg',True,img_dir,'bl_fmri_average_degree_validity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>directed</th>\n",
       "      <th>type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>subjectID</th>\n",
       "      <th>classID</th>\n",
       "      <th>colors</th>\n",
       "      <th>gender</th>\n",
       "      <th>age_range</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 16.307692307692307, 'Avg. Stre...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '10', 'metadata': {...</td>\n",
       "      <td>103818</td>\n",
       "      <td>run_2</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 16.307692307692307, 'Avg. Stre...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '10', 'metadata': {...</td>\n",
       "      <td>103818</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 18.16923076923077, 'Avg. Stren...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '29', 'metadata': {...</td>\n",
       "      <td>105923</td>\n",
       "      <td>run_2</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 18.16923076923077, 'Avg. Stren...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '29', 'metadata': {...</td>\n",
       "      <td>105923</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 15.6, 'Avg. Strength': 6.04927...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '47', 'metadata': {...</td>\n",
       "      <td>111312</td>\n",
       "      <td>run_2</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 7.430769230769231, 'Avg. Stren...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '77', 'metadata': {...</td>\n",
       "      <td>204521</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 18.076923076923077, 'Avg. Stre...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '28', 'metadata': {...</td>\n",
       "      <td>250427</td>\n",
       "      <td>run_2</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 18.076923076923077, 'Avg. Stre...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '28', 'metadata': {...</td>\n",
       "      <td>250427</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>31-35</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 14.415384615384616, 'Avg. Stre...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '11', 'metadata': {...</td>\n",
       "      <td>287248</td>\n",
       "      <td>run_2</td>\n",
       "      <td>orange</td>\n",
       "      <td>F</td>\n",
       "      <td>26-30</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>network</td>\n",
       "      <td>False</td>\n",
       "      <td>correlation</td>\n",
       "      <td>{'Avg. Degree': 14.415384615384616, 'Avg. Stre...</td>\n",
       "      <td>{'0': {'label': 'ROI_1', 'metadata': {'column_...</td>\n",
       "      <td>[{'source': '0', 'target': '11', 'metadata': {...</td>\n",
       "      <td>287248</td>\n",
       "      <td>run_2</td>\n",
       "      <td>blue</td>\n",
       "      <td>F</td>\n",
       "      <td>26-30</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  directed         type  \\\n",
       "0   network     False  correlation   \n",
       "1   network     False  correlation   \n",
       "2   network     False  correlation   \n",
       "3   network     False  correlation   \n",
       "4   network     False  correlation   \n",
       "..      ...       ...          ...   \n",
       "59  network     False  correlation   \n",
       "60  network     False  correlation   \n",
       "61  network     False  correlation   \n",
       "62  network     False  correlation   \n",
       "63  network     False  correlation   \n",
       "\n",
       "                                             metadata  \\\n",
       "0   {'Avg. Degree': 16.307692307692307, 'Avg. Stre...   \n",
       "1   {'Avg. Degree': 16.307692307692307, 'Avg. Stre...   \n",
       "2   {'Avg. Degree': 18.16923076923077, 'Avg. Stren...   \n",
       "3   {'Avg. Degree': 18.16923076923077, 'Avg. Stren...   \n",
       "4   {'Avg. Degree': 15.6, 'Avg. Strength': 6.04927...   \n",
       "..                                                ...   \n",
       "59  {'Avg. Degree': 7.430769230769231, 'Avg. Stren...   \n",
       "60  {'Avg. Degree': 18.076923076923077, 'Avg. Stre...   \n",
       "61  {'Avg. Degree': 18.076923076923077, 'Avg. Stre...   \n",
       "62  {'Avg. Degree': 14.415384615384616, 'Avg. Stre...   \n",
       "63  {'Avg. Degree': 14.415384615384616, 'Avg. Stre...   \n",
       "\n",
       "                                                nodes  \\\n",
       "0   {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "1   {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "2   {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "3   {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "4   {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "..                                                ...   \n",
       "59  {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "60  {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "61  {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "62  {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "63  {'0': {'label': 'ROI_1', 'metadata': {'column_...   \n",
       "\n",
       "                                                edges subjectID classID  \\\n",
       "0   [{'source': '0', 'target': '10', 'metadata': {...    103818   run_2   \n",
       "1   [{'source': '0', 'target': '10', 'metadata': {...    103818   run_2   \n",
       "2   [{'source': '0', 'target': '29', 'metadata': {...    105923   run_2   \n",
       "3   [{'source': '0', 'target': '29', 'metadata': {...    105923   run_2   \n",
       "4   [{'source': '0', 'target': '47', 'metadata': {...    111312   run_2   \n",
       "..                                                ...       ...     ...   \n",
       "59  [{'source': '0', 'target': '77', 'metadata': {...    204521   run_2   \n",
       "60  [{'source': '0', 'target': '28', 'metadata': {...    250427   run_2   \n",
       "61  [{'source': '0', 'target': '28', 'metadata': {...    250427   run_2   \n",
       "62  [{'source': '0', 'target': '11', 'metadata': {...    287248   run_2   \n",
       "63  [{'source': '0', 'target': '11', 'metadata': {...    287248   run_2   \n",
       "\n",
       "    colors gender age_range  age  \n",
       "0   orange      F     31-35   31  \n",
       "1     blue      F     31-35   31  \n",
       "2   orange      F     31-35   31  \n",
       "3     blue      F     31-35   31  \n",
       "4   orange      F     31-35   31  \n",
       "..     ...    ...       ...  ...  \n",
       "59    blue      F     31-35   31  \n",
       "60  orange      F     31-35   31  \n",
       "61    blue      F     31-35   31  \n",
       "62  orange      F     26-30   26  \n",
       "63    blue      F     26-30   26  \n",
       "\n",
       "[64 rows x 12 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_degree = []\n",
    "for i in hcp_provided_fmri['subjectID']:\n",
    "    data = hcp_provided_fmri.loc[hcp_provided_fmri['subjectID'] == i]\n",
    "    \n",
    "hcp_provided_fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hcp_provided_fmri.drop(columns={'colors'},inplace=True)\n",
    "hcp_provided_fmri_node_degree = []\n",
    "hcp_provided_fmri_subjects = []\n",
    "for i in hcp_provided_fmri.subjectID.unique():\n",
    "    for j in hcp_provided_fmri.loc[hcp_provided_fmri['subjectID'] == i]['nodes'].reset_index(drop=True)[0]:\n",
    "        hcp_provided_fmri_subjects = np.append(hcp_provided_fmri_subjects,i)\n",
    "        hcp_provided_fmri_node_degree = np.append(hcp_provided_fmri_node_degree,hcp_provided_fmri.loc[hcp_provided_fmri['subjectID'] == i]['nodes'].reset_index(drop=True)[0][j]['metadata']['Degree'])\n",
    "        \n",
    "bl_generated_fmri_node_degree = []\n",
    "bl_generated_fmri_subjects = []\n",
    "for i in bl_generated_fmri.subjectID.unique():\n",
    "    for j in bl_generated_fmri.loc[bl_generated_fmri['subjectID'] == i]['nodes'].reset_index(drop=True)[0]:\n",
    "        bl_generated_fmri_subjects = np.append(bl_generated_fmri_subjects,i)\n",
    "        bl_generated_fmri_node_degree = np.append(bl_generated_fmri_node_degree,bl_generated_fmri.loc[bl_generated_fmri['subjectID'] == i]['nodes'].reset_index(drop=True)[0][j]['metadata']['Degree'])\n",
    "        \n",
    "node_degree_validity = pd.DataFrame()\n",
    "node_degree_validity['degree'] = np.append(hcp_provided_fmri_node_degree,bl_generated_fmri_node_degree)\n",
    "node_degree_validity['classID'] = np.append([ 'hcp_provided' for f in hcp_provided_fmri_node_degree],[ 'bl_generated' for f in bl_generated_fmri_node_degree])\n",
    "node_degree_validity['subjectID'] = np.append(hcp_provided_fmri_subjects,bl_generated_fmri_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleplotScatter('',node_degree_validity.loc[node_degree_validity['classID'] == 'hcp_provided'],node_degree_validity.loc[node_degree_validity['classID'] == 'bl_generated'],'degree','degree',False,'subjectID','subjectID','ravel','linreg',True,img_dir,'fmri_yeo_validity_node_degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_data['classID'] = [ 'run_1' if f == 'test' else 'run_2' for f in subjects_data['classID'] ]\n",
    "# tract_profiles_run_1 = collectData('neuro/tractmeasures','profiles',['validity','!run_2'],'output_FiberStats.csv',subjects_data,colors,data_dir+\"/tract_profiles_within_app_validity_run_1.csv\")\n",
    "tract_profiles_run_1['classID'] = [ 'run_1' for f in tract_profiles_run_1['subjectID'] ]\n",
    "# tract_profiles_run_2 = collectData('neuro/tractmeasures','profiles',['validity','run_2'],'output_FiberStats.csv',subjects_data,colors,data_dir+\"/tract_profiles_within_app_validity_run_2.csv\")\n",
    "tract_profiles_run_2['classID'] = [ 'run_2' for f in tract_profiles_run_2['subjectID'] ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_profiles = pd.concat([tract_profiles_run_1,tract_profiles_run_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tract_profiles_average = computeMeanData(data_dir,tract_profiles,'tract_profiles_within_app_validity_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleplotScatter('',tract_profiles_average.loc[tract_profiles_average['classID'] == 'run_1'],tract_profiles_average.loc[tract_profiles_average['classID'] == 'run_2'],'fa','fa',False,'structureID','subjectID','ravel','linreg',True,img_dir,'tract_profiles_within_app_validity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=tract_profiles_average,y=tract_profiles_run_2['fa'].values,hue=tract_profiles_run_1['structureID'].values,legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
