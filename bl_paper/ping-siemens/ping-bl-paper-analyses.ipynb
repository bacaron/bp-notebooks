{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subjects\n",
    "def collectSubjectData(topPath,dataPath,groups,subjects,colors):\n",
    "\n",
    "    # set up variables\n",
    "    data_columns = ['subjectID','classID','colors']\n",
    "    data =  pd.DataFrame([],columns=data_columns)\n",
    "\n",
    "    # populate structure\n",
    "    data['subjectID'] = [ f for g in groups for f in subjects[g] ]\n",
    "    data['classID'] = [ g for g in groups for f in range(len(subjects[g]))]\n",
    "    data['colors'] = [ colors[c] for c in colors for f in subjects[c]]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'subjects.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### color dictionary\n",
    "def createColorDictionary(data,measure,colorPalette):\n",
    "\n",
    "    keys = data[measure].unique()\n",
    "    values = sns.color_palette(colorPalette,len(keys))\n",
    "    values = values.as_hex()\n",
    "\n",
    "    colors_dict = dict(zip(keys,values))\n",
    "\n",
    "\n",
    "    return colors_dict\n",
    "\n",
    "### load parcellation stats data \n",
    "### load data \n",
    "def collectData(datatype,datatype_tags,tags,filename,subjects_data,colors,outPath):\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "    \n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    paths = []\n",
    "    finish_dates = []\n",
    "    \n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            if datatype_tags in obj['output']['datatype_tags']:\n",
    "                if tags in obj['output']['tags']:\n",
    "                    finish_dates = np.append(finish_dates,obj[\"finish_date\"])\n",
    "                    if obj['output']['meta']['subject'] in subjects:\n",
    "                        index = np.where(subjects == obj['output']['meta']['subject'])\n",
    "                        if finish_dates[index] < obj[\"finish_date\"]:\n",
    "                            subjects = np.delete(subjects,index)\n",
    "                            paths = np.delete(paths,index)\n",
    "                            subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                            paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "                    else:\n",
    "                        subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                        paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "    \n",
    "    # sort paths by subject order\n",
    "    paths = [x for _,x in sorted(zip(subjects,paths))]\n",
    "\n",
    "    for i in paths:\n",
    "        tmpdata = pd.read_csv(i)                    \n",
    "        if 'classID' in tmpdata.keys():\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on=['subjectID','classID'])\n",
    "        else:\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on='subjectID')\n",
    "        data = data.append(tmpdata,ignore_index=True)\n",
    "            \n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    # subjects.csv\n",
    "    data.to_csv(outPath,index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### cut nodes\n",
    "def cutNodes(data,num_nodes,dataPath,foldername,savename):\n",
    "\n",
    "    # identify inner n nodes based on num_nodes input\n",
    "    total_nodes = len(data['nodeID'].unique())\n",
    "    cut_nodes = int((total_nodes - num_nodes) / 2)\n",
    "\n",
    "    # remove cut_nodes from dataframe\n",
    "    data = data[data['nodeID'].between((cut_nodes)+1,(num_nodes+cut_nodes))]\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'/'+foldername+'-'+savename+'.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def computeMeanData(dataPath,data,outname):\n",
    "\n",
    "    # make mean data frame\n",
    "    data_mean =  data.groupby(['subjectID','classID','structureID']).mean().reset_index()\n",
    "    data_mean['nodeID'] = [ 1 for f in range(len(data_mean['nodeID'])) ]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data_mean.to_csv(dataPath+outname+'.csv',index=False)\n",
    "\n",
    "    return data_mean\n",
    "\n",
    "### rank order effect size calculator\n",
    "def computeRankOrderEffectSize(groups,subjects,tissue,measures,stat,measures_to_average,data_dir):\n",
    "\n",
    "    comparison_array = list(combinations(groups,2)) # 2 x 2 array; 2 different comparisons, with two pairs per comparison. comparison_array[0] = (\"run_1\",\"run_2\")\n",
    "    es = {}\n",
    "    roes = {}\n",
    "\n",
    "    # compute effect size\n",
    "    for compar in comparison_array:\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.DataFrame([])\n",
    "        tmp = pd.DataFrame([])\n",
    "        tmp['structureID'] = stat['structureID'].unique()\n",
    "        for m in measures:\n",
    "            diff = stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').mean() - stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').mean()\n",
    "            pooled_var = (np.sqrt((stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').std() ** 2 + stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').std() ** 2) / 2))\n",
    "            effectSize = diff / pooled_var\n",
    "            tmp[m+\"_effect_size\"] = list(effectSize[m])\n",
    "        tmp.to_csv(data_dir+tissue+\"_effect_sizes_\"+compar[0]+\"_\"+compar[1]+\".csv\",index=False)\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.concat([es[compar[0]+\"_\"+compar[1]],tmp],ignore_index=True)\n",
    "\n",
    "    # rank order structures\n",
    "    for ma in measures_to_average:\n",
    "        if ma == ['ad','fa','md','rd','ga','ak','mk','rk']:\n",
    "            model = 'tensor'\n",
    "        elif ma == ['ndi','isovf','odi']:\n",
    "            model = 'noddi'\n",
    "        else:\n",
    "            model = ma\n",
    "\n",
    "        tmpdata = pd.DataFrame([])\n",
    "        tmpdata['structureID'] = stat['structureID'].unique()\n",
    "        for compar in comparison_array:\n",
    "            if model == 'tensor':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ad_effect_size','fa_effect_size','md_effect_size','rd_effect_size']].abs().mean(axis=1).tolist()\n",
    "            elif model == 'noddi':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ndi_effect_size','isovf_effect_size','odi_effect_size']].abs().mean(axis=1).tolist()\n",
    "            else:\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][[ma+'_effect_size']].abs().mean(axis=1).tolist()\n",
    "\n",
    "        tmpdata[model+\"_average_effect_size\"] =  tmpdata.mean(axis=1).tolist()\n",
    "        tmpdata.to_csv(data_dir+model+\"_average_\"+tissue+\"_effect_sizes.csv\",index=False)\n",
    "        roes[model] = tmpdata.sort_values(by=model+\"_average_effect_size\")['structureID'].tolist()\n",
    "\n",
    "    return roes\n",
    "\n",
    "def combineCorticalSubcortical(dataPath,corticalData,subcorticalData):\n",
    "\n",
    "    # remove unnecessary columns\n",
    "#     corticalData = corticalData.drop(columns=['snr','thickness'])\n",
    "    subcorticalData = subcorticalData.drop(columns=['parcID','number_of_voxels'])\n",
    "\n",
    "    # merge data frames\n",
    "    data = pd.concat([corticalData,subcorticalData],sort=False)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'graymatter_nodes.csv',index=False)\n",
    "\n",
    "    # identify gray matter names\n",
    "    graymatter_names = list(data['structureID'].unique())\n",
    "\n",
    "    # output track names\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    with open((dataPath+'graymatter_list.json'),'w') as gm_listf:\n",
    "        json.dump(graymatter_names,gm_listf)\n",
    "\n",
    "    return [graymatter_names,data]\n",
    "\n",
    "def computeDistance(x,y,metric):\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    if metric == 'euclidean':\n",
    "        dist = euclidean_distances([x,y])[0][1]\n",
    "    else:\n",
    "        dist = wasserstein([x,y])[0][1]\n",
    "        \n",
    "    return dist\n",
    "\n",
    "def profileFlipCheck(data,references_mean,subjects,structures,test_measure,flip_measures,dist_metric):\n",
    "    \n",
    "    flipped_subjects = []\n",
    "    flipped_structures = []\n",
    "    distance = []\n",
    "    flipped_distance = []\n",
    "    \n",
    "    for s in subjects:\n",
    "#         subj_data = data[data['subjectID'] == s]\n",
    "        for i in structures:\n",
    "#             struc_data = subj_data[subj_data['structureID'] == i]\n",
    "            struc_data = data.loc[data['subjectID'] == s].loc[data['structureID'] == i]\n",
    "#             references_data = references_mean[references_mean['structureID'] == i]\n",
    "            references_data = references_mean.loc[references_mean['structureID'] == i]\n",
    "            x = list(struc_data[test_measure].values.tolist())\n",
    "            y = list(references_data[test_measure].values.tolist())\n",
    "            dist = computeDistance(x,y,dist_metric)\n",
    "            dist_flipped = computeDistance(list(np.flip(x)),y,dist_metric)\n",
    "#             if np.abs(dist - dist_flipped) > 0.5:\n",
    "            if dist > dist_flipped:\n",
    "                if np.abs(dist - dist_flipped) > 0.1:\n",
    "#                     print('%s %s appears to be flipped. flipping data to match reference. distance=%s, flipped=%s' %(s,i,str(dist),str(dist_flipped)))\n",
    "#                     data[data['subjectID'] == s][data['structureID'] == i][flip_measures] = data[data['subjectID'] == s][data['structureID'] == i][flip_measures][::-1]\n",
    "                    flipped_subjects = np.append(flipped_subjects,s)\n",
    "                    flipped_structures = np.append(flipped_structures,i)\n",
    "                    distance = np.append(distance,dist)\n",
    "                    flipped_distance = np.append(flipped_distance,dist_flipped)\n",
    "    \n",
    "    output_summary = pd.DataFrame()\n",
    "    output_summary['flipped_subjects'] = flipped_subjects\n",
    "    output_summary['flipped_structures'] = flipped_structures\n",
    "    output_summary['distance'] = distance\n",
    "    output_summary['flipped_distance'] = flipped_distance\n",
    "    \n",
    "    return output_summary\n",
    "\n",
    "def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "    references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "    references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "    references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "    return references_mean, references_sd\n",
    "\n",
    "def computeDistance(x,y,metric):\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    if metric == 'euclidean':\n",
    "        dist = euclidean_distances([x,y])[0][1]\n",
    "    else:\n",
    "        dist = wasserstein([x,y])[0][1]\n",
    "        \n",
    "    return dist\n",
    "\n",
    "def outlierDetection(data,references_mean,references_sd,subjects,structures,measures,threshold,dist_metric):\n",
    "    \n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    outliers_subjects = []\n",
    "    outliers_structures = []\n",
    "    outliers_measures = []\n",
    "    outliers_metrics = []\n",
    "    dist = []\n",
    "\n",
    "#     for s in subjects:\n",
    "#         subj_data = data[data['subjectID'] == s]\n",
    "    for i in structures:\n",
    "        subj_data = data[data['structureID'] == i]\n",
    "        references_data = references_mean[references_mean['structureID'] == i]\n",
    "        for m in measures:\n",
    "            for s in subjects:\n",
    "                x = list(subj_data[m].values.tolist())\n",
    "                y = list(references_data[m].values.tolist())\n",
    "                dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "                \n",
    "            dist_percentiles = [np.percentile(dist,5),np.percentile(dist,95)]\n",
    "            outliers = np.any((dist < dist_percentiles[0])|(dist > dist_percentiles[1]))\n",
    "#             for o in outliers:\n",
    "                \n",
    "# #                 if np.abs(dist) > threshold:\n",
    "# #                     outliers_subjects = np.append(outliers_subjects,s)\n",
    "# #                     outliers_structures = np.append(outliers_structures,i)\n",
    "# #                     outliers_measures = np.append(outliers_measures,m)\n",
    "# #                     outliers_metrics = np.append(outliers_metrics,dist)\n",
    "\n",
    "#     outliers_dataframe = pd.DataFrame()\n",
    "#     outliers_dataframe['subjectID'] = outliers_subjects\n",
    "#     outliers_dataframe['structureID'] = outliers_structures\n",
    "#     outliers_dataframe['measures'] = outliers_measures\n",
    "#     outliers_dataframe['distance'] = outliers_metrics\n",
    "    \n",
    "#     return outliers_dataframe\n",
    "\n",
    "### scatter plot related scripts\n",
    "# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure\n",
    "def averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X = x_stat.groupby(measure).mean()[x_measure].tolist()\n",
    "    Y = y_stat.groupby(measure).mean()[y_measure].tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure\n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X,Y = [np.array([]),np.array([])]\n",
    "    for i in range(len(x_stat[measure].unique())):\n",
    "        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]\n",
    "        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]\n",
    "\n",
    "        if np.isnan(x).any() or np.isnan(y).any():\n",
    "            print(\"skipping %s due to nan\" %x_stat[measure].unique()[i])\n",
    "        else:\n",
    "            # checks to make sure the same data\n",
    "            if len(x) == len(y):\n",
    "                X = np.append(X,x)\n",
    "                Y = np.append(Y,y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject\n",
    "def ravelNetwork(x_stat,y_stat):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    X = np.ravel(x_stat).tolist()\n",
    "    Y = np.ravel(y_stat).tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel. \n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def ravelNonNetwork(x_stat,y_stat,x_measure,y_measure):\n",
    "\n",
    "    X = x_stat[x_measure].to_list()\n",
    "    Y = y_stat[y_measure].to_list()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# wrapper function to call either of the above scripts based on user input\n",
    "def setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,isnetwork,measure):\n",
    "\n",
    "    x_stat = x_data\n",
    "    y_stat = y_data\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        X,Y = averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'append':\n",
    "        X,Y = appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'ravel':\n",
    "        if isnetwork == True:\n",
    "            X,Y = ravelNetwork(x_stat,y_stat)\n",
    "        else:\n",
    "            X,Y = ravelNonNetwork(x_stat,y_stat,x_measure,y_measure)\n",
    "\n",
    "    return x_stat,y_stat,X,Y\n",
    "\n",
    "# function to shuffle data and colors\n",
    "def shuffleDataAlg(X,Y,hues):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    X,Y,hues = shuffle(X,Y,hues)\n",
    "\n",
    "    return X,Y,hues\n",
    "\n",
    "# simple display or figure save function\n",
    "def saveOrShowImg(dir_out,x_measure,y_measure,img_name):\n",
    "    import os,sys \n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    " \n",
    "        # save or show plot\n",
    "        if dir_out:\n",
    "            if not os.path.exists(dir_out):\n",
    "                os.mkdir(dir_out)\n",
    "\n",
    "            if x_measure == y_measure:\n",
    "                img_name_eps = img_name+'_'+x_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'.svg'\n",
    "            else:\n",
    "                img_name_eps = img_name+'_'+x_measure+'_vs_'+y_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'_vs_'+y_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'_vs_'+y_measure+'.svg'\n",
    "\n",
    "            plt.savefig(os.path.join(dir_out, img_name_eps),transparent=True)\n",
    "            plt.savefig(os.path.join(dir_out, img_name_png))     \n",
    "    #         plt.savefig(os.path.join(dir_out, img_name_svg))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results\n",
    "# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def relplotScatter(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,column_wrap,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data: CANNOT BE AVERAGE\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,'ravel',False,hue_measure)\n",
    "\n",
    "    p = sns.relplot(x=X,y=Y,col=x_stat[column_measure],hue=x_stat[hue_measure],kind=\"scatter\",s=100,col_wrap=column_wrap)\n",
    "\n",
    "    # setting counter. looping through axes to add important info and regression lines\n",
    "    i = 0\n",
    "    for ax in p.axes.flat:\n",
    "        x_lim,y_lim = [ax.get_xlim(),ax.get_ylim()]\n",
    "\n",
    "        if trendline == 'equality':\n",
    "            ax.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        elif trendline == 'linreg':\n",
    "            m,b = np.polyfit(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'],1)\n",
    "            ax.plot(ax.get_xticks(),m*ax.get_xticks() + b)\n",
    "            plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(x_measure)\n",
    "        ax.set_ylabel(y_measure)\n",
    "\n",
    "        # compute correlation for each subject and add to plots\n",
    "        corr = np.corrcoef(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'])[1][0]\n",
    "        plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # compute rmse for each subject and add to plots\n",
    "        rmse = np.sqrt(mean_squared_error(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y']))\n",
    "        plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # update counter\n",
    "        i = i+1\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def singleplotScatter(colors_dict,x_data,y_data,x_measure,y_measure,logX,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "    colors = sns.color_palette('colorblind',len(x_stat[hue_measure]))\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    if logX == True:\n",
    "        X = np.log10(X)\n",
    "\n",
    "    if colors_dict:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    else:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100)\t\t\t\t\t\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    if x_measure == y_measure:\n",
    "        p.axes.axis('square')\n",
    "        y_ticks = p.axes.get_yticks()\n",
    "        p.axes.set_xticks(y_ticks)\n",
    "        p.axes.set_yticks(p.axes.get_xticks())\n",
    "        p.axes.set_ylim(p.axes.get_xlim())\n",
    "        p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "    elif trendline == 'groupreg':\n",
    "        for g in range(len(groups)):\n",
    "            if stat_name == 'volume':\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            else:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure] for network correlations. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized.\n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def networkScatter(colors_dict,hues,groups,subjects,x_data,y_data,network_measure,shuffleData,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,\"\",\"\",\"ravel\",True,\"\")\n",
    "\n",
    "    # additional network setup\n",
    "    # hues = sns.color_palette(colormap,len(X))\n",
    "    # hues = hues.as_hex()\n",
    "    # keys = [ i for i in range(len(X)) ]\n",
    "    # colors_dict = dict(zip(hues,hues))\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    # if colors_dict:\n",
    "        # p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    # else:\n",
    "    p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    p.axes.axis('square')\n",
    "    y_ticks = p.axes.get_yticks()\n",
    "    p.axes.set_xticks(y_ticks)\n",
    "    p.axes.set_yticks(p.axes.get_xticks())\n",
    "    p.axes.set_ylim(p.axes.get_xlim())\n",
    "    p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s %s vs %s' %(network_measure,groups[0],groups[1]),fontsize=20)\n",
    "    plt.xlabel(groups[0],fontsize=18)\n",
    "    plt.ylabel(groups[1],fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,network_measure+'_'+groups[0],network_measure+'_'+groups[1],img_name)\n",
    "\n",
    "# uses matplotlib.pyplot's hist2d function to plot data from x_data[x_measure] and y_data[y_measure]. useful for supplementary figure or debugging or publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plot2dHist(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    plt.hist2d(x=X,y=Y,cmin=1,density=False,bins=(len(X)/10),cmap='magma',vmax=(len(X)/10))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # set title and x and y labels\n",
    "\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # # remove top and right spines from plot\n",
    "    # p.axes.spines[\"top\"].set_visible(False)\n",
    "    # p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "### tract profile data\n",
    "# uses matplotlib.pyplot's plot and fill_between functions to plot tract profile data from stat. useful for publication worthy figure\n",
    "# requires stat to be formatted in way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the track in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotProfiles(structures,stat,diffusion_measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os,sys\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # loop through all structures\n",
    "    for t in structures:\n",
    "        print(t)\n",
    "        # loop through all measures\n",
    "        for dm in diffusion_measures:\n",
    "            print(dm)\n",
    "\n",
    "            imgname=img_name+\"_\"+t+\"_\"+dm\n",
    "\n",
    "            # generate figures\n",
    "            fig = plt.figure(figsize=(15,15))\n",
    "#             fig = plt.figure()\n",
    "            fig.patch.set_visible(False)\n",
    "            p = plt.subplot()\n",
    "\n",
    "            # set title and catch array for legend handle\n",
    "            plt.title(\"%s Profiles %s: %s\" %(summary_method,t,dm),fontsize=20)\n",
    "\n",
    "            # loop through groups and plot profile data\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is nodes\n",
    "                x = stat['nodeID'].unique()\n",
    "\n",
    "                # y is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).mean()[dm][t]\n",
    "                elif summary_method == 'median':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).median()[dm][t]\n",
    "                elif summary_method == 'max':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).max()[dm][t]\n",
    "                elif summary_method == 'min':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).min()[dm][t]\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t] / np.sqrt(len(stat[stat['classID'] == stat.classID.unique()[g]]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t]\n",
    "\n",
    "                # plot summary\n",
    "                plt.plot(x,y,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],linewidth=5,label=stat.classID.unique()[g])\n",
    "\n",
    "                # plot shaded error\n",
    "                plt.fill_between(x,y-err,y+err,alpha=0.4,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='1 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "                plt.fill_between(x,y-(2*err),y+(2*err),alpha=0.2,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='2 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "\n",
    "            # set up labels and ticks\n",
    "            plt.xlabel('Location',fontsize=18)\n",
    "            plt.ylabel(dm,fontsize=18)\n",
    "            plt.xticks([x[0],x[-1]],['Begin','End'],fontsize=16)\n",
    "            plt.legend(fontsize=12)\n",
    "            y_lim = plt.ylim()\n",
    "            plt.yticks([np.round(y_lim[0],2),np.mean(y_lim),np.round(y_lim[1],2)],fontsize=16)\n",
    "\n",
    "            # remove top and right spines from plot\n",
    "            p.axes.spines[\"top\"].set_visible(False)\n",
    "            p.axes.spines[\"right\"].set_visible(False)\n",
    "            ax = plt.gca()\n",
    "\n",
    "            # save image or show image\n",
    "            saveOrShowImg(dir_out,dm,dm,imgname)\n",
    "\n",
    "### generic data plots\n",
    "## structure average\n",
    "# uses matplotlib.pyplot's errobar function to plot group average data for each structure with errorbars. useful for publication worthy figure\n",
    "# requires stat to be formatted in similar way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the structure in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotGroupStructureAverage(structures,tissue,stat,measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "\n",
    "    for dm in measures:\n",
    "        print(dm)\n",
    "\n",
    "        # generate figures\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "        fig.patch.set_visible(False)\n",
    "        p = plt.subplot()\n",
    "\n",
    "        # set y range\n",
    "        p.set_ylim([0,(len(structures)*len(stat.classID.unique()))+len(stat.classID.unique())])\n",
    "\n",
    "        # set spines and ticks, and labels\n",
    "        p.yaxis.set_ticks_position('left')\n",
    "        p.xaxis.set_ticks_position('bottom')\n",
    "        p.set_xlabel(dm,fontsize=18)\n",
    "        p.set_ylabel(\"Structures\",fontsize=18)\n",
    "        if len(stat.classID.unique()) < 3:\n",
    "            if len(stat.classID.unique()) == 2:\n",
    "                p.set_yticks(np.arange(1.5,(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "            else:\n",
    "                p.set_yticks(np.arange(1,len(structures)+1,step=1))\n",
    "        else:\n",
    "            p.set_yticks(np.arange((len(stat.classID.unique())-1),(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "        p.set_yticklabels(structures,fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "\n",
    "        # set title\n",
    "        plt.title(\"%s Group-Summary: %s\" %(summary_method,dm),fontsize=20)\n",
    "\n",
    "        # loop through structures\n",
    "        for t in range(len(structures)):\n",
    "            # loop through groups\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).mean()[dm][structures[t]]\n",
    "                elif summary_method == 'median':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).median()[dm][structures[t]]\n",
    "                elif summary_method == 'max':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).max()[dm][structures[t]]\n",
    "                elif summary_method == 'min':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).min()[dm][structures[t]]\n",
    "\n",
    "                # y is location on y axis\n",
    "                y = (len(stat.classID.unique())*(t+1)-len(stat.classID.unique()))+(g+1)\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]] / np.sqrt(len(stat[stat.classID.str.contains(stat.classID.unique()[g])]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]]\n",
    "\n",
    "                # plot data\n",
    "                if t == 0:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10,label=stat.classID.unique()[g])\n",
    "                else:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10)\n",
    "\n",
    "        # add legend\n",
    "        plt.legend(fontsize=16)\n",
    "\n",
    "        # remove top and right spines from plot\n",
    "        p.axes.spines[\"top\"].set_visible(False)\n",
    "        p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "        # save image or show image\n",
    "        saveOrShowImg(dir_out,dm,dm,img_name)\n",
    "\n",
    "def violinPlots(x_measure,y_measure,hue_measure,data,summary,scale,inner,cmap,dir_out,img_name):\n",
    "    \n",
    "    if hue_measure:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,hue=hue_measure,scale=scale,inner=inner,palette=cmap)\n",
    "    else:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,scale=scale,inner=inner,palette=cmap,orientation='horizontal')\n",
    "\n",
    "    if summary == 'mean':\n",
    "        summary = data.groupby([x_measure])[y_measure].mean()\n",
    "    elif summary == 'median':\n",
    "        summary = data.groupby([x_measure])[y_measure].median()\n",
    "    elif summary == 'mode':\n",
    "        summary = data.groupby([x_measure])[y_measure].mode()\n",
    "    elif summary == 'max':\n",
    "        summary = data.groupby([x_measure])[y_measure].max()\n",
    "    elif summary == 'min':\n",
    "        summary = data.groupby([x_measure])[y_measure].min()\n",
    "\n",
    "#     for xtick in violin.get_xticks():\n",
    "#         violin.text(xtick,summary[xtick],np.round(summary[xtick],3),horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "  \n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up variables\n",
      "setting up variables complete\n",
      "grabbing demographic data\n",
      "grabbing demographic data complete\n"
     ]
    }
   ],
   "source": [
    "### setting up variables and adding paths\n",
    "print(\"setting up variables\")\n",
    "topPath = \"./\"\n",
    "os.chdir(topPath)\n",
    "data_dir = topPath+'/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "img_dir = topPath+'/img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "groups = ['ping']\n",
    "colors_array = ['yellow']\n",
    "diff_micro_measures = ['ad','fa','md','rd','length','volume','count']\n",
    "\n",
    "print(\"setting up variables complete\")\n",
    "### grabbing subjects demographic data\n",
    "print(\"grabbing demographic data\")\n",
    "subjects_data = pd.read_csv('subjects_data.csv')\n",
    "print(\"grabbing demographic data complete\")\n",
    "\n",
    "colors = {}\n",
    "subjects = {}\n",
    "\n",
    "# loop through groups and identify subjects and set color schema for each group\n",
    "for g in range(len(groups)):\n",
    "    # set subjects array\n",
    "    subjects[groups[g]] =  subjects_data['subjectID']\n",
    "    subjects[groups[g]].sort_values()\n",
    "    # update subjects with HCP prefix to make plotting easier\n",
    "\n",
    "    # set colors array\n",
    "    colors_name = colors_array[g]\n",
    "    colors[groups[g]] = colors_array[g]\n",
    "\n",
    "\n",
    "### merge demo and subjects data\n",
    "# subjects_data = pd.merge(subjects_data,subjects_demo,on='subjectID')\n",
    "subjects_data['classID'] = [ groups[0] for f in range(len(subjects_data))]\n",
    "subjects_data['age'] = subjects_data['age'] / 12\n",
    "subjects_data.to_csv(data_dir+'/subjects_data_demo.csv')\n",
    "\n",
    "# # create subjects color dictionary\n",
    "colors_dict = createColorDictionary(subjects_data,'subjectID','colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing tractometry analyses\n"
     ]
    }
   ],
   "source": [
    "# #### tractometry analyses\n",
    "print(\"computing tractometry analyses\")\n",
    "\n",
    "# create data structures\n",
    "# tractmeasures data\n",
    "if os.path.isfile(data_dir+'/tractmeasures-total_nodes.csv'):\n",
    "    track_data = pd.read_csv(data_dir+'/tractmeasures-total_nodes.csv')\n",
    "    track_data = track_data.drop_duplicates()\n",
    "else:\n",
    "    track_data =  collectData('neuro/tractmeasures','macro_micro','dti','output_FiberStats.csv',subjects_data,colors,data_dir+'/tractmeasures-total_nodes.csv')\n",
    "    track_data = track_data.drop_duplicates()\n",
    "    \n",
    "# cut nodes\n",
    "if os.path.isfile(data_dir+'/tractmeasures-cut_nodes.csv'):\n",
    "    track_data_cut = pd.read_csv(data_dir+'/tractmeasures-cut_nodes.csv')\n",
    "    track_data_cut['colors'] = [ 'yellow' for f in track_data_cut['classID']]\n",
    "else:\n",
    "    track_data_cut = cutNodes(track_data,180,data_dir,'tractmeasures','cut_nodes')\n",
    "    track_data_cut['colors'] = [ colors[groups[0]] for f in track_data_cut['classID']]\n",
    "    track_data_cut.to_csv(data_dir+'/tractmeasures-cut_nodes.csv',index=False)\n",
    "\n",
    "# compute mean data\n",
    "if os.path.isfile(data_dir+'/tractmeasures-mean_nodes.csv'):\n",
    "    track_mean_data = pd.read_csv(data_dir+'/tractmeasures-mean_nodes.csv')\n",
    "else:\n",
    "    track_mean_data = computeMeanData(data_dir,track_data_cut,'tractmeasures-mean_nodes')\n",
    "    track_mean_data['colors'] = [ colors[groups[0]] for f in track_mean_data['subjectID']]\n",
    "    track_mean_data.to_csv(data_dir+'/tractmeasures-mean_nodes.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing group average gray matter parcel analyses\n"
     ]
    }
   ],
   "source": [
    "#### group average cortex mapping analyses\n",
    "print(\"computing group average gray matter parcel analyses\")\n",
    "\n",
    "# create data structures\n",
    "# cortical measures\n",
    "if os.path.isfile(data_dir+'/cortical_glasser.csv'):\n",
    "    cortical_glasser = pd.read_csv(data_dir+'/cortical_glasser.csv')\n",
    "    cortical_glasser = cortical_glasser.drop_duplicates()\n",
    "else:\n",
    "    cortical_glasser =  collectData('neuro/parc-stats','cortex_mapping_stats','dwi','parc_MEAN.csv',subjects_data,colors,data_dir+'/cortical_glasser.csv')\n",
    "    cortical_glasser = cortical_glasser.drop_duplicates()\n",
    "    cortical_glasser = cortical_glasser.loc[cortical_glasser['myelinmap'].isna()]\n",
    "    cortical_glasser = cortical_glasser.drop(columns={\"myelinmap\"})\n",
    "    cortical_glasser.to_csv(data_dir+'/cortical_glasser.csv',index=False)\n",
    "\n",
    "# replace vertex volume measure with overall parcel volume\n",
    "if os.path.isfile(data_dir+'/parcel_volume.csv'):\n",
    "    parcel_volume = pd.read_csv(data_dir+'/parcel_volume.csv')\n",
    "    parcel_volume = parcel_volume.drop_duplicates()\n",
    "else:\n",
    "    parcel_volume = collectData('neuro/parc-stats','surface','glasser','cortex.csv',subjects_data,colors,data_dir+'/parcel_volume.csv')\n",
    "    parcel_volume = parcel_volume.drop_duplicates()\n",
    "    parcel_volume = parcel_volume[~parcel_volume['structureID'].isin([\"lh_unknown_0\",\"rh_unknown_0\",\"lh_???\",\"rh_???\"])]\n",
    "    parcel_volume['structureID'] = [ f.split('h_')[1] for f in parcel_volume['structureID'] ]\n",
    "    parcel_volume = parcel_volume.rename(columns={\"gray_matter_volume_mm^3\": \"volume\", \"surface_area_mm^2\": \"surface_area\"})\n",
    "    parcel_volume = parcel_volume[['subjectID','structureID','volume','surface_area']]\n",
    "    parcel_volume.to_csv(data_dir+'/parcel_volume.csv',index=False)\n",
    "\n",
    "cortical_glasser = cortical_glasser.drop(columns='volume')\n",
    "cortical_glasser = pd.merge(cortical_glasser,parcel_volume,on=['subjectID','structureID'])\n",
    "\n",
    "# # subcortical measures\n",
    "if os.path.isfile(data_dir+'/subcortical.csv'):\n",
    "    subcortical = pd.read_csv(data_dir+'/subcortical.csv')\n",
    "    subcortical = subcortical.drop_duplicates()\n",
    "else:\n",
    "    subcortical =  collectData('neuro/parc-stats','subcort_stats','dwi','aseg_nodes.csv',subjects_data,colors,data_dir+'/subcortical.csv')\n",
    "    subcortical = subcortical.drop_duplicates()\n",
    "    subcortical.to_csv('subcortical.csv')\n",
    "\n",
    "# # extract only subcortical structures of interest\n",
    "# subcort_list = ['Left-Cerebellum-Cortex','Left-Thalamus-Proper','Left-Caudate','Left-Putamen','Left-Pallidum','Brain-Stem','Left-Hippocampus','Left-Amygdala','Left-Accumbens-area','Left-VentralDC','Right-Cerebellum-Cortex','Right-Thalamus-Proper','Right-Caudate','Right-Putamen','Right-Pallidum','Right-Hippocampus','Right-Amygdala','Right-Accumbens-area','Right-VentralDC'] \n",
    "# subcortical = subcortical[subcortical['structureID'].isin(subcort_list)]\n",
    "\n",
    "# clean and remove data that is outside physical bounds: cortical\n",
    "if os.path.isfile(data_dir+'/cortical_glasser_cleaned.csv'):\n",
    "    cortical_glasser = pd.read_csv(data_dir+'/cortical_glasser_cleaned.csv')\n",
    "else:\n",
    "    bad_cortical_glasser = pd.DataFrame()\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['ad'] > 3]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['ad'] < 0]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['fa'] > 1]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['fa'] < 0]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['md'] > 3]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['md'] < 0]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['rd'] > 3]])\n",
    "    bad_cortical_glasser = pd.concat([bad_cortical_glasser,cortical_glasser.loc[cortical_glasser['rd'] < 0]])\n",
    "    cortical_glasser = cortical_glasser.loc[~cortical_glasser.index.isin(bad_cortical_glasser.index)]\n",
    "    cortical_glasser.to_csv(data_dir+'/cortical_glasser_cleaned.csv',index=False)\n",
    "\n",
    "# clean and remove data that is outside physical bounds: subcortical\n",
    "if os.path.isfile(data_dir+'/subcortical_cleaned.csv'):\n",
    "    subcortical = pd.read_csv(data_dir+'/subcortical_cleaned.csv')\n",
    "else:\n",
    "    bad_subcortical = pd.DataFrame()\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['ad'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] > 1]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['fa'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['md'] < 0]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] > 3]])\n",
    "    bad_subcortical = pd.concat([bad_subcortical,subcortical.loc[subcortical['rd'] < 0]])\n",
    "    subcortical = subcortical.loc[~subcortical.index.isin(bad_subcortical.index)]\n",
    "    subcortical.to_csv(data_dir+'/subcortical_cleaned.csv',index=False)\n",
    "\n",
    "# combine the two\n",
    "if os.path.isfile(data_dir+'/graymatter_nodes.csv'):\n",
    "    graymatter = pd.read_csv(data_dir+'/graymatter_nodes.csv')\n",
    "    with open(data_dir+'/graymatter_list.json','r') as gmat_list:\n",
    "        graymatter_names = gmat_list.read()\n",
    "else:\n",
    "    [graymatter_names,graymatter] = combineCorticalSubcortical(data_dir,cortical_glasser,subcortical)\n",
    "    \n",
    "# for struc in graymatter.structureID.unique():\n",
    "#     print(struc)\n",
    "#     if '_ROI' in struc:\n",
    "#         data = cortical_glasser\n",
    "#         measures = [diff_micro_measures[1],diff_micro_measures[2],diff_micro_measures[4],diff_micro_measures[6],'volume','surface_area']\n",
    "#     else:\n",
    "#         data = graymatter\n",
    "#         measures = [diff_micro_measures[1],diff_micro_measures[2],diff_micro_measures[4],diff_micro_measures[6],'volume']\n",
    "#     for meas in measures:\n",
    "#         violinPlots('classID',meas,\"\",data.loc[data['structureID'] == struc],\"mean\",\"count\",\"points\",\"Oranges\",img_dir,struc+'-graymatter-diffusion-average-violins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectID</th>\n",
       "      <th>structureID</th>\n",
       "      <th>nodeID</th>\n",
       "      <th>ad</th>\n",
       "      <th>fa</th>\n",
       "      <th>md</th>\n",
       "      <th>rd</th>\n",
       "      <th>thickness</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>classID</th>\n",
       "      <th>volume</th>\n",
       "      <th>surface_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0133</td>\n",
       "      <td>L_V1_ROI</td>\n",
       "      <td>1</td>\n",
       "      <td>1.142991</td>\n",
       "      <td>0.134395</td>\n",
       "      <td>1.009352</td>\n",
       "      <td>0.942532</td>\n",
       "      <td>1.710567</td>\n",
       "      <td>M</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>ping</td>\n",
       "      <td>5216.0</td>\n",
       "      <td>2999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0133</td>\n",
       "      <td>L_MST_ROI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.972098</td>\n",
       "      <td>0.151260</td>\n",
       "      <td>0.838835</td>\n",
       "      <td>0.772203</td>\n",
       "      <td>2.472193</td>\n",
       "      <td>M</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>ping</td>\n",
       "      <td>596.0</td>\n",
       "      <td>283.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0133</td>\n",
       "      <td>L_V6_ROI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995697</td>\n",
       "      <td>0.149798</td>\n",
       "      <td>0.865406</td>\n",
       "      <td>0.800260</td>\n",
       "      <td>2.240462</td>\n",
       "      <td>M</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>ping</td>\n",
       "      <td>963.0</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0133</td>\n",
       "      <td>L_V2_ROI</td>\n",
       "      <td>1</td>\n",
       "      <td>1.052033</td>\n",
       "      <td>0.122879</td>\n",
       "      <td>0.937769</td>\n",
       "      <td>0.880636</td>\n",
       "      <td>1.758244</td>\n",
       "      <td>M</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>ping</td>\n",
       "      <td>4393.0</td>\n",
       "      <td>2619.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0133</td>\n",
       "      <td>L_V3_ROI</td>\n",
       "      <td>1</td>\n",
       "      <td>1.036660</td>\n",
       "      <td>0.133886</td>\n",
       "      <td>0.912189</td>\n",
       "      <td>0.849954</td>\n",
       "      <td>2.153966</td>\n",
       "      <td>M</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>ping</td>\n",
       "      <td>3249.0</td>\n",
       "      <td>1470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Posterior</td>\n",
       "      <td>1</td>\n",
       "      <td>2.142700</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>1.299400</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>ping</td>\n",
       "      <td>719.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Mid_Posterior</td>\n",
       "      <td>1</td>\n",
       "      <td>2.011500</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>1.369600</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>ping</td>\n",
       "      <td>383.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Central</td>\n",
       "      <td>1</td>\n",
       "      <td>1.990000</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>1.295700</td>\n",
       "      <td>0.948500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>ping</td>\n",
       "      <td>790.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4451</th>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Mid_Anterior</td>\n",
       "      <td>1</td>\n",
       "      <td>1.959200</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>1.368500</td>\n",
       "      <td>1.073200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>ping</td>\n",
       "      <td>673.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Anterior</td>\n",
       "      <td>1</td>\n",
       "      <td>1.975100</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>1.175600</td>\n",
       "      <td>0.775800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>ping</td>\n",
       "      <td>860.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43833 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subjectID       structureID  nodeID        ad        fa        md  \\\n",
       "0        P0133          L_V1_ROI       1  1.142991  0.134395  1.009352   \n",
       "1        P0133         L_MST_ROI       1  0.972098  0.151260  0.838835   \n",
       "2        P0133          L_V6_ROI       1  0.995697  0.149798  0.865406   \n",
       "3        P0133          L_V2_ROI       1  1.052033  0.122879  0.937769   \n",
       "4        P0133          L_V3_ROI       1  1.036660  0.133886  0.912189   \n",
       "...        ...               ...     ...       ...       ...       ...   \n",
       "4448     P1575      CC_Posterior       1  2.142700  0.576500  1.299400   \n",
       "4449     P1575  CC_Mid_Posterior       1  2.011500  0.426800  1.369600   \n",
       "4450     P1575        CC_Central       1  1.990000  0.478700  1.295700   \n",
       "4451     P1575   CC_Mid_Anterior       1  1.959200  0.436900  1.368500   \n",
       "4452     P1575       CC_Anterior       1  1.975100  0.554400  1.175600   \n",
       "\n",
       "            rd  thickness sex        age classID  volume  surface_area  \n",
       "0     0.942532   1.710567   M  15.416667    ping  5216.0        2999.0  \n",
       "1     0.772203   2.472193   M  15.416667    ping   596.0         283.0  \n",
       "2     0.800260   2.240462   M  15.416667    ping   963.0         451.0  \n",
       "3     0.880636   1.758244   M  15.416667    ping  4393.0        2619.0  \n",
       "4     0.849954   2.153966   M  15.416667    ping  3249.0        1470.0  \n",
       "...        ...        ...  ..        ...     ...     ...           ...  \n",
       "4448  0.877800        NaN   M   5.916667    ping   719.0           NaN  \n",
       "4449  1.048600        NaN   M   5.916667    ping   383.0           NaN  \n",
       "4450  0.948500        NaN   M   5.916667    ping   790.0           NaN  \n",
       "4451  1.073200        NaN   M   5.916667    ping   673.0           NaN  \n",
       "4452  0.775800        NaN   M   5.916667    ping   860.0           NaN  \n",
       "\n",
       "[43833 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graymatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa6ff93eb90>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot('volume',data=cortical_glasser.loc[cortical_glasser['structureID'] == 'L_V1_ROI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
