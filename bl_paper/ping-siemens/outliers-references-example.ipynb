{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,glob\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### subjects\n",
    "def collectSubjectData(topPath,dataPath,groups,subjects,colors):\n",
    "\n",
    "    # set up variables\n",
    "    data_columns = ['subjectID','classID','colors']\n",
    "    data =  pd.DataFrame([],columns=data_columns)\n",
    "\n",
    "    # populate structure\n",
    "    data['subjectID'] = [ f for g in groups for f in subjects[g] ]\n",
    "    data['classID'] = [ g for g in groups for f in range(len(subjects[g]))]\n",
    "    data['colors'] = [ colors[c] for c in colors for f in subjects[c]]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'subjects.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### color dictionary\n",
    "def createColorDictionary(data,measure,colorPalette):\n",
    "\n",
    "    keys = data[measure].unique()\n",
    "    values = sns.color_palette(colorPalette,len(keys))\n",
    "    values = values.as_hex()\n",
    "\n",
    "    colors_dict = dict(zip(keys,values))\n",
    "\n",
    "    return colors_dict\n",
    "\n",
    "### load parcellation stats data \n",
    "### load data \n",
    "def collectData(datatype,datatype_tags,tags,filename,subjects_data,colors,outPath):\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "\n",
    "    # grab path and data objects\n",
    "    objects = requests.get('https://brainlife.io/api/warehouse/secondary/list/%s'%os.environ['PROJECT_ID']).json()\n",
    "    \n",
    "    # subjects and paths\n",
    "    subjects = []\n",
    "    paths = []\n",
    "    \n",
    "    # set up output\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # loop through objects\n",
    "    for obj in objects:\n",
    "        if obj['datatype']['name'] == datatype:\n",
    "            if datatype_tags in obj['output']['datatype_tags']:\n",
    "                if tags in obj['output']['tags']:\n",
    "                    subjects = np.append(subjects,obj['output']['meta']['subject'])\n",
    "                    paths = np.append(paths,\"input/\"+obj[\"path\"]+\"/\"+filename)\n",
    "    \n",
    "    # sort paths by subject order\n",
    "    paths = [x for _,x in sorted(zip(subjects,paths))]\n",
    "\n",
    "    for i in paths:\n",
    "        tmpdata = pd.read_csv(i)\n",
    "        if tmpdata.subjectID.dtypes != 'object':\n",
    "            tmpdata['subjectID'] = [ str(int(np.float(f))) for f in tmpdata.subjectID ]\n",
    "        if 'classID' in tmpdata.keys():\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on=['subjectID','classID'])\n",
    "        else:\n",
    "            tmpdata = pd.merge(tmpdata,subjects_data,on='subjectID')\n",
    "        data = data.append(tmpdata,ignore_index=True)\n",
    "            \n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    # subjects.csv\n",
    "    data.to_csv(outPath,index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "### cut nodes\n",
    "def cutNodes(data,num_nodes,dataPath,foldername,savename):\n",
    "\n",
    "    # identify inner n nodes based on num_nodes input\n",
    "    total_nodes = len(data['nodeID'].unique())\n",
    "    cut_nodes = int((total_nodes - num_nodes) / 2)\n",
    "\n",
    "    # remove cut_nodes from dataframe\n",
    "    data = data[data['nodeID'].between((cut_nodes)+1,(num_nodes+cut_nodes))]\n",
    "\n",
    "    # replace empty spaces with nans\n",
    "    data = data.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'/'+foldername+'-'+savename+'.csv',index=False)\n",
    "\n",
    "    return data\n",
    "\n",
    "def computeMeanData(dataPath,data,outname):\n",
    "\n",
    "    # make mean data frame\n",
    "    data_mean =  data.groupby(['subjectID','classID','structureID']).mean().reset_index()\n",
    "    data_mean['nodeID'] = [ 1 for f in range(len(data_mean['nodeID'])) ]\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data_mean.to_csv(dataPath+outname+'.csv',index=False)\n",
    "\n",
    "    return data_mean\n",
    "\n",
    "### rank order effect size calculator\n",
    "def computeRankOrderEffectSize(groups,subjects,tissue,measures,stat,measures_to_average,data_dir):\n",
    "\n",
    "    comparison_array = list(combinations(groups,2)) # 2 x 2 array; 2 different comparisons, with two pairs per comparison. comparison_array[0] = (\"run_1\",\"run_2\")\n",
    "    es = {}\n",
    "    roes = {}\n",
    "\n",
    "    # compute effect size\n",
    "    for compar in comparison_array:\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.DataFrame([])\n",
    "        tmp = pd.DataFrame([])\n",
    "        tmp['structureID'] = stat['structureID'].unique()\n",
    "        for m in measures:\n",
    "            diff = stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').mean() - stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').mean()\n",
    "            pooled_var = (np.sqrt((stat[['structureID',m]][stat['classID'].str.contains(compar[0])].groupby('structureID').std() ** 2 + stat[['structureID',m]][stat['classID'].str.contains(compar[1])].groupby('structureID').std() ** 2) / 2))\n",
    "            effectSize = diff / pooled_var\n",
    "            tmp[m+\"_effect_size\"] = list(effectSize[m])\n",
    "        tmp.to_csv(data_dir+tissue+\"_effect_sizes_\"+compar[0]+\"_\"+compar[1]+\".csv\",index=False)\n",
    "        es[compar[0]+\"_\"+compar[1]] = pd.concat([es[compar[0]+\"_\"+compar[1]],tmp],ignore_index=True)\n",
    "\n",
    "    # rank order structures\n",
    "    for ma in measures_to_average:\n",
    "        if ma == ['ad','fa','md','rd','ga','ak','mk','rk']:\n",
    "            model = 'tensor'\n",
    "        elif ma == ['ndi','isovf','odi']:\n",
    "            model = 'noddi'\n",
    "        else:\n",
    "            model = ma\n",
    "\n",
    "        tmpdata = pd.DataFrame([])\n",
    "        tmpdata['structureID'] = stat['structureID'].unique()\n",
    "        for compar in comparison_array:\n",
    "            if model == 'tensor':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ad_effect_size','fa_effect_size','md_effect_size','rd_effect_size']].abs().mean(axis=1).tolist()\n",
    "            elif model == 'noddi':\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][['ndi_effect_size','isovf_effect_size','odi_effect_size']].abs().mean(axis=1).tolist()\n",
    "            else:\n",
    "                tmpdata[compar[0]+\"_\"+compar[1]+\"_\"+model+\"_average_effect_size\"] = es[compar[0]+\"_\"+compar[1]][[ma+'_effect_size']].abs().mean(axis=1).tolist()\n",
    "\n",
    "        tmpdata[model+\"_average_effect_size\"] =  tmpdata.mean(axis=1).tolist()\n",
    "        tmpdata.to_csv(data_dir+model+\"_average_\"+tissue+\"_effect_sizes.csv\",index=False)\n",
    "        roes[model] = tmpdata.sort_values(by=model+\"_average_effect_size\")['structureID'].tolist()\n",
    "\n",
    "    return roes\n",
    "\n",
    "def combineCorticalSubcortical(dataPath,corticalData,subcorticalData):\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    corticalData = corticalData.drop(columns=['snr','thickness'])\n",
    "    subcorticalData = subcorticalData.drop(columns=['parcID','number_of_voxels'])\n",
    "\n",
    "    # merge data frames\n",
    "    data = pd.concat([corticalData,subcorticalData],sort=False)\n",
    "\n",
    "    # output data structure for records and any further analyses\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    data.to_csv(dataPath+'graymatter_nodes.csv',index=False)\n",
    "\n",
    "    # identify gray matter names\n",
    "    graymatter_names = list(data['structureID'].unique())\n",
    "\n",
    "    # output track names\n",
    "    if not os.path.exists(dataPath):\n",
    "        os.mkdir(dataPath)\n",
    "\n",
    "    with open((dataPath+'graymatter_list.json'),'w') as gm_listf:\n",
    "        json.dump(graymatter_names,gm_listf)\n",
    "\n",
    "    return [graymatter_names,data]\n",
    "\n",
    "# # def computeDistance(x,y,metric):\n",
    "\n",
    "# #     from sklearn.metrics.pairwise import euclidean_distances\n",
    "# #     from scipy.stats import wasserstein_distance\n",
    "\n",
    "# #     if metric == 'euclidean':\n",
    "# #         dist = euclidean_distances([x,y])[0][1]\n",
    "# #     else:\n",
    "# #         dist = wasserstein_distance(x,y)\n",
    "        \n",
    "# #     return dist\n",
    "\n",
    "# def computeDistance(x,y,metric):\n",
    "\n",
    "#     from sklearn.metrics.pairwise import euclidean_distances\n",
    "#     from scipy.stats import wasserstein_distance\n",
    "\n",
    "#     if metric == 'euclidean':\n",
    "#         dist = euclidean_distances([x,y])[0][1]\n",
    "#     else:\n",
    "#         dist = [ wasserstein_distance([x],[y[0]]) for x in x ]\n",
    "        \n",
    "#     return dist\n",
    "\n",
    "\n",
    "# def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "#     references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "#     references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "#     references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "#     return references_mean, references_sd\n",
    "\n",
    "# # def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "# #     dist = []\n",
    "# #     subj = []\n",
    "# #     meas = []\n",
    "# #     struc = []\n",
    "\n",
    "# #     for i in structures:\n",
    "# #         print(i)\n",
    "# #         subj_data = data.loc[data['structureID'] == i]\n",
    "# #         references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "# #         for m in measures:\n",
    "# #             for s in subj_data.subjectID.unique():\n",
    "# #                 x = list(subj_data.loc[subj_data['subjectID'] == s][m].values.tolist())\n",
    "# #                 y = list(references_data[0][m].values.tolist())\n",
    "# #                 dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "# #                 subj = np.append(subj,s)\n",
    "# #                 meas = np.append(meas,m)\n",
    "# #                 struc = np.append(struc,i)\n",
    "\n",
    "# #     dist_dataframe = pd.DataFrame()\n",
    "# #     dist_dataframe['subjectID'] = subj\n",
    "# #     dist_dataframe['structureID'] = struc\n",
    "# #     dist_dataframe['measures'] = meas\n",
    "# #     dist_dataframe['distance'] = dist\n",
    "    \n",
    "# #     return dist_dataframe\n",
    "\n",
    "# def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "#     dist = []\n",
    "#     subj = []\n",
    "#     meas = []\n",
    "#     struc = []\n",
    "\n",
    "#     for i in structures:\n",
    "#         print(i)\n",
    "#         subj_data = data.loc[data['structureID'] == i]\n",
    "#         references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "#         for m in measures:\n",
    "#             dist = np.append(dist,computeDistance(subj_data[m].values,references_data[0][m].values,'emd'))\n",
    "#             subj = np.append(subj,subj_data.subjectID.unique().tolist())\n",
    "#             meas = np.append(meas,[ m for f in range(len(subj_data[m])) ])\n",
    "#             struc = np.append(struc,[ i for f in range(len(subj_data[m])) ])\n",
    "\n",
    "#     dist_dataframe = pd.DataFrame()\n",
    "#     dist_dataframe['subjectID'] = subj\n",
    "#     dist_dataframe['structureID'] = struc\n",
    "#     dist_dataframe['measures'] = meas\n",
    "#     dist_dataframe['distance'] = dist\n",
    "    \n",
    "#     return dist_dataframe\n",
    "\n",
    "# def outputReferenceJson(ref_data,measures,data_dir,filename):\n",
    "    \n",
    "#     reference_json = []\n",
    "#     for st in ref_data.structureID.unique():\n",
    "#         i=0\n",
    "#         tmp = {}\n",
    "#         tmp['structurename'] = st\n",
    "#         tmp['source'] = \"HCP\"\n",
    "#         for meas in measures:\n",
    "#             tmp[meas] = {}\n",
    "#             tmp[meas]['mean'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().mean()\n",
    "#             tmp[meas]['min'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().min()\n",
    "#             tmp[meas]['max'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().max()\n",
    "#             tmp[meas]['sd'] = ref_data.loc[ref_data['structureID'] == st][meas].dropna().std()\n",
    "#         reference_json.append(tmp)\n",
    "\n",
    "#     with open(data_dir+'/'+filename,'w') as ref_out_f:\n",
    "#         json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "#     return reference_json\n",
    "\n",
    "# def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "    \n",
    "#     reference_data = pd.DataFrame()\n",
    "    \n",
    "#     for s in outliers.structureID.unique():\n",
    "#         for m in outliers.measures.unique():\n",
    "#             if profile:\n",
    "#                 tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID','nodeID',m]].reset_index(drop=True)\n",
    "#             else:\n",
    "#                 tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID',m]].reset_index(drop=True)\n",
    "#             reference_data = pd.concat([reference_data,tmpdata])\n",
    "#     if not profile:\n",
    "#         reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "    \n",
    "#     reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "    \n",
    "#     return reference_data\n",
    "\n",
    "# def computeOutliers(distances,threshold):\n",
    "    \n",
    "#     outliers = pd.DataFrame()\n",
    "    \n",
    "#     for i in distances.structureID.unique():\n",
    "#         for m in distances.measures.unique():\n",
    "#             tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "#             outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "#     return outliers\n",
    "\n",
    "# def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,data_dir,filename):\n",
    "    \n",
    "#     import numpy as np, pandas as pd\n",
    "\n",
    "#     outliers_subjects = []\n",
    "#     outliers_structures = []\n",
    "#     outliers_measures = []\n",
    "#     outliers_metrics = []\n",
    "\n",
    "#     distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "#     outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "#     if build_outliers:\n",
    "#         if 'neID' in data.columns:\n",
    "#             reference_dataframe = buildReferenceData(data,outliers_dataframe,True)\n",
    "#             reference_json = []\n",
    "#         else:\n",
    "#             reference_dataframe = buildReferenceData(data,outliers_dataframe,False)\n",
    "#             reference_json = outputReferenceJson(reference_dataframe,measures,data_dir,filename)\n",
    "#     else:\n",
    "#         reference_dataframe = []\n",
    "#         reference_json = []\n",
    "        \n",
    "#     return distances, outliers_dataframe, reference_dataframe, reference_json\n",
    "\n",
    "def computeDistance(data,references_data,measures,metric):\n",
    "\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    if metric == 'euclidean':\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: euclidean_distances([x[measures].values.tolist(),references_data[measures].values.tolist()])[0][1]).values\n",
    "    else:\n",
    "        dist = data.groupby('subjectID',sort=False).apply(lambda x: wasserstein_distance(x[measures],[references_data[measures].values[0]]))\n",
    "\n",
    "    return dist\n",
    "\n",
    "def computeReferences(x,groupby_measures,index_measure,diff_measures):\n",
    "    \n",
    "    references_mean = x.groupby(groupby_measures).mean().reset_index(index_measure)\n",
    "    references_sd = x.groupby(groupby_measures).std().reset_index(index_measure)\n",
    "    references_sd[diff_measures] = references_sd[diff_measures] * 2\n",
    "    \n",
    "    return references_mean, references_sd\n",
    "\n",
    "def createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric):\n",
    "    \n",
    "    from scipy.stats import wasserstein_distance\n",
    "\n",
    "    dist = []\n",
    "    subj = []\n",
    "    meas = []\n",
    "    struc = []\n",
    "\n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        subj_data = data.loc[data['structureID'] == i]\n",
    "        references_data = computeReferences(subj_data,groupby_measure,groupby_measure,measures)\n",
    "        for m in measures:\n",
    "            if dist_metric == 'euclidean':\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'euclidean'))\n",
    "            else:\n",
    "                dist = np.append(dist,computeDistance(subj_data,references_data[0],m,'emd'))\n",
    "            \n",
    "            subj = np.append(subj,subj_data.subjectID.unique().tolist())\n",
    "            meas = np.append(meas,[ m for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "            struc = np.append(struc,[ i for f in range(len(subj_data.subjectID.unique().tolist())) ])\n",
    "\n",
    "    dist_dataframe = pd.DataFrame()\n",
    "    dist_dataframe['subjectID'] = subj\n",
    "    dist_dataframe['structureID'] = struc\n",
    "    dist_dataframe['measures'] = meas\n",
    "    dist_dataframe['distance'] = dist\n",
    "    \n",
    "    return dist_dataframe\n",
    "\n",
    "def outputReferenceJson(ref_data,measures,profile,resample_points,data_dir,filename):\n",
    "    \n",
    "    import json\n",
    "    from scipy.signal import resample\n",
    "\n",
    "    reference_json = []\n",
    "    for st in ref_data.structureID.unique():\n",
    "        tmp = {}\n",
    "        tmp['structurename'] = st\n",
    "        tmp['source'] = \"ping-siemens\"\n",
    "        for meas in measures:\n",
    "            tmp[meas] = {}\n",
    "            if profile:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][['nodeID',meas]].dropna().groupby('nodeID')[meas]\n",
    "            else:\n",
    "                gb_frame = ref_data.loc[ref_data['structureID'] == st][[meas]].dropna()[meas]\n",
    "            \n",
    "            if resample_points:\n",
    "                mean_tmp = resample(gb_frame.mean().values.tolist(),resample_points).tolist()\n",
    "                min_tmp = resample(gb_frame.min().values.tolist(),resample_points).tolist()\n",
    "                max_tmp = resample(gb_frame.max().values.tolist(),resample_points).tolist()\n",
    "                sd_tmp = resample(gb_frame.std().values.tolist(),resample_points).tolist()\n",
    "            else:\n",
    "                mean_tmp = gb_frame.mean()\n",
    "                min_tmp = gb_frame.min()\n",
    "                max_tmp = gb_frame.max()\n",
    "                sd_tmp = gb_frame.std()\n",
    "\n",
    "            tmp[meas]['mean'] = mean_tmp\n",
    "            tmp[meas]['min'] = min_tmp\n",
    "            tmp[meas]['max'] = max_tmp\n",
    "            tmp[meas]['sd'] = sd_tmp\n",
    "        reference_json.append(tmp)\n",
    "\n",
    "    with open(data_dir+'/'+filename+'.json','w') as ref_out_f:\n",
    "        json.dump(reference_json,ref_out_f)\n",
    "    \n",
    "    return reference_json\n",
    "\n",
    "def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "    \n",
    "    reference_data = pd.DataFrame()\n",
    "    \n",
    "    for s in outliers.structureID.unique():\n",
    "        for m in outliers.measures.unique():\n",
    "            if profile:\n",
    "                tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID','nodeID',m]].reset_index(drop=True)\n",
    "            else:\n",
    "                tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID',m]].reset_index(drop=True)\n",
    "            reference_data = pd.concat([reference_data,tmpdata])\n",
    "    if not profile:\n",
    "        reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "    \n",
    "    reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "    \n",
    "    return reference_data\n",
    "\n",
    "def computeOutliers(distances,threshold):\n",
    "    \n",
    "    outliers = pd.DataFrame()\n",
    "    \n",
    "    for i in distances.structureID.unique():\n",
    "        for m in distances.measures.unique():\n",
    "            tmpdata = distances.loc[distances['structureID'] == i].loc[distances['measures'] == m]\n",
    "            outliers = pd.concat([outliers,tmpdata[tmpdata['distance'] > np.percentile(tmpdata['distance'],threshold)]])\n",
    "            \n",
    "    return outliers\n",
    "\n",
    "def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,profile,resample_points,data_dir,filename):\n",
    "    \n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    outliers_subjects = []\n",
    "    outliers_structures = []\n",
    "    outliers_measures = []\n",
    "    outliers_metrics = []\n",
    "\n",
    "    distances = createDistanceDataframe(data,structures,groupby_measure,measures,dist_metric)\n",
    "    outliers_dataframe = computeOutliers(distances,threshold)\n",
    "    \n",
    "    if build_outliers:\n",
    "        reference_dataframe = buildReferenceData(data,outliers_dataframe,profile,data_dir,filename)\n",
    "#         reference_json = outputReferenceJson(reference_dataframe,measures,profile,resample_points,data_dir,filename)\n",
    "    else:\n",
    "        reference_dataframe = []\n",
    "    reference_json = []\n",
    "        \n",
    "    return distances, outliers_dataframe, reference_dataframe, reference_json\n",
    "\n",
    "\n",
    "def profileFlipCheck(data,subjects,structures,test_measure,flip_measures,dist_metric,threshold,outPath):\n",
    "    \n",
    "    flipped_subjects = []\n",
    "    flipped_structures = []\n",
    "    distance = []\n",
    "    flipped_distance = []\n",
    "    \n",
    "    for i in structures:\n",
    "        print(i)\n",
    "        struc_data = data.loc[data['structureID'] == i]\n",
    "        references_data = computeReferences(struc_data,'nodeID','nodeID',flip_measures)\n",
    "        differences = []\n",
    "        dist = []\n",
    "        dist_flipped = []\n",
    "\n",
    "        for s in subjects:\n",
    "            subj_data = struc_data.loc[data['subjectID'] == s]\n",
    "            x = list(subj_data[test_measure].values.tolist())\n",
    "            y = list(references_data[0][test_measure].values.tolist())\n",
    "            dist = np.append(dist,computeDistance(x,y,dist_metric))\n",
    "            dist_flipped = np.append(dist_flipped,computeDistance(list(np.flip(x)),y,dist_metric))\n",
    "            differences =  np.append(differences,(dist[-1]-dist_flipped[-1]))\n",
    "        \n",
    "        percentile_threshold = np.percentile(differences,threshold)\n",
    "#         print(percentile_threshold)\n",
    "        for m in range(len(differences)):\n",
    "            if differences[m] > 0 and differences[m] > percentile_threshold:\n",
    "#             if differences[m] > percentile_threshold:\n",
    "#                 print(subjects[m])\n",
    "                flipped_subjects = np.append(flipped_subjects,subjects[m])\n",
    "                flipped_structures = np.append(flipped_structures,i)\n",
    "                distance = np.append(distance,dist[m])\n",
    "                flipped_distance = np.append(flipped_distance,dist_flipped[m])\n",
    "    \n",
    "    output_summary = pd.DataFrame()\n",
    "    output_summary['flipped_subjects'] = flipped_subjects\n",
    "    output_summary['flipped_structures'] = flipped_structures\n",
    "    output_summary['distance'] = distance\n",
    "    output_summary['flipped_distance'] = flipped_distance\n",
    "    \n",
    "    if outPath:\n",
    "        output_summary.to_csv(outPath+'_flipped_profiles.csv',index=False)\n",
    "    \n",
    "    return output_summary\n",
    "\n",
    "### scatter plot related scripts\n",
    "# groups data by input measure and computes mean for each value in that column. x_stat is a pd dataframe, with each row being a single value, and each column being a different ID value or measure\n",
    "def averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X = x_stat.groupby(measure).mean()[x_measure].tolist()\n",
    "    Y = y_stat.groupby(measure).mean()[y_measure].tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# groups data by input measure and creates an array by appending data into x and y arrays. x_stat and y_stat are pd dataframes, with each row being a single value, and each column being a different ID value or measure\n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure):\n",
    "\n",
    "    X,Y = [np.array([]),np.array([])]\n",
    "    for i in range(len(x_stat[measure].unique())):\n",
    "        x = x_stat[x_stat[measure] == x_stat[measure].unique()[i]][x_measure]\n",
    "        y = y_stat[y_stat[measure] == y_stat[measure].unique()[i]][y_measure]\n",
    "\n",
    "        if np.isnan(x).any() or np.isnan(y).any():\n",
    "            print(\"skipping %s due to nan\" %x_stat[measure].unique()[i])\n",
    "        else:\n",
    "            # checks to make sure the same data\n",
    "            if len(x) == len(y):\n",
    "                X = np.append(X,x)\n",
    "                Y = np.append(Y,y)\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels networks. x_stat and y_stat should be S x M, where S is the number of subjects and M is the adjacency matrix for that subject\n",
    "def ravelNetwork(x_stat,y_stat):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    X = np.ravel(x_stat).tolist()\n",
    "    Y = np.ravel(y_stat).tolist()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# unravels nonnetwork data. x_stat and y_stat should be pd dataframes. x_measure and y_measure are the measure to unrvavel. \n",
    "# designed for test retest. x_stat and y_stat should have the same number of rows. but more importantly, should correspond to the same source (i.e. subject)\n",
    "# can be same pd.dataframe, but indexing of specific subject groups\n",
    "def ravelNonNetwork(x_stat,y_stat,x_measure,y_measure):\n",
    "\n",
    "    X = x_stat[x_measure].to_list()\n",
    "    Y = y_stat[y_measure].to_list()\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "# wrapper function to call either of the above scripts based on user input\n",
    "def setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,isnetwork,measure):\n",
    "\n",
    "    x_stat = x_data\n",
    "    y_stat = y_data\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        X,Y = averageWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'append':\n",
    "        X,Y = appendWithinColumn(x_stat,y_stat,x_measure,y_measure,measure)\n",
    "    elif ravelAverageAppend == 'ravel':\n",
    "        if isnetwork == True:\n",
    "            X,Y = ravelNetwork(x_stat,y_stat)\n",
    "        else:\n",
    "            X,Y = ravelNonNetwork(x_stat,y_stat,x_measure,y_measure)\n",
    "\n",
    "    return x_stat,y_stat,X,Y\n",
    "\n",
    "# function to shuffle data and colors\n",
    "def shuffleDataAlg(X,Y,hues):\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    X,Y,hues = shuffle(X,Y,hues)\n",
    "\n",
    "    return X,Y,hues\n",
    "\n",
    "# simple display or figure save function\n",
    "def saveOrShowImg(dir_out,x_measure,y_measure,img_name):\n",
    "    import os,sys \n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        # this will suppress all warnings in this block\n",
    "        warnings.simplefilter(\"ignore\")\n",
    " \n",
    "        # save or show plot\n",
    "        if dir_out:\n",
    "            if not os.path.exists(dir_out):\n",
    "                os.mkdir(dir_out)\n",
    "\n",
    "            if x_measure == y_measure:\n",
    "                img_name_eps = img_name+'_'+x_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'.svg'\n",
    "            else:\n",
    "                img_name_eps = img_name+'_'+x_measure+'_vs_'+y_measure+'.eps'\n",
    "                img_name_png = img_name+'_'+x_measure+'_vs_'+y_measure+'.png'\n",
    "                img_name_svg = img_name+'_'+x_measure+'_vs_'+y_measure+'.svg'\n",
    "\n",
    "            plt.savefig(os.path.join(dir_out, img_name_eps),transparent=True)\n",
    "            plt.savefig(os.path.join(dir_out, img_name_png))     \n",
    "    #         plt.savefig(os.path.join(dir_out, img_name_svg))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "# uses seaborn's relplot function to plot data for each unique value in a column of a pandas dataframe (ex. subjects, structureID). useful for supplementary figures or sanity checking or preliminary results\n",
    "# column measure is the measure within which each unique value will have its own plot. hue_measure is the column to use for coloring the data. column_wrap is how many panels you want per row\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def relplotScatter(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,column_wrap,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data: CANNOT BE AVERAGE\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,'ravel',False,hue_measure)\n",
    "\n",
    "    p = sns.relplot(x=X,y=Y,col=x_stat[column_measure],hue=x_stat[hue_measure],kind=\"scatter\",s=100,col_wrap=column_wrap)\n",
    "\n",
    "    # setting counter. looping through axes to add important info and regression lines\n",
    "    i = 0\n",
    "    for ax in p.axes.flat:\n",
    "        x_lim,y_lim = [ax.get_xlim(),ax.get_ylim()]\n",
    "\n",
    "        if trendline == 'equality':\n",
    "            ax.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "        elif trendline == 'linreg':\n",
    "            m,b = np.polyfit(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'],1)\n",
    "            ax.plot(ax.get_xticks(),m*ax.get_xticks() + b)\n",
    "            plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.set_xlabel(x_measure)\n",
    "        ax.set_ylabel(y_measure)\n",
    "\n",
    "        # compute correlation for each subject and add to plots\n",
    "        corr = np.corrcoef(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y'])[1][0]\n",
    "        plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # compute rmse for each subject and add to plots\n",
    "        rmse = np.sqrt(mean_squared_error(p.data[p.data[column_measure] == x_stat[column_measure].unique()[i]]['x'],p.data[p.data[column_measure] == y_stat[column_measure].unique()[i]]['y']))\n",
    "        plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=12,verticalalignment=\"top\",horizontalalignment=\"left\",transform=ax.transAxes)\n",
    "\n",
    "        # update counter\n",
    "        i = i+1\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure]. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def singleplotScatter(colors_dict,x_data,y_data,x_measure,y_measure,logX,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "    colors = sns.color_palette('colorblind',len(x_stat[hue_measure]))\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    if logX == True:\n",
    "        X = np.log10(X)\n",
    "\n",
    "    if colors_dict:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    else:\n",
    "        p = sns.scatterplot(x=X,y=Y,hue=hues,s=100)\t\t\t\t\t\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    if x_measure == y_measure:\n",
    "        p.axes.axis('square')\n",
    "        y_ticks = p.axes.get_yticks()\n",
    "        p.axes.set_xticks(y_ticks)\n",
    "        p.axes.set_yticks(p.axes.get_xticks())\n",
    "        p.axes.set_ylim(p.axes.get_xlim())\n",
    "        p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "    elif trendline == 'groupreg':\n",
    "        for g in range(len(groups)):\n",
    "            if stat_name == 'volume':\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=np.log10(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name]),y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            else:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm])\n",
    "                ax = sns.regplot(x=stat[['structureID',stat_name]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[stat_name],y=stat[['structureID',dm]][stat['subjectID'].str.contains('%s_' %str(g+1))].groupby('structureID',as_index=False).mean()[dm],color=colors[groups[g]],scatter=True,line_kws={'label':\"y={0:.5f}x+{1:.4f}\".format(slope,intercept)})\n",
    "\n",
    "            ax.legend()\n",
    "\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "# uses seaborn's scatter function to plot data from x_data[x_measure] and y_data[y_measure] for network correlations. useful for publication worthy figure\n",
    "# column measure is the measure within which data will be summarized.\n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def networkScatter(colors_dict,hues,groups,subjects,x_data,y_data,network_measure,shuffleData,trendline,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,\"\",\"\",\"ravel\",True,\"\")\n",
    "\n",
    "    # additional network setup\n",
    "    # hues = sns.color_palette(colormap,len(X))\n",
    "    # hues = hues.as_hex()\n",
    "    # keys = [ i for i in range(len(X)) ]\n",
    "    # colors_dict = dict(zip(hues,hues))\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "    # if colors_dict:\n",
    "        # p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "    # else:\n",
    "    p = sns.scatterplot(x=X,y=Y,hue=hues,s=100,palette=colors_dict,legend=False)\n",
    "\n",
    "    # set x and ylimits, plot line of equality, and legend\n",
    "    p.axes.axis('square')\n",
    "    y_ticks = p.axes.get_yticks()\n",
    "    p.axes.set_xticks(y_ticks)\n",
    "    p.axes.set_yticks(p.axes.get_xticks())\n",
    "    p.axes.set_ylim(p.axes.get_xlim())\n",
    "    p.axes.set_xlim(p.axes.get_xlim())\n",
    "\n",
    "    x_lim,y_lim = [p.axes.get_xlim(),p.axes.get_ylim()]\n",
    "\n",
    "    # trendline: either equality or linear regression\n",
    "    if trendline == 'equality':\n",
    "        p.plot(x_lim,y_lim,ls=\"--\",c='k')\n",
    "    elif trendline == 'linreg':\n",
    "        m,b = np.polyfit(X,Y,1)\n",
    "        p.plot(p.get_xticks(),m*p.get_xticks() + b,c='k')\n",
    "        plt.text(0.1,0.7,'y = %s x + %s' %(str(np.round(m,4)),str(np.round(b,4))),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute correlation for each subject and add to plots\n",
    "    corr = np.corrcoef(X,Y)[1][0]\n",
    "    plt.text(0.1,0.9,'r = %s' %str(np.round(corr,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # compute rmse for each subject and add to plots\n",
    "    rmse = np.sqrt(mean_squared_error(X,Y))\n",
    "    plt.text(0.1,0.8,'rmse = %s' %str(np.round(rmse,4)),fontsize=16,verticalalignment=\"top\",horizontalalignment=\"left\",transform=p.axes.transAxes)\n",
    "\n",
    "    # set title and x and y labels\n",
    "    plt.title('%s %s vs %s' %(network_measure,groups[0],groups[1]),fontsize=20)\n",
    "    plt.xlabel(groups[0],fontsize=18)\n",
    "    plt.ylabel(groups[1],fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # remove top and right spines from plot\n",
    "    p.axes.spines[\"top\"].set_visible(False)\n",
    "    p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,network_measure+'_'+groups[0],network_measure+'_'+groups[1],img_name)\n",
    "\n",
    "# uses matplotlib.pyplot's hist2d function to plot data from x_data[x_measure] and y_data[y_measure]. useful for supplementary figure or debugging or publication worthy figure\n",
    "# column measure is the measure within which data will be summarized. hue_measure is the column to use for coloring the data. \n",
    "# ravelAverageAppend is a string value of either 'append' to use the append function, 'ravel' to use the ravel function, or 'average' to use the average function\n",
    "# trendline, depending on user input, can either be the linear regression between x_data[x_measure] and y_data[y_measure] or the line of equality\n",
    "# dir_out and img_name are the directory where the figures should be saved and the name for the image. will save .eps and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plot2dHist(x_data,y_data,x_measure,y_measure,column_measure,hue_measure,ravelAverageAppend,trendline,shuffleData,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # grab data\n",
    "    [x_stat,y_stat,X,Y] = setupData(x_data,y_data,x_measure,y_measure,ravelAverageAppend,False,column_measure)\n",
    "\n",
    "    if ravelAverageAppend == 'average':\n",
    "        if isinstance(x_stat[hue_measure].unique()[0],str):\n",
    "            hues = x_stat[hue_measure].unique().tolist()\n",
    "        else:\n",
    "            hues = x_stat.groupby(column_measure).mean()[hue_measure].tolist()\n",
    "    else:\n",
    "        hues = list(x_stat[hue_measure])\n",
    "\n",
    "    if shuffleData == True:\n",
    "        X,Y,hues = shuffleDataAlg(X,Y,hues)\n",
    "\n",
    "\n",
    "    # generate new figure for each\n",
    "    p = plt.figure()\n",
    "\n",
    "    plt.hist2d(x=X,y=Y,cmin=1,density=False,bins=(len(X)/10),cmap='magma',vmax=(len(X)/10))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # set title and x and y labels\n",
    "\n",
    "    plt.title('%s vs %s' %(x_measure,y_measure),fontsize=20)\n",
    "    plt.xlabel(x_measure,fontsize=18)\n",
    "    plt.ylabel(y_measure,fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    # # remove top and right spines from plot\n",
    "    # p.axes.spines[\"top\"].set_visible(False)\n",
    "    # p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # save image or show image\n",
    "    saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "### tract profile data\n",
    "# uses matplotlib.pyplot's plot and fill_between functions to plot tract profile data from stat. useful for publication worthy figure\n",
    "# requires stat to be formatted in way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the track in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotProfiles(structures,stat,diffusion_measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os,sys\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # loop through all structures\n",
    "    for t in structures:\n",
    "        print(t)\n",
    "        # loop through all measures\n",
    "        for dm in diffusion_measures:\n",
    "            print(dm)\n",
    "\n",
    "            imgname=img_name+\"_\"+t+\"_\"+dm\n",
    "\n",
    "            # generate figures\n",
    "            fig = plt.figure(figsize=(15,15))\n",
    "#             fig = plt.figure()\n",
    "            fig.patch.set_visible(False)\n",
    "            p = plt.subplot()\n",
    "\n",
    "            # set title and catch array for legend handle\n",
    "            plt.title(\"%s Profiles %s: %s\" %(summary_method,t,dm),fontsize=20)\n",
    "\n",
    "            # loop through groups and plot profile data\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is nodes\n",
    "                x = stat['nodeID'].unique()\n",
    "\n",
    "                # y is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).mean()[dm][t]\n",
    "                elif summary_method == 'median':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).median()[dm][t]\n",
    "                elif summary_method == 'max':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).max()[dm][t]\n",
    "                elif summary_method == 'min':\n",
    "                    y = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).min()[dm][t]\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t] / np.sqrt(len(stat[stat['classID'] == stat.classID.unique()[g]]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat['classID'] == stat.classID.unique()[g]].groupby(['structureID','nodeID']).std()[dm][t]\n",
    "\n",
    "                # plot summary\n",
    "                plt.plot(x,y,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],linewidth=5,label=stat.classID.unique()[g])\n",
    "\n",
    "                # plot shaded error\n",
    "                plt.fill_between(x,y-err,y+err,alpha=0.4,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='1 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "                plt.fill_between(x,y-(2*err),y+(2*err),alpha=0.2,color=stat[stat['classID'] == stat.classID.unique()[g]]['colors'].unique()[0],label='2 %s %s' %(error_method,stat.classID.unique()[g]))\n",
    "\n",
    "            # set up labels and ticks\n",
    "            plt.xlabel('Location',fontsize=18)\n",
    "            plt.ylabel(dm,fontsize=18)\n",
    "            plt.xticks([x[0],x[-1]],['Begin','End'],fontsize=16)\n",
    "            plt.legend(fontsize=12)\n",
    "            y_lim = plt.ylim()\n",
    "            plt.yticks([np.round(y_lim[0],2),np.mean(y_lim),np.round(y_lim[1],2)],fontsize=16)\n",
    "\n",
    "            # remove top and right spines from plot\n",
    "            p.axes.spines[\"top\"].set_visible(False)\n",
    "            p.axes.spines[\"right\"].set_visible(False)\n",
    "            ax = plt.gca()\n",
    "\n",
    "            # save image or show image\n",
    "#             saveOrShowImg(dir_out,dm,dm,imgname)\n",
    "    return fig\n",
    "\n",
    "### generic data plots\n",
    "## structure average\n",
    "# uses matplotlib.pyplot's errobar function to plot group average data for each structure with errorbars. useful for publication worthy figure\n",
    "# requires stat to be formatted in similar way of AFQ_Brwoser and Yeatman et al 2018 () 'nodes.csv' files\n",
    "# groups is a list array of names of groups found in 'classID' of stat to plot\n",
    "# colors is a dictionary with the classID from groups set as the key and a color name as the value. will use these colors in profiles\n",
    "# tracks is a list array that will be looped through to make plots. if only one track is wanted, set structures=['structure_name'], with 'structure_name' being the name of the structure in the 'structureID' field of stat\n",
    "# stat is the pandas dataframe with all of the profile data. each row is a node for a track for a subject\n",
    "# diffusion_measures is a list array of the column measures found within stat. was developed with diffusion MRI metrics in mind, but can be any measure\n",
    "# summary_method is a string of either 'mean' to plot the average profile data, 'max' to plot max, 'min' to plot min, and 'median' to plot median\n",
    "# error_method is a string of either 'std' for the error bars to be set to the standard deviation or 'sem' for standard error of mean\n",
    "# dir_out and imgName are the directory where the figures should be saved and the name for the image. will save .pdf and .png\n",
    "# if want to view plot instead of save, set dir_out=\"\"\n",
    "def plotGroupStructureAverage(structures,tissue,stat,measures,summary_method,error_method,dir_out,img_name):\n",
    "\n",
    "    import os,sys\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "\n",
    "    for dm in measures:\n",
    "        print(dm)\n",
    "\n",
    "        # generate figures\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "        fig.patch.set_visible(False)\n",
    "        p = plt.subplot()\n",
    "\n",
    "        # set y range\n",
    "        p.set_ylim([0,(len(structures)*len(stat.classID.unique()))+len(stat.classID.unique())])\n",
    "\n",
    "        # set spines and ticks, and labels\n",
    "        p.yaxis.set_ticks_position('left')\n",
    "        p.xaxis.set_ticks_position('bottom')\n",
    "        p.set_xlabel(dm,fontsize=18)\n",
    "        p.set_ylabel(\"Structures\",fontsize=18)\n",
    "        if len(stat.classID.unique()) < 3:\n",
    "            if len(stat.classID.unique()) == 2:\n",
    "                p.set_yticks(np.arange(1.5,(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "            else:\n",
    "                p.set_yticks(np.arange(1,len(structures)+1,step=1))\n",
    "        else:\n",
    "            p.set_yticks(np.arange((len(stat.classID.unique())-1),(len(structures)*len(stat.classID.unique())),step=len(stat.classID.unique())))\n",
    "        p.set_yticklabels(structures,fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "\n",
    "        # set title\n",
    "        plt.title(\"%s Group-Summary: %s\" %(summary_method,dm),fontsize=20)\n",
    "\n",
    "        # loop through structures\n",
    "        for t in range(len(structures)):\n",
    "            # loop through groups\n",
    "            for g in range(len(stat.classID.unique())):\n",
    "                # x is summary (mean, median, max, main) profile data\n",
    "                if summary_method == 'mean':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).mean()[dm][structures[t]]\n",
    "                elif summary_method == 'median':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).median()[dm][structures[t]]\n",
    "                elif summary_method == 'max':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).max()[dm][structures[t]]\n",
    "                elif summary_method == 'min':\n",
    "                    x = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).min()[dm][structures[t]]\n",
    "\n",
    "                # y is location on y axis\n",
    "                y = (len(stat.classID.unique())*(t+1)-len(stat.classID.unique()))+(g+1)\n",
    "\n",
    "                # error bar is either: standard error of mean (sem), standard deviation (std)\n",
    "                if error_method == 'sem':\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]] / np.sqrt(len(stat[stat.classID.str.contains(stat.classID.unique()[g])]['subjectID'].unique()))\n",
    "                else:\n",
    "                    err = stat[stat.classID.str.contains(stat.classID.unique()[g])].groupby(tissue).std()[dm][structures[t]]\n",
    "\n",
    "                # plot data\n",
    "                if t == 0:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10,label=stat.classID.unique()[g])\n",
    "                else:\n",
    "                    p.errorbar(x=x,y=y,xerr=err,barsabove=True,ecolor='black',color=stat[stat['classID'].str.contains(stat.classID.unique()[g])]['colors'].unique()[0],marker='o',ms=10)\n",
    "\n",
    "        # add legend\n",
    "        plt.legend(fontsize=16)\n",
    "\n",
    "        # remove top and right spines from plot\n",
    "        p.axes.spines[\"top\"].set_visible(False)\n",
    "        p.axes.spines[\"right\"].set_visible(False)\n",
    "\n",
    "        # save image or show image\n",
    "        saveOrShowImg(dir_out,dm,dm,img_name)\n",
    "\n",
    "def violinPlots(x_measure,y_measure,hue_measure,data,summary,scale,inner,cmap,dir_out,img_name):\n",
    "    \n",
    "    if hue_measure:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,hue=hue_measure,scale=scale,inner=inner,orientation='horizontal')\n",
    "    else:\n",
    "        violin = sns.violinplot(x=x_measure,y=y_measure,data=data,scale=scale,inner=inner,orientation='horizontal')\n",
    "\n",
    "    if summary == 'mean':\n",
    "        summary = data.groupby([x_measure])[y_measure].mean()\n",
    "    elif summary == 'median':\n",
    "        summary = data.groupby([x_measure])[y_measure].median()\n",
    "    elif summary == 'mode':\n",
    "        summary = data.groupby([x_measure])[y_measure].mode()\n",
    "    elif summary == 'max':\n",
    "        summary = data.groupby([x_measure])[y_measure].max()\n",
    "    elif summary == 'min':\n",
    "        summary = data.groupby([x_measure])[y_measure].min()\n",
    "\n",
    "#     for xtick in violin.get_xticks():\n",
    "#         violin.text(xtick,summary[xtick],np.round(summary[xtick],3),horizontalalignment='center',size='medium',color='w',weight='semibold')\n",
    "  \n",
    "#     saveOrShowImg(dir_out,x_measure,y_measure,img_name)\n",
    "\n",
    "    return violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up variables\n",
      "setting up variables complete\n",
      "grabbing demographic data\n",
      "grabbing demographic data complete\n"
     ]
    }
   ],
   "source": [
    "### setting up variables and adding paths\n",
    "print(\"setting up variables\")\n",
    "topPath = \"./\"\n",
    "os.chdir(topPath)\n",
    "data_dir = topPath+'/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "img_dir = topPath+'/img/'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "groups = ['ping']\n",
    "colors_array = ['yellow']\n",
    "diff_micro_measures = ['ad','fa','md','rd','length','volume','count']\n",
    "\n",
    "print(\"setting up variables complete\")\n",
    "\n",
    "### create subjects.csv\n",
    "# print(\"creating subjects.csv\")\n",
    "# from compile_data import collectSubjectData\n",
    "# subjects_data = collectSubjectData(topPath,data_dir,groups,subjects,colors)\n",
    "# print(\"creating subjects.csv complete\")\n",
    "\n",
    "### grabbing subjects demographic data\n",
    "print(\"grabbing demographic data\")\n",
    "subjects_data = pd.read_csv(data_dir+'/subjects_data_demo.csv')\n",
    "print(\"grabbing demographic data complete\")\n",
    "\n",
    "colors = {}\n",
    "subjects = {}\n",
    "\n",
    "# loop through groups and identify subjects and set color schema for each group\n",
    "for g in range(len(groups)):\n",
    "    # set subjects array\n",
    "    subjects[groups[g]] =  subjects_data['subjectID']\n",
    "    subjects[groups[g]].sort_values()\n",
    "    # update subjects with HCP prefix to make plotting easier\n",
    "\n",
    "    # set colors array\n",
    "    colors_name = colors_array[g]\n",
    "    colors[groups[g]] = colors_array[g]\n",
    "\n",
    "\n",
    "### merge demo and subjects data\n",
    "# subjects_data = pd.merge(subjects_data,subjects_demo,on='subjectID')\n",
    "subjects_data['classID'] = [ groups[0] for f in range(len(subjects_data))]\n",
    "subjects_data.to_csv(data_dir+'/subjects_data_demo.csv')\n",
    "\n",
    "# # create subjects color dictionary\n",
    "colors_dict = createColorDictionary(subjects_data,'subjectID','colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "subcortical = pd.read_csv('./data/subcortical_reference_preclean.csv')\n",
    "track_data = pd.read_csv(data_dir+'/tractmeasures-reference_preclean.csv')\n",
    "# track_data['colors'] = ['yellow' for f in track_data['subjectID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcID</th>\n",
       "      <th>subjectID</th>\n",
       "      <th>structureID</th>\n",
       "      <th>nodeID</th>\n",
       "      <th>number_of_voxels</th>\n",
       "      <th>ad</th>\n",
       "      <th>fa</th>\n",
       "      <th>md</th>\n",
       "      <th>rd</th>\n",
       "      <th>gray_matter_volume_mm^3</th>\n",
       "      <th>sex</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>P0133</td>\n",
       "      <td>Left-Cerebral-White-Matter</td>\n",
       "      <td>1</td>\n",
       "      <td>254214</td>\n",
       "      <td>1.1730</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>254214.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>P0133</td>\n",
       "      <td>Left-Cerebral-Cortex</td>\n",
       "      <td>1</td>\n",
       "      <td>269805</td>\n",
       "      <td>1.1130</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>0.9106</td>\n",
       "      <td>269805.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>P0133</td>\n",
       "      <td>Left-Inf-Lat-Vent</td>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "      <td>2.5920</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>2.0895</td>\n",
       "      <td>1.8383</td>\n",
       "      <td>254.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>P0133</td>\n",
       "      <td>Left-Cerebellum-White-Matter</td>\n",
       "      <td>1</td>\n",
       "      <td>15761</td>\n",
       "      <td>1.1368</td>\n",
       "      <td>0.3502</td>\n",
       "      <td>0.8160</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>15761.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>P0133</td>\n",
       "      <td>Left-Cerebellum-Cortex</td>\n",
       "      <td>1</td>\n",
       "      <td>59880</td>\n",
       "      <td>1.1135</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>0.9446</td>\n",
       "      <td>0.8602</td>\n",
       "      <td>59880.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>251</td>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Posterior</td>\n",
       "      <td>1</td>\n",
       "      <td>719</td>\n",
       "      <td>2.1427</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>1.2994</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>719.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>252</td>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Mid_Posterior</td>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "      <td>2.0115</td>\n",
       "      <td>0.4268</td>\n",
       "      <td>1.3696</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>383.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>253</td>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Central</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>1.9900</td>\n",
       "      <td>0.4787</td>\n",
       "      <td>1.2957</td>\n",
       "      <td>0.9485</td>\n",
       "      <td>790.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4451</th>\n",
       "      <td>254</td>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Mid_Anterior</td>\n",
       "      <td>1</td>\n",
       "      <td>673</td>\n",
       "      <td>1.9592</td>\n",
       "      <td>0.4369</td>\n",
       "      <td>1.3685</td>\n",
       "      <td>1.0732</td>\n",
       "      <td>673.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>255</td>\n",
       "      <td>P1575</td>\n",
       "      <td>CC_Anterior</td>\n",
       "      <td>1</td>\n",
       "      <td>860</td>\n",
       "      <td>1.9751</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>1.1756</td>\n",
       "      <td>0.7758</td>\n",
       "      <td>860.0</td>\n",
       "      <td>M</td>\n",
       "      <td>ping-siemens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4453 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      parcID subjectID                   structureID  nodeID  \\\n",
       "0          2     P0133    Left-Cerebral-White-Matter       1   \n",
       "1          3     P0133          Left-Cerebral-Cortex       1   \n",
       "2          5     P0133             Left-Inf-Lat-Vent       1   \n",
       "3          7     P0133  Left-Cerebellum-White-Matter       1   \n",
       "4          8     P0133        Left-Cerebellum-Cortex       1   \n",
       "...      ...       ...                           ...     ...   \n",
       "4448     251     P1575                  CC_Posterior       1   \n",
       "4449     252     P1575              CC_Mid_Posterior       1   \n",
       "4450     253     P1575                    CC_Central       1   \n",
       "4451     254     P1575               CC_Mid_Anterior       1   \n",
       "4452     255     P1575                   CC_Anterior       1   \n",
       "\n",
       "      number_of_voxels      ad      fa      md      rd  \\\n",
       "0               254214  1.1730  0.3512  0.8372  0.6692   \n",
       "1               269805  1.1130  0.1406  0.9781  0.9106   \n",
       "2                  254  2.5920  0.2222  2.0895  1.8383   \n",
       "3                15761  1.1368  0.3502  0.8160  0.6556   \n",
       "4                59880  1.1135  0.1755  0.9446  0.8602   \n",
       "...                ...     ...     ...     ...     ...   \n",
       "4448               719  2.1427  0.5765  1.2994  0.8778   \n",
       "4449               383  2.0115  0.4268  1.3696  1.0486   \n",
       "4450               790  1.9900  0.4787  1.2957  0.9485   \n",
       "4451               673  1.9592  0.4369  1.3685  1.0732   \n",
       "4452               860  1.9751  0.5544  1.1756  0.7758   \n",
       "\n",
       "      gray_matter_volume_mm^3 sex       classID  \n",
       "0                    254214.0   M  ping-siemens  \n",
       "1                    269805.0   M  ping-siemens  \n",
       "2                       254.0   M  ping-siemens  \n",
       "3                     15761.0   M  ping-siemens  \n",
       "4                     59880.0   M  ping-siemens  \n",
       "...                       ...  ..           ...  \n",
       "4448                    719.0   M  ping-siemens  \n",
       "4449                    383.0   M  ping-siemens  \n",
       "4450                    790.0   M  ping-siemens  \n",
       "4451                    673.0   M  ping-siemens  \n",
       "4452                    860.0   M  ping-siemens  \n",
       "\n",
       "[4453 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graymatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def buildReferenceData(data,outliers,profile,data_dir,filename):\n",
    "data = track_data\n",
    "outliers = outliers_afcc\n",
    "profile = True\n",
    "\n",
    "    \n",
    "\n",
    "reference_data = pd.DataFrame()\n",
    "\n",
    "for s in outliers.structureID.unique():\n",
    "    for m in outliers.measures.unique():\n",
    "        if profile:\n",
    "            tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID','nodeID',m]].reset_index()\n",
    "        else:\n",
    "            tmpdata = data.loc[data['structureID'] == s].loc[~data['subjectID'].isin(outliers.loc[outliers['structureID'] == s].loc[outliers['measures'] == m].subjectID.unique())][['structureID','subjectID',m]].reset_index()\n",
    "        reference_data = pd.concat([reference_data,tmpdata])\n",
    "if not profile:\n",
    "    reference_data = reference_data.groupby(['structureID','subjectID']).mean().reset_index()\n",
    "\n",
    "# reference_data.to_csv(data_dir+'/'+filename+'.csv',index=False)\n",
    "    \n",
    "#     return reference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data['colors'] = [ 'yellow' for f in track_data['sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left-Hippocampus\n"
     ]
    }
   ],
   "source": [
    "## compute outliers\n",
    "# track data: example anterior frontall CC\n",
    "# def outlierDetection(data,structures,groupby_measure,measures,threshold,dist_metric,build_outliers,profile,resample_points,data_dir,filename):\n",
    "# track_distances, track_outliers, track_reference_dataframe, track_reference_json\n",
    "# distances_afcc, outliers_afcc, reference_dataframe_afcc, track_reference_json_afcc = outlierDetection(track_data,['anterioFrontalCC'],'nodeID',['volume','fa'],99,'euclidean',True,True,\"\",data_dir,'track_reference_ping_afcc')\n",
    "# reference_dataframe_afcc['classID'] = [ 'ping' for f in reference_dataframe_afcc['subjectID']]\n",
    "# reference_dataframe_afcc['colors'] = [ 'yellow' for f in reference_dataframe_afcc['subjectID']]\n",
    "# outliers_afcc.to_csv('./data/outliers_afcc_ping.csv',index=False)\n",
    "# reference_dataframe_afcc.to_csv('./data/reference_dataframe_afcc_ping.csv',index=False)\n",
    "\n",
    "# # generate plot pre-outlier computation\n",
    "# fig = plotProfiles(['anterioFrontalCC'],track_data,['fa'],'mean','std',\"\",\"\")\n",
    "# plt.savefig('./img/anterior_frontal_cc_fa_pre_outliers_example.eps')\n",
    "# plt.savefig('./img/anterior_frontal_cc_fa_pre_outliers_example.png')\n",
    "# plt.close()\n",
    "\n",
    "# # generate outliers and reference plot\n",
    "# fig = plotProfiles(['anterioFrontalCC'],reference_dataframe_afcc,['fa'],'mean','std',\"\",\"\")\n",
    "# sns.lineplot(x='nodeID',y='fa',data=track_data.loc[track_data['structureID'] == 'anterioFrontalCC'].loc[track_data['subjectID'].isin(outliers_afcc.loc[outliers_afcc['measures'] == 'fa']['subjectID'].unique())],hue='subjectID',palette=colors_dict,legend=False,linewidth=5)\n",
    "# plt.savefig('./img/anterior_frontal_cc_fa_outliers_example.eps')\n",
    "# plt.savefig('./img/anterior_frontal_cc_fa_outliers_example.png')\n",
    "# plt.close()\n",
    "\n",
    "# subcortical data: left hippocampus\n",
    "distances_hipp, outliers_hipp, reference_dataframe_hipp, reference_json_hipp = outlierDetection(subcortical,['Left-Hippocampus'],'structureID',['gray_matter_volume_mm^3','fa'],99,'emd',True,False,\"\",data_dir,'subcortical_reference_ping_left_hippocampus')\n",
    "reference_dataframe_hipp['classID'] = [ 'ping' for f in reference_dataframe_hipp['subjectID']]\n",
    "reference_dataframe_hipp['colors'] = [ 'yellow' for f in reference_dataframe_hipp['subjectID']]\n",
    "outliers_hipp.to_csv('./data/outliers_hipp_ping.csv',index=False)\n",
    "reference_dataframe_hipp.to_csv('./data/reference_dataframe_hipp_ping.csv',index=False)\n",
    "\n",
    "# generate distribution of volume pre outliers\n",
    "sns.violinplot(x='gray_matter_volume_mm^3',data=subcortical.loc[subcortical['structureID'] == 'Left-Hippocampus'],scale='count',inner='points')\n",
    "plt.savefig('./img/left_hippocampus_volume_pre_outliers_example.png')\n",
    "plt.savefig('./img/left_hippocampus_volume_pre_outliers_example.eps')\n",
    "plt.close()\n",
    "\n",
    "# generate distribution of volume with outliers\n",
    "sns.violinplot(x='gray_matter_volume_mm^3',data=reference_dataframe_hipp,scale='count',inner='points')\n",
    "sns.swarmplot(x='gray_matter_volume_mm^3',data=subcortical.loc[subcortical['structureID'] == 'Left-Hippocampus'].loc[subcortical['subjectID'].isin(outliers_hipp.loc[outliers_hipp['measures'] == 'gray_matter_volume_mm^3']['subjectID'].unique())],color='red')\n",
    "plt.savefig('./img/left_hippocampus_volume_outliers_example.png')\n",
    "plt.savefig('./img/left_hippocampus_volume_pre_outliers_example.eps')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "afcc = track_data.loc[track_data['structureID'] == 'anterioFrontalCC']\n",
    "afcc.to_csv('./data/tractmeasures-total_nodes-merged-afcc_camcan.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6da16d3f50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.violinplot(x='volume',data=reference_dataframe_hipp,scale='count',inner='points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
